{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/spike_sorting_jct/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from kilosort.io import load_ops\n",
    "import sys\n",
    "import spikeinterface as si\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.widgets as sw\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from kilosort import io\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "global_job_kwargs = dict(n_jobs = 4)\n",
    "si.set_global_job_kwargs(**global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_spike_inf(file_path):\n",
    "    cluster_inf = pd.read_csv(file_path + \"/analyzer_kilosort4_binary/extensions/quality_metrics/metrics.csv\")\n",
    "    cluster_inf.columns = ['cluster', 'num_spikes', 'firing_rate', 'presence_ratio', 'snr',\n",
    "                           'isi_violations_ratio', 'isi_violations_count', 'rp_contamination',\n",
    "                           'rp_violations', 'sliding_rp_violation', 'amplitude_cutoff',\n",
    "                           'amplitude_median', 'amplitude_cv_median', 'amplitude_cv_range',\n",
    "                           'sync_spike_2', 'sync_spike_4', 'sync_spike_8', 'firing_range',\n",
    "                           'drift_ptp', 'drift_std', 'drift_mad', 'sd_ratio']\n",
    "    cluster_inf['cluster'] = cluster_inf['cluster'].astype(str)\n",
    "    cluster_inf['position_1'] = None\n",
    "    cluster_inf['position_2'] = None\n",
    "\n",
    "    def get_best_channels(results_dir):\n",
    "        \"\"\"Get channel numbers with largest template norm for each cluster.\"\"\"\n",
    "        templates = np.load(results_dir + '/templates.npy')\n",
    "        best_chans = (templates**2).sum(axis=1).argmax(axis=-1)\n",
    "        return best_chans\n",
    "\n",
    "    def get_six_best_channels(results_dir):\n",
    "        \"\"\"Get channel numbers with largest template norm for each cluster.\"\"\"\n",
    "        templates = np.load(results_dir + '/templates.npy')\n",
    "        template_norms = (templates ** 2).sum(axis=1)\n",
    "        best_chans = np.argsort(template_norms, axis=-1)[:, -6:][:, ::-1]\n",
    "        return best_chans\n",
    "\n",
    "    best_chans = get_best_channels(results_dir=file_path + \"/kilosort4/sorter_output\")\n",
    "    best_six_chans = get_six_best_channels(results_dir=file_path + \"/kilosort4/sorter_output\")\n",
    "    cluster_inf['best_chans'] = best_chans\n",
    "    cluster_inf['best_six_chans'] = best_six_chans.tolist()\n",
    "\n",
    "    spike_clusters = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/spike_clusters.npy\").astype(str))\n",
    "    spike_positions = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/spike_positions.npy\").astype(float))\n",
    "    spike_templates = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/spike_templates.npy\"))\n",
    "    spike_times = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/spike_times.npy\").astype(int))\n",
    "    tf = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/tF.npy\")[:, 0, :])\n",
    "\n",
    "    spike_inf = pd.concat((spike_clusters, spike_positions, spike_templates, spike_times, tf), axis=1)\n",
    "    spike_inf.columns = ['cluster', 'position_1', 'position_2', 'templates', 'time', 'PC_1', 'PC_2', 'PC_3', 'PC_4', 'PC_5', 'PC_6']\n",
    "\n",
    "    for i in spike_inf['cluster'].value_counts().index:\n",
    "        temp = spike_inf[spike_inf['cluster'] == i]\n",
    "        cluster_inf.loc[cluster_inf['cluster'] == i, 'position_1'] = np.mean(temp['position_1'])\n",
    "        cluster_inf.loc[cluster_inf['cluster'] == i, 'position_2'] = np.mean(temp['position_2'])\n",
    "\n",
    "    cluster_inf['probe_group'] = \"1\"\n",
    "\n",
    "    for i in spike_inf['cluster'].value_counts().index:\n",
    "        cluster_rows = cluster_inf[cluster_inf['cluster'] == i]\n",
    "        if (cluster_rows['position_1'] > 100).any() and (cluster_rows['position_1'] < 250).any():\n",
    "            cluster_inf.loc[cluster_inf['cluster'] == i, 'probe_group'] = \"2\"\n",
    "        elif (cluster_rows['position_1'] > 250).any() and (cluster_rows['position_1'] < 400).any():\n",
    "            cluster_inf.loc[cluster_inf['cluster'] == i, 'probe_group'] = \"3\"\n",
    "        elif (cluster_rows['position_1'] > 400).any() and (cluster_rows['position_1'] < 550).any():\n",
    "            cluster_inf.loc[cluster_inf['cluster'] == i, 'probe_group'] = \"4\"\n",
    "        elif (cluster_rows['position_1'] > 550).any():\n",
    "            cluster_inf.loc[cluster_inf['cluster'] == i, 'probe_group'] = \"5\"\n",
    "\n",
    "    cluster_inf = cluster_inf[((cluster_inf['snr'] > 3) & (cluster_inf['num_spikes'] > int(5000))) | ((cluster_inf['snr'] < 3) & (cluster_inf['num_spikes'] > 8000))]\n",
    "    spike_inf = spike_inf[spike_inf['cluster'].isin(list(cluster_inf['cluster']))]\n",
    "    spike_inf = spike_inf[spike_inf['time'] > 200]\n",
    "    cluster_inf['date'] = date\n",
    "    spike_inf['date'] = date\n",
    "\n",
    "    return cluster_inf, spike_inf\n",
    "\n",
    "all_cluster_inf = pd.DataFrame()\n",
    "all_spike_inf = pd.DataFrame()\n",
    "\n",
    "for date in os.listdir(\"/media/ubuntu/sda/data/sort_output/mouse6/grating\"):\n",
    "    cluster_inf, spike_inf = get_spike_inf(file_path=f\"/media/ubuntu/sda/data/sort_output/mouse6/grating/{date}\")\n",
    "    all_cluster_inf = pd.concat([all_cluster_inf, cluster_inf], ignore_index=True)\n",
    "    all_spike_inf = pd.concat([all_spike_inf, spike_inf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_inf['Neuron'] = None\n",
    "neuron_count = 1\n",
    "all_cluster_inf.at[0, 'Neuron'] = f'Neuron_{neuron_count}'\n",
    "\n",
    "for i in range(1, len(all_cluster_inf)):\n",
    "    is_new_neuron = True\n",
    "    for j in range(i):\n",
    "        if abs(all_cluster_inf.iloc[i]['position_1'] - all_cluster_inf.iloc[j]['position_1']) < 10 and \\\n",
    "           abs(all_cluster_inf.iloc[i]['position_2'] - all_cluster_inf.iloc[j]['position_2']) < 10:\n",
    "            all_cluster_inf.at[i, 'Neuron'] = all_cluster_inf.at[j, 'Neuron']\n",
    "            is_new_neuron = False\n",
    "            break\n",
    "    if is_new_neuron:\n",
    "        neuron_count += 1\n",
    "        all_cluster_inf.at[i, 'Neuron'] = f'Neuron_{neuron_count}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"/media/ubuntu/sda/data/sort_output/mouse6/grating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_date = pd.crosstab(all_cluster_inf['Neuron'], all_cluster_inf['date'])   \n",
    "neuron_date[neuron_date > 1] = 1\n",
    "neuron_date = neuron_date.sum(axis=1)\n",
    "neuron_date = neuron_date[neuron_date == 14]\n",
    "neuron_date = neuron_date.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_inf = all_cluster_inf[all_cluster_inf['Neuron'].isin(neuron_date)]\n",
    "all_cluster_inf['cluster_date'] = all_cluster_inf['date']  + \"_\" +  all_cluster_inf['cluster']\n",
    "all_spike_inf['cluster_date'] = all_spike_inf['date']  + \"_\" +  all_spike_inf['cluster']\n",
    "\n",
    "all_spike_inf = all_spike_inf[all_spike_inf['cluster_date'].isin(list(all_cluster_inf['cluster_date']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spike_waveforms(spikes, results_dir, bfile=None, chan=None):\n",
    "    \"\"\"Get waveform for each spike in `spikes`, multi- or single-channel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spikes : list or array-like\n",
    "        Spike times (in units of samples) for the desired waveforms, from\n",
    "        `spike_times.npy`.\n",
    "    results_dir : str or Path\n",
    "        Path to directory where Kilosort4 sorting results were saved.\n",
    "    bfile : kilosort.io.BinaryFiltered; optional\n",
    "        Kilosort4 data file object. By default, this will be loaded using the\n",
    "        information in `ops.npy` in the saved results.\n",
    "    chan : int; optional.\n",
    "        Channel to use for single-channel waveforms. If not specified, all\n",
    "        channels will be returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    waves : np.ndarray\n",
    "        Array of spike waveforms with shape `(nt, len(spikes))`.\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(spikes, int):\n",
    "        spikes = [spikes]\n",
    "\n",
    "    if bfile is None:\n",
    "        ops = io.load_ops(results_dir + '/ops.npy')\n",
    "        bfile = io.bfile_from_ops(ops)\n",
    "    whitening_mat_inv = np.load(results_dir + '/whitening_mat_inv.npy')\n",
    "\n",
    "    waves = []\n",
    "    for t in spikes:\n",
    "        tmin = t - bfile.nt0min\n",
    "        tmax = t + (bfile.nt - bfile.nt0min)\n",
    "        w = bfile[tmin:tmax].cpu().numpy()\n",
    "        if whitening_mat_inv is not None:\n",
    "            w = whitening_mat_inv @ w\n",
    "        if w.shape[1] == bfile.nt:\n",
    "            # Don't include spikes at the start or end of the recording that\n",
    "            # get truncated to fewer time points.\n",
    "            waves.append(w)\n",
    "    waves = np.stack(waves, axis=-1)\n",
    "\n",
    "    if chan is not None:\n",
    "        waves = waves[chan,:]\n",
    "    \n",
    "    bfile.close()\n",
    "\n",
    "    return waves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kilosort import io\n",
    "def save_waveform_data(waveform_dict, waveform_mean, neuron):\n",
    "    waveform_dict_path = f\"/media/ubuntu/sda/data/filter_neuron/mouse_6/grating/waveform/waveform_dict_{neuron}.pkl\"\n",
    "    waveform_mean_path = f\"/media/ubuntu/sda/data/filter_neuron/mouse_6/grating/waveform/waveform_mean_{neuron}.csv\"\n",
    "    \n",
    "    pd.to_pickle(waveform_dict, waveform_dict_path)\n",
    "    waveform_mean.to_csv(waveform_mean_path)\n",
    "os.makedirs(\"/media/ubuntu/sda/data/filter_neuron/mouse_6/grating/waveform/\", exist_ok=True)\n",
    "for neuron in all_cluster_inf['Neuron'].unique():\n",
    "    waveform_dict = {}\n",
    "    waveform_mean = pd.DataFrame()\n",
    "    \n",
    "    neuron_inf = all_cluster_inf[all_cluster_inf['Neuron'] == neuron]\n",
    "    \n",
    "    for date in neuron_inf['date'].unique():\n",
    "        cluster_inf_date = neuron_inf[neuron_inf['date'] == date]\n",
    "        spike_inf_date = all_spike_inf[all_spike_inf['date'] == date]\n",
    "        \n",
    "        for cluster in cluster_inf_date['cluster'].unique():\n",
    "            temp = spike_inf_date[spike_inf_date['cluster'] == cluster]\n",
    "            best_chan = cluster_inf_date.loc[cluster_inf_date['cluster'] == cluster, 'best_chans'].values[0]\n",
    "            waveforms = get_spike_waveforms(spikes=list(temp['time']), results_dir=f\"/media/ubuntu/sda/data/sort_output/mouse6/grating/{date}/kilosort4/sorter_output\", chan=best_chan)\n",
    "            \n",
    "            date_cluster = f\"{date}_{cluster}\"\n",
    "            waveform_dict[date_cluster] = waveforms\n",
    "            \n",
    "            mean_waveform = np.mean(waveforms, axis=1)\n",
    "            waveform_mean[date_cluster] = mean_waveform\n",
    "    \n",
    "    waveform_mean = waveform_mean.T\n",
    "    save_waveform_data(waveform_dict, waveform_mean, neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "num = 0\n",
    "results = {}\n",
    "folder_path = '/media/ubuntu/sda/data/filter_neuron/mouse_6/grating/waveform/'\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.startswith('waveform_mean_Neuron_') and f.endswith('.csv')]\n",
    "\n",
    "\n",
    "label_df = pd.DataFrame()\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, csv_file), index_col=0)\n",
    "    \n",
    "    from sklearn.cluster import DBSCAN\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(df)\n",
    "\n",
    "    eps = 100\n",
    "    min_samples = 1\n",
    "\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(principal_components)\n",
    "\n",
    "    label = pd.DataFrame(dbscan.labels_, columns=['labels'])\n",
    "    label['cluster_date'] = df.index\n",
    "    label['date'] = label['cluster_date'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "    remain_label = label['labels'].value_counts()\n",
    "    remain_label = remain_label[remain_label >= 14]\n",
    "    for i in remain_label.index:\n",
    "        temp = label[label['labels'] == i]\n",
    "        if temp['date'].nunique() != 14:\n",
    "            remain_label = remain_label.drop(i)\n",
    "    label = label[label['labels'].isin(remain_label.index)]\n",
    "    for i in label['labels'].unique():\n",
    "        results[num] = label.loc[label['labels'] ==i, 'cluster_date'].values\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_inf['Neuron'] = None\n",
    "for key,item in results.items():\n",
    "    all_cluster_inf.loc[all_cluster_inf['cluster_date'].isin(item), 'Neuron'] = f'Neuron_{key+1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_inf = all_cluster_inf.dropna(subset=['Neuron'])\n",
    "all_spike_inf = all_spike_inf[all_spike_inf['cluster_date'].isin(all_cluster_inf['cluster_date'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spike_inf['Neuron'] = None\n",
    "for i in range(len(all_cluster_inf)):\n",
    "    all_spike_inf.loc[all_spike_inf['cluster_date'] == all_cluster_inf.iloc[i, 29], \"Neuron\"] = all_cluster_inf.iloc[i, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_inf['neuron_date'] = all_cluster_inf['date'] + \"_\" + all_cluster_inf['Neuron']\n",
    "all_spike_inf['neuron_date'] = all_spike_inf['date'] + \"_\" + all_spike_inf['Neuron']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_inf.to_csv('/media/ubuntu/sda/data/filter_neuron/mouse_6/grating/cluster_inf.tsv', sep = '\\t')\n",
    "all_spike_inf.to_csv(\"/media/ubuntu/sda/data/filter_neuron/mouse_6/grating/spike_inf.tsv\", sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

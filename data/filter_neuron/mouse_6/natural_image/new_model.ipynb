{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\"统一管理模型所有超参数的配置类\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_neuron=256,        # 输入神经元数量\n",
    "                 time_bins=100,           # 时间步数量\n",
    "                 d_model = 128,           # Transformer模型维度\n",
    "                 nhead=4,                # 注意力头数\n",
    "                 num_transformer_layers=2, # Transformer层数\n",
    "                 conv_channels=64,       # 卷积通道数\n",
    "                 num_conv_blocks=3,      # 卷积块数量\n",
    "                 num_classes=10,         # 分类类别数\n",
    "                 residual_dims=[256, 512, 1024], # 残差层维度\n",
    "                 use_positional_encoding=True,  # 是否使用位置编码\n",
    "                 dim_feedforward_ratio=4,       # FeedForward维度比例\n",
    "                 activation='relu',\n",
    "                 use_neuron_masking=True,  # 新增：是否启用神经元遮蔽\n",
    "                 mask_ratio=0.15,\n",
    "                 mask_replacement='random'):\n",
    "        \n",
    "        # Transformer \n",
    "        self.transformer = {\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_layers': num_transformer_layers,\n",
    "            'dim_feedforward': d_model * dim_feedforward_ratio,\n",
    "            'activation': activation\n",
    "        }\n",
    "        \n",
    "        # cnn\n",
    "        self.convolution = {\n",
    "            'channels': conv_channels,\n",
    "            'num_blocks': num_conv_blocks,\n",
    "            'kernel_size': (3, 3),\n",
    "            'pool_size': (2, 2)\n",
    "        }\n",
    "        \n",
    "        # resnet\n",
    "        self.residual = {\n",
    "            'dims': residual_dims,\n",
    "            'skip_connection': True\n",
    "        }\n",
    "        \n",
    "        self.masking = {\n",
    "            'enabled': use_neuron_masking,\n",
    "            'ratio': mask_ratio,\n",
    "            'replacement': mask_replacement\n",
    "        }\n",
    "\n",
    "        self.input_dim = input_neuron\n",
    "        self.time_steps = time_bins\n",
    "        self.num_classes = num_classes\n",
    "        self.positional_encoding = use_positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class NeuronMasker(nn.Module):\n",
    "    def __init__(self, mask_ratio=0.15, replacement='zero'):\n",
    "        super().__init__()\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.replacement = replacement\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            batch_size, seq_len, feat_dim = x.shape\n",
    "            mask = torch.rand_like(x) < self.mask_ratio\n",
    "            \n",
    "            if self.replacement == 'zero':\n",
    "                x_masked = x.masked_fill(mask, 0)\n",
    "            elif self.replacement == 'random':\n",
    "                random_values = torch.randn_like(x) * 0.02\n",
    "                x_masked = x.masked_scatter(mask, random_values)\n",
    "            return x_masked\n",
    "        \n",
    "class ResidualLinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.downsample = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x + residual\n",
    "\n",
    "class TimeTransformerConvModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self._init_masking()\n",
    "        self.input_proj = nn.Linear(config.input_dim, config.transformer['d_model'])\n",
    "        self.pos_encoder = PositionalEncoding(config.transformer['d_model']) if config.positional_encoding else nn.Identity()\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.transformer['d_model'],\n",
    "            nhead=config.transformer['nhead'],\n",
    "            dim_feedforward=config.transformer['dim_feedforward'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, config.transformer['num_layers'])\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential()\n",
    "        in_channels = 1\n",
    "        for _ in range(config.convolution['num_blocks']):\n",
    "            self.conv_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, config.convolution['channels'], \n",
    "                            kernel_size=config.convolution['kernel_size'], padding='same'),\n",
    "                    nn.BatchNorm2d(config.convolution['channels']),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=config.convolution['pool_size'])\n",
    "                )\n",
    "            )\n",
    "            in_channels = config.convolution['channels']\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(config.convolution['channels'], config.num_classes)\n",
    "        \n",
    "        self.residual_layers = nn.Sequential()\n",
    "        current_dim = config.convolution['channels']\n",
    "        for dim in config.residual['dims']:\n",
    "            self.residual_layers.append(ResidualLinearBlock(current_dim, dim))\n",
    "            current_dim = dim\n",
    "        if current_dim != 1024:\n",
    "            self.residual_layers.append(nn.Linear(current_dim, 1024))\n",
    "            self.residual_layers.append(nn.LayerNorm(1024))\n",
    "\n",
    "    def _init_masking(self):\n",
    "        if self.config.masking['enabled']:\n",
    "            self.masker = NeuronMasker(\n",
    "                mask_ratio=self.config.masking['ratio'],\n",
    "                replacement=self.config.masking['replacement']\n",
    "            )\n",
    "        else:\n",
    "            self.masker = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.masker(x)  # [B, T, D]\n",
    "        \n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        features = self.residual_layers(x)\n",
    "        \n",
    "        return logits, features\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 50)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.load(\"/media/ubuntu/sda/data/filter_neuron/mouse_6/natural_image/seg_fr/021322_2_4.npy\")\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25181818181818183"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "277/ (22*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import csv\n",
    "from torch import Tensor\n",
    "import itertools\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_binned_spiketrains(trigger_time_df, spike_inf_df, target_image):\n",
    "    \"\"\"\n",
    "    生成指定image下的分箱脉冲矩阵\n",
    "    \n",
    "    参数\n",
    "    ----\n",
    "    trigger_time_df : pd.DataFrame\n",
    "        列包括：start, end, image, date, order\n",
    "    spike_inf_df : pd.DataFrame\n",
    "        列包括：time, neuron, date\n",
    "    target_image : str/int\n",
    "        目标图像标识\n",
    "    \n",
    "    返回\n",
    "    ----\n",
    "    binned_data : list of ndarray\n",
    "        [\n",
    "            # Trial 1 的矩阵 (neurons × 100 bins)\n",
    "            array([[n0_bin1_count, n0_bin2_count, ...],\n",
    "                   [n1_bin1_count, n1_bin2_count, ...],\n",
    "                   ...]),\n",
    "            # Trial 2\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # =====================================\n",
    "    # 步骤 1: 筛选目标试次并转换时间单位\n",
    "    # =====================================\n",
    "    mask = (trigger_time_df['image'] == target_image)\n",
    "    target_triggers = trigger_time_df[mask].sort_values('order')\n",
    "    \n",
    "    # 转换时间单位 (0.1ms → 秒)\n",
    "    target_triggers = target_triggers.copy()\n",
    "    target_triggers['start'] = target_triggers['start'] * 0.1e-3\n",
    "    target_triggers['end'] = target_triggers['end'] * 0.1e-3\n",
    "\n",
    "    # =====================================\n",
    "    # 步骤 2: 处理神经脉冲数据\n",
    "    # =====================================\n",
    "    target_spikes = spike_inf_df.copy()\n",
    "    target_spikes['time'] = target_spikes['time'] * 0.1e-3 \n",
    "    \n",
    "    # 获取所有唯一神经元ID并排序（基于完整数据集）\n",
    "    all_neuron_ids = sorted(spike_inf_df['Neuron'].unique()) if not spike_inf_df.empty else []\n",
    "\n",
    "    # =====================================\n",
    "    # 步骤 3: 分箱处理每个试次\n",
    "    # =====================================\n",
    "    binned_data = []\n",
    "    for _, trial in target_triggers.iterrows():\n",
    "        trial_start = trial['start']\n",
    "        trial_end = trial['end']\n",
    "        trial_duration = trial_end - trial_start\n",
    "        \n",
    "        spike_mask = (target_spikes['time'] >= trial_start) & (target_spikes['time'] < trial_end)\n",
    "        trial_spikes = target_spikes[spike_mask].copy()\n",
    "        trial_spikes['rel_time'] = trial_spikes['time'] - trial_start\n",
    "        \n",
    "        bin_matrix = np.zeros((len(all_neuron_ids), 100), dtype=int)\n",
    "        \n",
    "        neuron_groups = trial_spikes.groupby('Neuron')\n",
    "        for neuron_idx, neuron_id in enumerate(all_neuron_ids):\n",
    "            if neuron_id in neuron_groups.groups:\n",
    "                group = neuron_groups.get_group(neuron_id)\n",
    "                times = group['rel_time'].values\n",
    "                \n",
    "                counts, _ = np.histogram(times, bins=100, range=(0, trial_duration))\n",
    "                bin_matrix[neuron_idx] = counts\n",
    "                \n",
    "        binned_data.append(bin_matrix)\n",
    "    \n",
    "    return binned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPDataset(Dataset):\n",
    "    def __init__(self, EP_data, labels, features):  \n",
    "        self.EP_data = EP_data\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.EP_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        EP_tensor = torch.tensor(self.EP_data[idx].T, dtype=torch.float32) \n",
    "        label = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "        feature = self.features[idx]\n",
    "        \n",
    "        return EP_tensor, label, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self,\n",
    "                 input_neuron=25,        \n",
    "                 time_bins=20,          \n",
    "                 d_model = 150,          \n",
    "                 nhead=10,                \n",
    "                 num_transformer_layers=1, \n",
    "                 conv_channels=64,      \n",
    "                 num_conv_blocks=3,      \n",
    "                 num_classes=117,        \n",
    "                 residual_dims=[256, 512, 1024], \n",
    "                 use_positional_encoding=True,  \n",
    "                 dim_feedforward_ratio=4,      \n",
    "                 activation='relu',\n",
    "                 use_neuron_masking=True,  \n",
    "                 mask_ratio=0,\n",
    "                 mask_replacement='zero'):\n",
    "        \n",
    "        # Transformer \n",
    "        self.transformer = {\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_layers': num_transformer_layers,\n",
    "            'dim_feedforward': d_model * dim_feedforward_ratio,\n",
    "            'activation': activation\n",
    "        }\n",
    "        \n",
    "        # cnn\n",
    "        self.convolution = {\n",
    "            'channels': conv_channels,\n",
    "            'num_blocks': num_conv_blocks,\n",
    "            'kernel_size': (3, 3),\n",
    "            'pool_size': (2, 2)\n",
    "        }\n",
    "        \n",
    "        # resnet\n",
    "        self.residual = {\n",
    "            'dims': residual_dims,\n",
    "            'skip_connection': True\n",
    "        }\n",
    "        \n",
    "        self.masking = {\n",
    "            'enabled': use_neuron_masking,\n",
    "            'ratio': mask_ratio,\n",
    "            'replacement': mask_replacement\n",
    "        }\n",
    "\n",
    "        self.input_dim = input_neuron\n",
    "        self.time_steps = time_bins\n",
    "        self.num_classes = num_classes\n",
    "        self.positional_encoding = use_positional_encoding\n",
    "        self.lr = 2e-4\n",
    "        self.epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuronMasker(nn.Module):\n",
    "    def __init__(self, mask_ratio=0.15, replacement='random'):\n",
    "        super().__init__()\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.replacement = replacement\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            if x is None:\n",
    "                raise ValueError(\"Input tensor x is None\")\n",
    "                \n",
    "            batch_size, seq_len, feat_dim = x.shape\n",
    "            mask = torch.rand(batch_size, 1, feat_dim, device=x.device) < self.mask_ratio\n",
    "            mask = mask.expand_as(x)\n",
    "            \n",
    "            if self.replacement == 'zero':\n",
    "                x_masked = x.masked_fill(mask, 0)\n",
    "            elif self.replacement == 'random':\n",
    "                random_values = torch.randn_like(x) * 0.02\n",
    "                x_masked = x.masked_scatter(mask, random_values)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid replacement: {self.replacement}\")\n",
    "            \n",
    "            return x_masked \n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class ResidualLinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.downsample = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x + residual\n",
    "\n",
    "class TimeTransformerConvModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.input_proj = nn.Linear(config.input_dim, config.transformer['d_model'])\n",
    "        self.pos_encoder = PositionalEncoding(config.transformer['d_model']) if config.positional_encoding else nn.Identity()\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.transformer['d_model'],\n",
    "            nhead=config.transformer['nhead'],\n",
    "            dim_feedforward=config.transformer['dim_feedforward'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, config.transformer['num_layers'])\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential()\n",
    "        in_channels = 1\n",
    "        for _ in range(config.convolution['num_blocks']):\n",
    "            self.conv_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, config.convolution['channels'], \n",
    "                            kernel_size=config.convolution['kernel_size'], padding='same'),\n",
    "                    nn.BatchNorm2d(config.convolution['channels']),\n",
    "                    nn.ELU(),\n",
    "                    nn.MaxPool2d(kernel_size=config.convolution['pool_size'])\n",
    "                )\n",
    "            )\n",
    "            in_channels = config.convolution['channels']\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(config.convolution['channels'], config.num_classes)\n",
    "        \n",
    "        self.residual_layers = nn.Sequential()\n",
    "        current_dim = config.convolution['channels']\n",
    "        for dim in config.residual['dims']:\n",
    "            self.residual_layers.append(ResidualLinearBlock(current_dim, dim))\n",
    "            current_dim = dim\n",
    "        if current_dim != 1024:\n",
    "            self.residual_layers.append(nn.Linear(current_dim, 1024))\n",
    "            self.residual_layers.append(nn.LayerNorm(1024))\n",
    "\n",
    "\n",
    "        self.masker = NeuronMasker(\n",
    "            mask_ratio=self.config.masking['ratio'],\n",
    "            replacement=self.config.masking['replacement']\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.masker(x)  # [B, T, D]\n",
    "        #print(x)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        features = self.residual_layers(x)\n",
    "        \n",
    "        return logits, features\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, alpha=0, temp=0.07):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha      # 分类损失权重\n",
    "        self.temp = temp\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.temp = temp\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def contrastive_loss(self, h_neuro, h_img):\n",
    "        h_neuro = F.normalize(h_neuro, dim=1) + 1e-10\n",
    "        h_img = F.normalize(h_img, dim=1) + 1e-10\n",
    "        \n",
    "        logits_ab = torch.matmul(h_neuro, h_img.T) / self.temp\n",
    "        logits_ba = torch.matmul(h_img, h_neuro.T) / self.temp\n",
    "        \n",
    "        labels = torch.arange(h_neuro.size(0), device=h_neuro.device)\n",
    "        loss_ab = F.cross_entropy(logits_ab, labels)\n",
    "        loss_ba = F.cross_entropy(logits_ba, labels)\n",
    "        \n",
    "        return (loss_ab + loss_ba) / 2\n",
    "    \n",
    "    def forward(self, logits, labels, img_feature, features):\n",
    "        loss_cls = self.ce_loss(logits, labels)\n",
    "        loss_cont = self.contrastive_loss(features, img_feature)\n",
    "        total_loss = self.alpha * loss_cls + (1 - self.alpha) * loss_cont\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, device, criterion, config):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (neuro, labels, img_feature) in enumerate(dataloader):\n",
    "        neuro = neuro.to(device)\n",
    "        labels = labels.to(device)\n",
    "        img_feature = img_feature.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, features = model(neuro)\n",
    "        loss = criterion(logits, labels, img_feature, features)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), \n",
    "            max_norm=3.0,                   \n",
    "            norm_type=2.0                   \n",
    "        )\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计指标\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_loss = total_loss / len(dataloader)\n",
    "    train_accuracy = correct / total\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device, criterion, config, image_cluster):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    correct_2way = 0\n",
    "    correct_4way = 0\n",
    "    correct_10way = 0\n",
    "    total = 0\n",
    "    \n",
    "    # 从 DataFrame 创建聚类映射字典\n",
    "    cluster_2_map = torch.tensor(image_cluster[\"2_cluster\"].values, device=device)\n",
    "    cluster_4_map = torch.tensor(image_cluster[\"4_cluster\"].values, device=device)\n",
    "    cluster_10_map = torch.tensor(image_cluster[\"10_cluster\"].values, device=device)\n",
    "    \n",
    "    for neuro, labels, img_feature in dataloader:\n",
    "        neuro = neuro.to(device)\n",
    "        labels = labels.to(device)\n",
    "        img_feature = img_feature.to(device)\n",
    "        \n",
    "        logits, features = model(neuro)\n",
    "        \n",
    "        loss = criterion(logits, labels, img_feature, features)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # （Top-1 和 Top-5）\n",
    "        _, predicted_top1 = torch.max(logits, 1)\n",
    "        correct_top1 += (predicted_top1 == labels).sum().item()\n",
    "        _, predicted_top5 = logits.topk(5, dim=1)\n",
    "        correct_top5 += torch.sum(predicted_top5.eq(labels.view(-1, 1))).item()\n",
    "        \n",
    "        # (2-way, 4-way, 10-way)\n",
    "        cluster_2_pred = cluster_2_map[predicted_top1]\n",
    "        cluster_4_pred = cluster_4_map[predicted_top1]\n",
    "        cluster_10_pred = cluster_10_map[predicted_top1]\n",
    "        \n",
    "        cluster_2_true = cluster_2_map[labels]\n",
    "        cluster_4_true = cluster_4_map[labels]\n",
    "        cluster_10_true = cluster_10_map[labels]\n",
    "        \n",
    "        correct_2way += (cluster_2_pred == cluster_2_true).sum().item()\n",
    "        correct_4way += (cluster_4_pred == cluster_4_true).sum().item()\n",
    "        correct_10way += (cluster_10_pred == cluster_10_true).sum().item()\n",
    "        \n",
    "        total += labels.size(0)\n",
    "    \n",
    "    test_loss = total_loss / len(dataloader)\n",
    "    test_accuracy = correct_top1 / total\n",
    "    top5_accuracy = correct_top5 / total\n",
    "    accuracy_2way = correct_2way / total\n",
    "    accuracy_4way = correct_4way / total\n",
    "    accuracy_10way = correct_10way / total\n",
    "    \n",
    "    return test_loss, test_accuracy, top5_accuracy, accuracy_2way, accuracy_4way, accuracy_10way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_loop(config, model, train_loader, test_loader, device, image_cluster):\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "    criterion = MultitaskLoss(alpha=0.7, temp=0.07)\n",
    "    \n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs, test_top5, acc_2way, acc_4way, acc_10way = [], [], [], [], [], []\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss, train_acc = train_model(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        test_loss, test_acc, top5_acc, accuracy_2way, accuracy_4way, accuracy_10way = evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=test_loader,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            config=config,\n",
    "            image_cluster = image_cluster\n",
    "        )\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        test_top5.append(top5_acc)\n",
    "        acc_2way.append(accuracy_2way)\n",
    "        acc_4way.append(accuracy_4way)\n",
    "        acc_10way.append(accuracy_10way)\n",
    "        \n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        # 打印日志\n",
    "        #print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        #print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%}\")\n",
    "        #print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2%} | Top-5 Acc: {top5_acc:.2%}\")\n",
    "        #print(f\"2-way Acc: {accuracy_2way:.2%} | 4-way Acc: {accuracy_4way:.2%} | 10-way Acc: {accuracy_10way:.2%}\")\n",
    "        #print(\"-\" * 60)\n",
    "        \n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(test_losses, label=\"Test\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label=\"Train Acc\")\n",
    "    plt.plot(test_accs, label=\"Test Acc\")\n",
    "    plt.plot(test_top5, label=\"Test Top-5\")\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        \"best_test_acc\": best_acc,\n",
    "        \"final_top5_acc\": test_top5[-1],\n",
    "        \"train_history\": {\n",
    "            \"loss\": train_losses,\n",
    "            \"accuracy\": train_accs\n",
    "        },\n",
    "        \"test_history\": {\n",
    "            \"loss\": test_losses,\n",
    "            \"accuracy\": test_accs,\n",
    "            \"top5_accuracy\": test_top5,\n",
    "            \"acc_2way\": acc_2way,\n",
    "            \"acc_4way\": acc_4way,\n",
    "            \"acc_10way\": acc_10way\n",
    "        },\n",
    "        \"best_epoch\": best_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_order = ['030222', '042422', '052322', '062322', '082422', \n",
    "              '092222', '102522', '112822', '122322', \n",
    "              '012123', '022423', '032323', '042323', '052423', '062323', '072123']\n",
    "\n",
    "date_order_num = [int(i) for i in date_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_time = pd.read_csv(\"/root/autodl-tmp/trigger_time_mouse2.tsv\", sep = '\\t')\n",
    "with open(\"/root/autodl-tmp/image_feature_list.pkl\", 'rb') as f:\n",
    "    image_feature_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'030222_100_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         EP_data_train_dict[i] \u001b[38;5;241m=\u001b[39m EP_data_dict[i]\n\u001b[0;32m---> 33\u001b[0m current_input_neuron \u001b[38;5;241m=\u001b[39m \u001b[43mEP_data_train_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m030222_100_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     35\u001b[0m EP_data_train_EP_data \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m EP_data_train_dict\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     36\u001b[0m EP_data_train_image \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m EP_data_train_dict\u001b[38;5;241m.\u001b[39mvalues()]  \n",
      "\u001b[0;31mKeyError\u001b[0m: '030222_100_1'"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for trial in range(1, 21):\n",
    "    for date_len in range(2, len(date_order)):\n",
    "        test_month = [date_order[date_len]]\n",
    "        all_spike_inf = pd.read_csv(f\"/root/autodl-tmp/closed_loop/mouse5/spike_072123.tsv\", sep=\"\\t\")\n",
    "        date_order_temp = date_order[:date_len+1]\n",
    "\n",
    "        EP_data_dict = {}\n",
    "        for date in date_order_temp:\n",
    "            temp = trigger_time[trigger_time['date'] == int(date)]\n",
    "            temp_spike = all_spike_inf[all_spike_inf['date'] == int(date)]\n",
    "\n",
    "            for image in range(1, 118):\n",
    "                num = 1\n",
    "                spike_train = generate_binned_spiketrains(temp, temp_spike, image)\n",
    "                for i in range(len(spike_train)):\n",
    "                    EP_data_dict[f\"{date}_{image}_{num}\"] = [spike_train[i], date, image - 1]\n",
    "                    num += 1\n",
    "\n",
    "        for i in EP_data_dict.keys():\n",
    "            EP_data_dict[i].append(image_feature_list[EP_data_dict[i][2]])\n",
    "\n",
    "        EP_data_train_dict = {}\n",
    "        EP_data_test_dict = {}\n",
    "\n",
    "        for i in EP_data_dict.keys():\n",
    "            if EP_data_dict[i][1] in test_month:\n",
    "                EP_data_test_dict[i] = EP_data_dict[i]\n",
    "            else:\n",
    "                EP_data_train_dict[i] = EP_data_dict[i]\n",
    "\n",
    "        current_input_neuron = EP_data_train_dict\n",
    "        [0].shape[0]\n",
    "\n",
    "        EP_data_train_EP_data = [item[0] for item in EP_data_train_dict.values()]\n",
    "        EP_data_train_image = [item[2] for item in EP_data_train_dict.values()]  \n",
    "        EP_data_train_feature = [item[3] for item in EP_data_train_dict.values()]\n",
    "\n",
    "        EP_data_test_EP_data = [item[0] for item in EP_data_test_dict.values()]\n",
    "        EP_data_test_image = [item[2] for item in EP_data_test_dict.values()] \n",
    "        EP_data_test_feature = [item[3] for item in EP_data_test_dict.values()]\n",
    "\n",
    "        train_dataset = EPDataset(\n",
    "            EP_data_train_EP_data, EP_data_train_image, EP_data_train_feature\n",
    "        )\n",
    "        test_dataset = EPDataset(\n",
    "            EP_data_test_EP_data, EP_data_test_image, EP_data_test_feature\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "        config = ModelConfig(input_neuron=current_input_neuron)\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = TimeTransformerConvModel(config).to(device)\n",
    "\n",
    "        image_cluster = pd.read_csv(\"image_cluster.csv\")\n",
    "        print(f'Training on slide: {date_len}, Trial: {trial}')\n",
    "        results = main_train_loop(\n",
    "            config=config,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            device=device,\n",
    "            image_cluster = image_cluster\n",
    "        )\n",
    "\n",
    "        result_dict[date_len] = results\n",
    "\n",
    "    with open(f\"/root/visual_decode/mouse5/results_control/result_control_{trial}.pkl\", 'wb') as f:\n",
    "        pickle.dump(result_dict, f)\n",
    "    print(f\"Results saved for slide {date_len} and trial {trial}.\")\n",
    "    print(\"_\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "class EP_dataset(Dataset):\n",
    "    def __init__(self, EP_folderpath, image_folderpath, batch_size=20):\n",
    "        self.EP_folderpath = EP_folderpath\n",
    "        self.image_folderpath = image_folderpath\n",
    "        self.batch_size = batch_size\n",
    "        self.progress_counter = 0\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "\n",
    "        self.EP_data_list, self.image_label_list = self.EP_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.EP_data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        EP_data = self.EP_data_list[idx]\n",
    "        EP_data = EP_data.T\n",
    "        #print(EP_data.shape)\n",
    "        label = self.image_label_list[idx]\n",
    "\n",
    "        EP_data = scaler.fit_transform(EP_data)\n",
    "        EP_data = torch.tensor(EP_data, dtype=torch.float32)\n",
    "        EP_data = self.max_pool(EP_data)\n",
    "        EP_data = EP_data.T\n",
    "\n",
    "        #if random.random() > 0.3: \n",
    "        #    mask_len = random.randint(3, 10)  \n",
    "        #    start = random.randint(0, 60 - mask_len)\n",
    "        #    EP_data[start:start+mask_len] = 0\n",
    "\n",
    "        #if random.random() > 0.5:\n",
    "        #    fft_data = torch.fft.fft(EP_data)\n",
    "        #    fft_data[10:-10] *= random.uniform(0.2, 0.8)  \n",
    "        #    EP_data = torch.fft.ifft(fft_data).real\n",
    "        \n",
    "        #if random.random() > 0.3:\n",
    "        #    mask_ch = random.randint(3, 5)\n",
    "        #    channels = random.sample(range(EP_data.shape[0]), mask_ch)\n",
    "        #    EP_data[channels] = 0\n",
    "        \n",
    "        #noise = torch.randn_like(EP_data) * 0.03 \n",
    "        #EP_data += noise\n",
    "\n",
    "        label = torch.tensor(int(label), dtype=torch.long)  \n",
    "\n",
    "        #print(EP_data.shape)\n",
    "\n",
    "\n",
    "        self.progress_counter += 1\n",
    "        if self.progress_counter % 1000 == 0:\n",
    "            print(f\"Processed {self.progress_counter}/{len(self.EP_data_list)} items\")\n",
    "\n",
    "        return EP_data, label\n",
    "\n",
    "\n",
    "    def EP_dataset(self):\n",
    "        EP_segmentation_list = os.listdir(self.EP_folderpath)\n",
    "        print(f'Total Dataset Number: {len(EP_segmentation_list)}')\n",
    "        EP_data_list = []\n",
    "        image_label_list = []\n",
    "\n",
    "        for EP_data_dir in EP_segmentation_list:\n",
    "            EP_data = np.load(os.path.join(self.EP_folderpath, EP_data_dir))\n",
    "\n",
    "            image_label = EP_data_dir.split(\"_\")[2]\n",
    "\n",
    "            EP_data_list.append(EP_data)\n",
    "            image_label_list.append(image_label)\n",
    "\n",
    "        return EP_data_list, image_label_list\n",
    "    \n",
    "Full_dataset = EP_dataset(EP_folderpath= \"/media/ubuntu/sda/data/mouse6/output/04_get_neuron_channel/segmentation_raw\", image_folderpath=\"/media/ubuntu/sda/data/trigger/NaturalImages_new_2\")\n",
    "train_dataset, test_dataset = train_test_split(Full_dataset, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "image_feature = pd.read_csv(\"/media/ubuntu/sda/data/mouse6/output/06_visual_decoding/image_feature.csv\").iloc[:, 1:]\n",
    "image_feature = torch.tensor(image_feature.to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, dropout=0.5):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        \n",
    "        self.value_embedding = nn.Linear(c_in, d_model)  \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.value_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class DualAttentionTransformer(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model)\n",
    "        \n",
    "        self.attn_layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'channel_attn': ChannelAttentionLayer(\n",
    "                    d_model=configs.d_model,\n",
    "                    n_heads=configs.n_heads\n",
    "                ),\n",
    "                'time_attn': TimeAttentionLayer(\n",
    "                    d_model=configs.d_model,\n",
    "                    n_heads=configs.n_heads \n",
    "                )\n",
    "            }) for _ in range(configs.e_layers)\n",
    "        ])\n",
    "        \n",
    "        self.channel_conv = nn.Sequential(\n",
    "            nn.Conv1d(configs.d_model, configs.d_model*2, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(configs.d_model*2, configs.d_model, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc):\n",
    "        x = self.enc_embedding(x_enc)  \n",
    "        \n",
    "        for layer in self.attn_layers:\n",
    "            x = layer['channel_attn'](x) \n",
    "            x = layer['time_attn'](x)    \n",
    "            #x = x + self.channel_conv(x.permute(0,2,1)).permute(0,2,1)\n",
    "            \n",
    "        return x.permute(0, 2, 1)  \n",
    "\n",
    "class ChannelAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            kdim=d_model,\n",
    "            vdim=d_model\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model*4, d_model),\n",
    "            nn.Dropout(dropout))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "    \n",
    "        \n",
    "        attn_output, _ = self.attention(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x\n",
    "        )\n",
    "        \n",
    "        x = self.norm(residual + self.dropout(attn_output))\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm(x + ffn_output)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class TimeAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attention(x, x, x)  \n",
    "        attn_out = self.dropout(attn_out)\n",
    "        x = self.norm(x + attn_out)\n",
    "        return x\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=20):\n",
    "        super().__init__()\n",
    "        self.channel_conv = nn.Identity()\n",
    "        \n",
    "        self.time_conv = nn.Sequential(\n",
    "            nn.Conv1d(200, 64, kernel_size=25, stride=5, padding=12),  \n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool1d(200), \n",
    "            nn.Conv1d(64, 40, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(40, emb_size),\n",
    "            nn.LayerNorm(emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = self.channel_conv(x)  \n",
    "        x = self.time_conv(x)     \n",
    "        \n",
    "        x = x.permute(0, 2, 1)    \n",
    "        return self.projection(x)\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "class FlattenHead(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class Enc_eeg(nn.Sequential):\n",
    "    def __init__(self, emb_size=20, **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(emb_size),\n",
    "            FlattenHead()\n",
    "        )\n",
    "\n",
    "class Proj_eeg(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=4000, proj_dim=1024, drop_proj=0.5):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.distributed.nn\n",
    "from torch import distributed as dist, nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "try:\n",
    "    import horovod.torch as hvd\n",
    "except ImportError:\n",
    "    hvd = None\n",
    "\n",
    "\n",
    "def gather_features(\n",
    "    image_features,\n",
    "    text_features,\n",
    "    local_loss=False,\n",
    "    gather_with_grad=False,\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    use_horovod=False,\n",
    "):\n",
    "    if use_horovod:\n",
    "        assert hvd is not None, \"Please install horovod\"\n",
    "        if gather_with_grad:\n",
    "            all_image_features = hvd.allgather(image_features)\n",
    "            all_text_features = hvd.allgather(text_features)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                all_image_features = hvd.allgather(image_features)\n",
    "                all_text_features = hvd.allgather(text_features)\n",
    "            if not local_loss:\n",
    "                # ensure grads for local rank when all_* features don't have a gradient\n",
    "                gathered_image_features = list(\n",
    "                    all_image_features.chunk(world_size, dim=0)\n",
    "                )\n",
    "                gathered_text_features = list(\n",
    "                    all_text_features.chunk(world_size, dim=0)\n",
    "                )\n",
    "                gathered_image_features[rank] = image_features\n",
    "                gathered_text_features[rank] = text_features\n",
    "                all_image_features = torch.cat(gathered_image_features, dim=0)\n",
    "                all_text_features = torch.cat(gathered_text_features, dim=0)\n",
    "    else:\n",
    "        # We gather tensors from all gpus\n",
    "        if gather_with_grad:\n",
    "            all_image_features = torch.cat(\n",
    "                torch.distributed.nn.all_gather(image_features), dim=0\n",
    "            )\n",
    "            all_text_features = torch.cat(\n",
    "                torch.distributed.nn.all_gather(text_features), dim=0\n",
    "            )\n",
    "        else:\n",
    "            gathered_image_features = [\n",
    "                torch.zeros_like(image_features) for _ in range(world_size)\n",
    "            ]\n",
    "            gathered_text_features = [\n",
    "                torch.zeros_like(text_features) for _ in range(world_size)\n",
    "            ]\n",
    "            dist.all_gather(gathered_image_features, image_features)\n",
    "            dist.all_gather(gathered_text_features, text_features)\n",
    "            if not local_loss:\n",
    "                # ensure grads for local rank when all_* features don't have a gradient\n",
    "                gathered_image_features[rank] = image_features\n",
    "                gathered_text_features[rank] = text_features\n",
    "            all_image_features = torch.cat(gathered_image_features, dim=0)\n",
    "            all_text_features = torch.cat(gathered_text_features, dim=0)\n",
    "\n",
    "    return all_image_features, all_text_features\n",
    "\n",
    "class ClipLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        local_loss=False,\n",
    "        gather_with_grad=False,\n",
    "        cache_labels=False,\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "        use_horovod=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.local_loss = local_loss\n",
    "        self.gather_with_grad = gather_with_grad\n",
    "        self.cache_labels = cache_labels\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.use_horovod = use_horovod\n",
    "\n",
    "        # cache state\n",
    "        self.prev_num_logits = 0\n",
    "        self.labels = {}\n",
    "\n",
    "    def forward(self, image_features, text_features, logit_scale):\n",
    "        device = image_features.device\n",
    "        if self.world_size > 1:\n",
    "            all_image_features, all_text_features = gather_features(\n",
    "                image_features,\n",
    "                text_features,\n",
    "                self.local_loss,\n",
    "                self.gather_with_grad,\n",
    "                self.rank,\n",
    "                self.world_size,\n",
    "                self.use_horovod,\n",
    "            )\n",
    "\n",
    "            if self.local_loss:\n",
    "                logits_per_image = logit_scale * image_features @ all_text_features.T\n",
    "                logits_per_text = logit_scale * text_features @ all_image_features.T\n",
    "            else:\n",
    "                logits_per_image = (\n",
    "                    logit_scale * all_image_features @ all_text_features.T\n",
    "                )\n",
    "                logits_per_text = logits_per_image.T\n",
    "        else:\n",
    "            logits_per_image = logit_scale * image_features @ text_features.T\n",
    "            logits_per_text = logit_scale * text_features @ image_features.T\n",
    "\n",
    "        num_logits = logits_per_image.shape[0]\n",
    "        if self.prev_num_logits != num_logits or device not in self.labels:\n",
    "            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n",
    "            if self.world_size > 1 and self.local_loss:\n",
    "                labels = labels + num_logits * self.rank\n",
    "            if self.cache_labels:\n",
    "                self.labels[device] = labels\n",
    "                self.prev_num_logits = num_logits\n",
    "        else:\n",
    "            labels = self.labels[device]\n",
    "\n",
    "        total_loss = (\n",
    "            F.cross_entropy(logits_per_image, labels)\n",
    "            + F.cross_entropy(logits_per_text, labels)\n",
    "        ) / 2\n",
    "        return total_loss\n",
    "\n",
    "class global_config:\n",
    "    def __init__(self):\n",
    "        self.task_name = 'classification'\n",
    "        self.enc_in = 30       \n",
    "        self.d_model = 200     \n",
    "        self.dropout = 0.4   \n",
    "        self.n_heads = 2\n",
    "        self.e_layers = 2   \n",
    "\n",
    "\n",
    "class EP_encoder(nn.Module):    \n",
    "    def __init__(self, global_config):\n",
    "        super().__init__()\n",
    "        self.encoder = DualAttentionTransformer(global_config)   \n",
    "        self.enc_eeg = Enc_eeg()\n",
    "        self.proj_eeg = Proj_eeg(embedding_dim=20*50*4)  \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n",
    "\n",
    "        self.loss_func = ClipLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)       \n",
    "        eeg_embedding = self.enc_eeg(x) \n",
    "        return self.proj_eeg(eeg_embedding)\n",
    "\n",
    "device = 'cuda'\n",
    "config = global_config() \n",
    "EP_encoder_v1 = EP_encoder(config).to(device)\n",
    "\n",
    "batch_size_per_gpu = 24\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size_per_gpu,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size_per_gpu,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def train_model(encoder_modle, dataloader, optimizer, device, image_feature):\n",
    "    encoder_modle.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    EP_feature_list = []\n",
    "    predict_label = []\n",
    "    actual_label = []\n",
    "    correct_top5 = 0\n",
    "    image_feature = image_feature.to(device)\n",
    "    img_features_all = image_feature.to(device).float()\n",
    "    for batch_id, (EP_data, label) in enumerate(dataloader):\n",
    "\n",
    "        EP_data = EP_data.to(device)\n",
    "        label = label.to(device)\n",
    "        batch_size = EP_data.size(0)\n",
    "        selected_image_features = image_feature[label - 1]\n",
    "        #print(selected_image_features.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        EP_feature = encoder_modle(EP_data)\n",
    "        EP_feature_list.append(EP_feature)\n",
    "        logit_scale = encoder_modle.logit_scale\n",
    "\n",
    "        EP_feature = F.normalize(EP_feature, dim=-1)\n",
    "        selected_image_features = F.normalize(selected_image_features, dim=-1)\n",
    "        \n",
    "        img_loss = encoder_modle.loss_func(selected_image_features,EP_feature, encoder_modle.logit_scale)\n",
    "        loss = img_loss\n",
    "        loss.backward()\n",
    "        \n",
    "        img_features_all = F.normalize(img_features_all, dim = -1)\n",
    "        logits_img = logit_scale * EP_feature @ img_features_all.T\n",
    "        logits_single = logits_img\n",
    "        predicted = torch.argmax(logits_single, dim=1)\n",
    "        predicted += 1\n",
    "\n",
    "\n",
    "        _, top5_predicted = torch.topk(logits_single, 5, dim=1)\n",
    "        top5_predicted += 1\n",
    "        for i in range(batch_size):\n",
    "            if label[i] in top5_predicted[i]:\n",
    "                correct_top5 += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        #print(\"train_loss:\")\n",
    "        #print(img_loss.item())\n",
    "        total_loss += img_loss.item()\n",
    "        total += batch_size\n",
    "\n",
    "        predict_label.append(predicted)\n",
    "        actual_label.append(label)\n",
    "\n",
    "    predict_label = torch.cat(predict_label, dim=0)\n",
    "    actual_label = torch.cat(actual_label, dim=0)\n",
    "\n",
    "    correct += (predict_label == actual_label).sum().item()\n",
    "\n",
    "    average_loss = total_loss / (batch_id+1)\n",
    "    accuracy = correct / total\n",
    "    accuracy_top5 = correct_top5/total\n",
    "\n",
    "    return average_loss, accuracy, torch.cat(EP_feature_list, dim=0), accuracy_top5\n",
    "    \n",
    "def evaluate_model(encoder_modle, dataloader, device, image_feature):\n",
    "    encoder_modle.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    EP_feature_list = []\n",
    "    predict_label = []\n",
    "    actual_label = []\n",
    "    image_feature = image_feature.to(device)\n",
    "    img_features_all = image_feature.to(device).float()\n",
    "    correct_top5 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_id, (EP_data, label) in enumerate(dataloader):\n",
    "\n",
    "            EP_data = EP_data.to(device)\n",
    "            label = label.to(device)\n",
    "            selected_image_features = image_feature[label - 1]\n",
    "            batch_size = EP_data.size(0)\n",
    "            logit_scale = encoder_modle.logit_scale\n",
    "\n",
    "            EP_feature = encoder_modle(EP_data)\n",
    "\n",
    "            EP_feature_list.append(EP_feature)\n",
    "\n",
    "            EP_feature = F.normalize(EP_feature, dim=-1)\n",
    "            selected_image_features = F.normalize(selected_image_features, dim=-1)\n",
    "            \n",
    "            img_loss = encoder_modle.loss_func(selected_image_features,EP_feature, encoder_modle.logit_scale)\n",
    "            \n",
    "            total_loss += img_loss.item()\n",
    "\n",
    "            img_features_all = F.normalize(img_features_all, dim = -1)\n",
    "            logits_img = logit_scale * EP_feature @ img_features_all.T\n",
    "            logits_single = logits_img\n",
    "            predicted = torch.argmax(logits_single, dim=1)\n",
    "            predicted += 1\n",
    "\n",
    "            _, top5_predicted = torch.topk(logits_single, 5, dim=1)\n",
    "            top5_predicted += 1\n",
    "            for i in range(batch_size):\n",
    "                if label[i] in top5_predicted[i]:\n",
    "                    correct_top5 += 1\n",
    "                    \n",
    "            total += batch_size\n",
    "    \n",
    "            predict_label.append(predicted)\n",
    "            actual_label.append(label)\n",
    "    \n",
    "        predict_label = torch.cat(predict_label, dim=0)\n",
    "        actual_label = torch.cat(actual_label, dim=0)\n",
    "    \n",
    "        correct += (predict_label == actual_label).sum().item()\n",
    "    \n",
    "        average_loss = total_loss / (batch_id+1)\n",
    "        accuracy = correct / total\n",
    "        accuracy_top5 = correct_top5/total\n",
    "\n",
    "        return average_loss, accuracy, predict_label, actual_label, accuracy_top5\n",
    "        \n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': EP_encoder_v1.encoder.parameters(), 'lr': 1e-3},\n",
    "    {'params': EP_encoder_v1.proj_eeg.parameters(), 'lr': 5e-3}\n",
    "], weight_decay=0.001)  \n",
    "\n",
    "scheduler = optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[\n",
    "        optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=10),\n",
    "        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "    ],\n",
    "    milestones=[10]\n",
    ")\n",
    "\n",
    "def main_train_loop(date, encoder_modle, train_loader, test_loader, optimizer, device, config, image_feature):\n",
    "    print('start train')\n",
    "    train_losses, train_accuracies = [], []\n",
    "    test_losses, test_accuracies = [], []\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(30):\n",
    "        print(epoch + 1)\n",
    "        train_loss, train_accuracy, features_tensor, train_accuracy_top5 = train_model(encoder_modle, train_loader, optimizer, device, image_feature)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        print('end_train')\n",
    "        test_loss, test_accuracy, test_predict, test_actual, accuracy_top5 = evaluate_model(encoder_modle, test_loader, device, image_feature)\n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch +1) % 5 == 0:     \n",
    "            print(f'Epoch: {epoch + 1}')\n",
    "            os.makedirs(f\"./models/'classification'/{date}\", exist_ok=True)             \n",
    "            file_path = f\"./models/'classification'/{date}/{epoch+1}.pth\"\n",
    "            torch.save(encoder_modle.state_dict(), file_path)  \n",
    "\n",
    "\n",
    "            print(f\"model saved in {file_path}!\")\n",
    "        \n",
    "        print(f\"train loss: {train_loss} at epoch: {epoch + 1}\")\n",
    "        print(f\"train accuracy: {train_accuracy} at epoch: {epoch + 1}\")\n",
    "        print(f\"train top5-accuracy: {train_accuracy_top5} at epoch: {epoch + 1}\")\n",
    "        print(f\"test loss: {test_loss} at epoch: {epoch + 1}\")\n",
    "        print(f\"test accuracy: {test_accuracy} at epoch: {epoch + 1}\")\n",
    "        print(f\"test top5-accuracy: {accuracy_top5} at epoch: {epoch + 1}\")\n",
    "        print(\"----------\")\n",
    "            \n",
    "        epoch_results = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_predict\": test_predict,\n",
    "        \"test_actual\": test_actual\n",
    "        }\n",
    "    \n",
    "    results.append(epoch_results)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Loss curve\n",
    "    axs[0].plot(train_losses, label='Train Loss')\n",
    "    axs[0].plot(test_losses, label='Test Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].set_title(\"Loss Curve\")\n",
    "\n",
    "    # Overall accuracy curve\n",
    "    axs[1].plot(train_accuracies, label='Train Accuracy')\n",
    "    axs[1].plot(test_accuracies, label='Test Accuracy')\n",
    "    axs[1].legend()\n",
    "    axs[1].set_title(\"Accuracy Curve\")\n",
    "\n",
    "    plt.savefig(f'{date}_curve.png')\n",
    "\n",
    "    return results\n",
    "\n",
    "results = main_train_loop(encoder_modle=EP_encoder_v1, train_loader=train_loader, test_loader=test_loader, optimizer=optimizer, device=device, config=global_config,date='20250309_raw', image_feature=image_feature)\n",
    "\n",
    "import pickle\n",
    "with open('results_20250309_raw.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

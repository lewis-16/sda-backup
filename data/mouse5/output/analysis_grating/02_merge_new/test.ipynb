{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/spike_sorting_jct/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from kilosort.io import load_ops\n",
    "import sys\n",
    "import spikeinterface as si\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.widgets as sw\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "global_job_kwargs = dict(n_jobs = 4)\n",
    "si.set_global_job_kwargs(**global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir(\"/media/ubuntu/sda/data/mouse5/output/analysis_grating/00_sorter_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_num_test(test_num):\n",
    "    import os\n",
    "    for date in os.listdir(\"/media/ubuntu/sda/data/mouse5/output/analysis_grating/00_sorter_output\"):\n",
    "\n",
    "        def get_spike_inf(file_path):\n",
    "            cluster_inf = pd.read_csv(file_path + \"/analyzer_kilosort4_binary/extensions/quality_metrics/metrics.csv\")\n",
    "            cluster_inf.columns = ['cluster', 'num_spikes', 'firing_rate', 'presence_ratio', 'snr',\n",
    "            'isi_violations_ratio', 'isi_violations_count', 'rp_contamination',\n",
    "            'rp_violations', 'sliding_rp_violation', 'amplitude_cutoff',\n",
    "            'amplitude_median', 'amplitude_cv_median', 'amplitude_cv_range',\n",
    "            'sync_spike_2', 'sync_spike_4', 'sync_spike_8', 'firing_range',\n",
    "            'drift_ptp', 'drift_std', 'drift_mad', 'sd_ratio']\n",
    "            cluster_inf['cluster'] = cluster_inf['cluster'].astype(str)\n",
    "            cluster_inf['position_1'] = None\n",
    "            cluster_inf['position_2'] = None\n",
    "\n",
    "            def get_best_channels(results_dir):\n",
    "                \"\"\"Get channel numbers with largest template norm for each cluster.\"\"\"\n",
    "                templates = np.load(results_dir + '/templates.npy')\n",
    "                best_chans = (templates**2).sum(axis=1).argmax(axis=-1)\n",
    "                return best_chans\n",
    "            \n",
    "            def get_six_best_channels(results_dir):\n",
    "                \"\"\"Get channel numbers with largest template norm for each cluster.\"\"\"\n",
    "                templates = np.load(results_dir + '/templates.npy')\n",
    "                template_norms = (templates ** 2).sum(axis=1)\n",
    "                best_chans = np.argsort(template_norms, axis=-1)[:, -6:][:, ::-1]\n",
    "                \n",
    "                return best_chans\n",
    "            best_chans = get_best_channels(results_dir=file_path + \"/kilosort4/sorter_output\")\n",
    "            best_six_chans = get_six_best_channels(results_dir=file_path + \"/kilosort4/sorter_output\")\n",
    "            cluster_inf['best_chans'] = best_chans\n",
    "            cluster_inf['best_six_chans'] = best_six_chans.tolist()\n",
    "\n",
    "            spike_clusters = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/spike_clusters.npy\").astype(str))\n",
    "            spike_positions = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/spike_positions.npy\").astype(float))\n",
    "            spike_templates = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/spike_templates.npy\"))\n",
    "            spike_times = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/spike_times.npy\").astype(int))\n",
    "            tf = pd.DataFrame(np.load(file_path + \"/kilosort4/sorter_output/tF.npy\")[:, 0, :])\n",
    "\n",
    "            spike_inf = pd.concat((spike_clusters, spike_positions, spike_templates, spike_times, tf), axis=1)\n",
    "            spike_inf.columns = ['cluster', 'position_1', 'position_2', 'templates', 'time', 'PC_1', 'PC_2', 'PC_3', 'PC_4', 'PC_5', 'PC_6']\n",
    "\n",
    "            for i in spike_inf['cluster'].value_counts().index:\n",
    "                temp = spike_inf[spike_inf['cluster'] == i]\n",
    "                cluster_inf.loc[cluster_inf['cluster'] == i, 'position_1'] = np.mean(temp['position_1'])\n",
    "                cluster_inf.loc[cluster_inf['cluster'] == i, 'position_2'] = np.mean(temp['position_2'])\n",
    "            \n",
    "            cluster_inf['probe_group'] = \"1\"\n",
    "\n",
    "            for i in spike_inf['cluster'].value_counts().index:\n",
    "                cluster_rows = cluster_inf[cluster_inf['cluster'] == i]\n",
    "                if (cluster_rows['position_1'] > 100).any() and (cluster_rows['position_1'] < 250).any():\n",
    "                    cluster_inf.loc[cluster_inf['cluster'] == i, 'probe_group'] = \"2\"\n",
    "                elif (cluster_rows['position_1'] > 250).any() and (cluster_rows['position_1'] < 400).any():\n",
    "                    cluster_inf.loc[cluster_inf['cluster'] == i, 'probe_group'] = \"3\"\n",
    "                elif (cluster_rows['position_1'] > 400).any() and (cluster_rows['position_1'] < 550).any():\n",
    "                    cluster_inf.loc[cluster_inf['cluster'] == i, 'probe_group'] = \"4\"\n",
    "                elif (cluster_rows['position_1'] > 550).any() :\n",
    "                    cluster_inf.loc[cluster_inf['cluster'] == i, 'probe_group'] = \"5\"\n",
    "            \n",
    "            spike_inf['neuron'] = None\n",
    "\n",
    "            cluster_potent_inf = cluster_inf[(cluster_inf['snr'] > 3) | ((cluster_inf['num_spikes'] > 5000) & (cluster_inf['num_spikes'] <= 8000))]\n",
    "            cluster_inf = cluster_inf[((cluster_inf['snr'] > 3) & (cluster_inf['num_spikes'] > int(test_num))) | ((cluster_inf['snr'] < 3) & (cluster_inf['num_spikes'] > 8000))]\n",
    "\n",
    "            spike_potent_inf = spike_inf[spike_inf['cluster'].isin(list(cluster_potent_inf['cluster']))]\n",
    "            spike_inf = spike_inf[spike_inf['cluster'].isin(list(cluster_inf['cluster']))]\n",
    "\n",
    "            return cluster_inf, spike_inf, cluster_potent_inf, spike_potent_inf\n",
    "\n",
    "        cluster_inf, spike_inf, cluster_potent_inf, spike_potent_inf = get_spike_inf(file_path=f\"/media/ubuntu/sda/data/mouse5/output/analysis_grating/00_sorter_output/{date}\")\n",
    "        similarity = np.load(f\"/media/ubuntu/sda/data/mouse5/output/analysis_grating/00_sorter_output/{date}/kilosort4/sorter_output/similar_templates.npy\")\n",
    "\n",
    "        similarity = pd.DataFrame(similarity, columns=range(len(similarity)))\n",
    "        similarity.index = range(len(similarity))\n",
    "\n",
    "        similarity = similarity.loc[cluster_inf['cluster'].value_counts().index.astype(int), cluster_inf['cluster'].value_counts().index.astype(int)]\n",
    "        indices = np.where((similarity > 0.9) & (similarity < 1))\n",
    "        pairs = list(zip(similarity.index[indices[0]], similarity.columns[indices[1]]))\n",
    "        lower_triangle_pairs = [(row, col) for row, col in pairs if similarity.index.get_loc(row) < similarity.columns.get_loc(col)]\n",
    "        pairs = [[str(row), str(col)] for row, col in lower_triangle_pairs]\n",
    "\n",
    "        for a, b in pairs:\n",
    "            row_a = cluster_inf[cluster_inf['cluster'] == a]\n",
    "            row_b = cluster_inf[cluster_inf['cluster'] == b]\n",
    "\n",
    "            if not row_a.empty and not row_b.empty:\n",
    "                cluster_inf.loc[cluster_inf['cluster'] == a, 'num_spikes'] += row_b['num_spikes'].values[0]\n",
    "                cluster_inf = cluster_inf[cluster_inf['cluster'] != b]\n",
    "                spike_inf.loc[spike_inf['cluster'] == b, 'cluster'] = a\n",
    "\n",
    "        cluster_list = []\n",
    "        for cluster in spike_inf['cluster'].value_counts().index:\n",
    "            temp = spike_inf[spike_inf['cluster'] == cluster]\n",
    "            has_positive = (temp['PC_1'] > 0).sum()\n",
    "            has_negative = (temp['PC_1'] < 0).sum()\n",
    "            if has_positive > 2000 and has_negative > 500:\n",
    "                cluster_list.append(cluster)\n",
    "        cluster_list\n",
    "\n",
    "        spike_inf['cluster_devide'] = spike_inf['cluster']\n",
    "\n",
    "        for cluster in cluster_list:\n",
    "            spike_inf.loc[(spike_inf['PC_1'] > 0) & (spike_inf['cluster'] == cluster), 'cluster_devide'] = f'{cluster}_1'\n",
    "\n",
    "            temp = spike_inf[spike_inf['cluster'] == cluster]\n",
    "            cluster_inf.loc[cluster_inf['cluster'] == cluster, 'num_spikes'] = temp['cluster_devide'].value_counts()[0]\n",
    "            cluster_temp = cluster_inf[cluster_inf['cluster'] == cluster]\n",
    "            cluster_temp['cluster'] = f'{cluster}_1'\n",
    "            cluster_temp['num_spikes'] = temp['cluster_devide'].value_counts()[1]\n",
    "            cluster_inf = pd.concat((cluster_inf, cluster_temp))\n",
    "        spike_inf['cluster'] = spike_inf['cluster_devide']\n",
    "\n",
    "        def get_spike_waveforms(spikes, results_dir, bfile=None, chan=None):\n",
    "            \"\"\"Get waveform for each spike in `spikes`, multi- or single-channel.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            spikes : list or array-like\n",
    "                Spike times (in units of samples) for the desired waveforms, from\n",
    "                `spike_times.npy`.\n",
    "            results_dir : str or Path\n",
    "                Path to directory where Kilosort4 sorting results were saved.\n",
    "            bfile : kilosort.io.BinaryFiltered; optional\n",
    "                Kilosort4 data file object. By default, this will be loaded using the\n",
    "                information in `ops.npy` in the saved results.\n",
    "            chan : int; optional.\n",
    "                Channel to use for single-channel waveforms. If not specified, all\n",
    "                channels will be returned.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            waves : np.ndarray\n",
    "                Array of spike waveforms with shape `(nt, len(spikes))`.\n",
    "            \n",
    "            \"\"\"\n",
    "            if isinstance(spikes, int):\n",
    "                spikes = [spikes]\n",
    "\n",
    "            if bfile is None:\n",
    "                ops = io.load_ops(results_dir + '/ops.npy')\n",
    "                bfile = io.bfile_from_ops(ops)\n",
    "            whitening_mat_inv = np.load(results_dir + '/whitening_mat_inv.npy')\n",
    "\n",
    "            waves = []\n",
    "            for t in spikes:\n",
    "                tmin = t - bfile.nt0min\n",
    "                tmax = t + (bfile.nt - bfile.nt0min)\n",
    "                w = bfile[tmin:tmax].cpu().numpy()\n",
    "                if whitening_mat_inv is not None:\n",
    "                    w = whitening_mat_inv @ w\n",
    "                if w.shape[1] == bfile.nt:\n",
    "                    # Don't include spikes at the start or end of the recording that\n",
    "                    # get truncated to fewer time points.\n",
    "                    waves.append(w)\n",
    "            waves = np.stack(waves, axis=-1)\n",
    "\n",
    "            if chan is not None:\n",
    "                waves = waves[chan,:]\n",
    "            \n",
    "            bfile.close()\n",
    "\n",
    "            return waves\n",
    "\n",
    "        spike_inf = spike_inf[spike_inf['time'] > 200]\n",
    "        from kilosort import io\n",
    "        waveform_dict = {}\n",
    "        for cluster in cluster_inf['cluster'].value_counts().index:\n",
    "            temp = spike_inf[spike_inf['cluster_devide'] == cluster]\n",
    "            best_chan = cluster_inf.loc[cluster_inf['cluster'] == cluster, 'best_chans']\n",
    "            waveform_dict[cluster] = get_spike_waveforms(spikes=list(temp['time']), results_dir=f\"/media/ubuntu/sda/data/mouse5/output/analysis_grating/00_sorter_output/{date}/kilosort4/sorter_output\", chan=best_chan)\n",
    "\n",
    "        waveform_mean = pd.DataFrame()\n",
    "        for cluster, df in waveform_dict.items():\n",
    "            waveform_mean = pd.concat((waveform_mean, pd.DataFrame(np.mean(df, axis=2)[0], columns=[cluster])), axis=1)\n",
    "\n",
    "        os.makedirs(f\"/media/ubuntu/sda/data/mouse5/output/analysis_grating//{test_num}/{date}\", exist_ok=True)\n",
    "        cluster_inf.to_csv(f\"/media/ubuntu/sda/data/mouse5/output/analysis_grating//{test_num}/{date}/cluster_inf.tsv\", sep = '\\t')\n",
    "        spike_inf.to_csv(f\"/media/ubuntu/sda/data/mouse5/output/analysis_grating//{test_num}/{date}/spike_inf.tsv\", sep='\\t')\n",
    "        cluster_potent_inf.to_csv(f\"/media/ubuntu/sda/data/mouse5/output/analysis_grating//{test_num}/{date}/cluster_potent_inf.tsv\", sep='\\t')\n",
    "        waveform_mean.to_csv(f\"/media/ubuntu/sda/data/mouse5/output/analysis_grating//{test_num}/{date}/waveform_mean.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_num_test(test_num = '5000')\n",
    "#threshold_num_test(test_num = '6000')\n",
    "#threshold_num_test(test_num = '7000')\n",
    "#threshold_num_test(test_num = '8000')\n",
    "#threshold_num_test(test_num = '9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_num_test(test_num = '6000')\n",
    "threshold_num_test(test_num = '7000')\n",
    "threshold_num_test(test_num = '8000')\n",
    "threshold_num_test(test_num = '9000')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import csv\n",
    "from torch import Tensor\n",
    "import itertools\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPDataset(Dataset):\n",
    "    def __init__(self, EP_data, labels, features):  \n",
    "        self.EP_data = EP_data\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.EP_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        EP_tensor = torch.tensor(self.EP_data[idx].T, dtype=torch.float32) \n",
    "        label = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "        feature = self.features[idx]\n",
    "                \n",
    "        return EP_tensor, label, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self,\n",
    "                 input_neuron=25,        \n",
    "                 time_bins=20,          \n",
    "                 d_model = 150,          \n",
    "                 nhead=10,                \n",
    "                 num_transformer_layers=1, \n",
    "                 conv_channels=64,      \n",
    "                 num_conv_blocks=3,      \n",
    "                 num_classes=117,        \n",
    "                 residual_dims=[256, 512, 1024], \n",
    "                 use_positional_encoding=True,  \n",
    "                 dim_feedforward_ratio=4,      \n",
    "                 activation='relu',\n",
    "                 use_neuron_masking=True,  \n",
    "                 mask_ratio=0,\n",
    "                 mask_replacement='zero',\n",
    "                 epochs = 100):\n",
    "        \n",
    "        # Transformer \n",
    "        self.transformer = {\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_layers': num_transformer_layers,\n",
    "            'dim_feedforward': d_model * dim_feedforward_ratio,\n",
    "            'activation': activation\n",
    "        }\n",
    "        \n",
    "        # cnn\n",
    "        self.convolution = {\n",
    "            'channels': conv_channels,\n",
    "            'num_blocks': num_conv_blocks,\n",
    "            'kernel_size': (3, 3),\n",
    "            'pool_size': (2, 2)\n",
    "        }\n",
    "        \n",
    "        # resnet\n",
    "        self.residual = {\n",
    "            'dims': residual_dims,\n",
    "            'skip_connection': True\n",
    "        }\n",
    "        \n",
    "        self.masking = {\n",
    "            'enabled': use_neuron_masking,\n",
    "            'ratio': mask_ratio,\n",
    "            'replacement': mask_replacement\n",
    "        }\n",
    "\n",
    "        self.input_dim = input_neuron\n",
    "        self.time_steps = time_bins\n",
    "        self.num_classes = num_classes\n",
    "        self.positional_encoding = use_positional_encoding\n",
    "        self.lr = 2e-4\n",
    "        self.epochs = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def detect_lost_neurons(ep_data_list):\n",
    "    if not ep_data_list:\n",
    "        return np.zeros(25, dtype=bool)\n",
    "    \n",
    "    all_data = np.stack(ep_data_list)\n",
    "    neuron_activity = np.sum(np.abs(all_data), axis=(0, 2))\n",
    "    return neuron_activity == 0\n",
    "    \n",
    "class NeuronMasker(nn.Module):\n",
    "    def __init__(self, mask_ratio=0.15, replacement='random'):\n",
    "        super().__init__()\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.replacement = replacement\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            if x is None:\n",
    "                raise ValueError(\"Input tensor x is None\")\n",
    "                \n",
    "            batch_size, seq_len, feat_dim = x.shape\n",
    "            mask = torch.rand(batch_size, 1, feat_dim, device=x.device) < self.mask_ratio\n",
    "            mask = mask.expand_as(x)\n",
    "            \n",
    "            if self.replacement == 'zero':\n",
    "                x_masked = x.masked_fill(mask, 0)\n",
    "            elif self.replacement == 'random':\n",
    "                random_values = torch.randn_like(x) * 0.02\n",
    "                x_masked = x.masked_scatter(mask, random_values)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid replacement: {self.replacement}\")\n",
    "            \n",
    "            return x_masked \n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class ResidualLinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.downsample = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x + residual\n",
    "\n",
    "class TimeTransformerConvModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.input_proj = nn.Linear(config.input_dim, config.transformer['d_model'])\n",
    "        self.pos_encoder = PositionalEncoding(config.transformer['d_model']) if config.positional_encoding else nn.Identity()\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.transformer['d_model'],\n",
    "            nhead=config.transformer['nhead'],\n",
    "            dim_feedforward=config.transformer['dim_feedforward'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, config.transformer['num_layers'])\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential()\n",
    "        in_channels = 1\n",
    "        for _ in range(config.convolution['num_blocks']):\n",
    "            self.conv_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, config.convolution['channels'], \n",
    "                            kernel_size=config.convolution['kernel_size'], padding='same'),\n",
    "                    nn.BatchNorm2d(config.convolution['channels']),\n",
    "                    nn.ELU(),\n",
    "                    nn.MaxPool2d(kernel_size=config.convolution['pool_size'])\n",
    "                )\n",
    "            )\n",
    "            in_channels = config.convolution['channels']\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(config.convolution['channels'], config.num_classes)\n",
    "        \n",
    "        self.residual_layers = nn.Sequential()\n",
    "        current_dim = config.convolution['channels']\n",
    "        for dim in config.residual['dims']:\n",
    "            self.residual_layers.append(ResidualLinearBlock(current_dim, dim))\n",
    "            current_dim = dim\n",
    "        if current_dim != 1024:\n",
    "            self.residual_layers.append(nn.Linear(current_dim, 1024))\n",
    "            self.residual_layers.append(nn.LayerNorm(1024))\n",
    "\n",
    "\n",
    "        self.masker = NeuronMasker(\n",
    "            mask_ratio=self.config.masking['ratio'],\n",
    "            replacement=self.config.masking['replacement']\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.masker(x)  # [B, T, D]\n",
    "        #print(x)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        features = self.residual_layers(x)\n",
    "        \n",
    "        return logits, features\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.3, temp=0.07):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha      # 分类损失权重\n",
    "        self.temp = temp\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.temp = temp\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def contrastive_loss(self, h_neuro, h_img):\n",
    "        h_neuro = F.normalize(h_neuro, dim=1) + 1e-10\n",
    "        h_img = F.normalize(h_img, dim=1) + 1e-10\n",
    "        \n",
    "        logits_ab = torch.matmul(h_neuro, h_img.T) / self.temp\n",
    "        logits_ba = torch.matmul(h_img, h_neuro.T) / self.temp\n",
    "        \n",
    "        labels = torch.arange(h_neuro.size(0), device=h_neuro.device)\n",
    "        loss_ab = F.cross_entropy(logits_ab, labels)\n",
    "        loss_ba = F.cross_entropy(logits_ba, labels)\n",
    "        \n",
    "        return (loss_ab + loss_ba) / 2\n",
    "    \n",
    "    def forward(self, logits, labels, img_feature, features):\n",
    "        loss_cls = self.ce_loss(logits, labels)\n",
    "        loss_cont = self.contrastive_loss(features, img_feature)\n",
    "        total_loss = self.alpha * loss_cls + (1 - self.alpha) * loss_cont\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, device, criterion, config, use_distributed=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # 如果是分布式训练，设置epoch\n",
    "    if use_distributed and hasattr(dataloader.sampler, 'set_epoch'):\n",
    "        dataloader.sampler.set_epoch(0)  # 这里可以根据epoch参数调整\n",
    "    \n",
    "    for batch_idx, (neuro, labels, img_feature) in enumerate(dataloader):\n",
    "        neuro = neuro.to(device)\n",
    "        labels = labels.to(device)\n",
    "        img_feature = img_feature.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 如果是DDP模型，使用module属性\n",
    "        if hasattr(model, 'module'):\n",
    "            logits, features = model.module(neuro)\n",
    "        else:\n",
    "            logits, features = model(neuro)\n",
    "            \n",
    "        loss = criterion(logits, labels, img_feature, features)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), \n",
    "            max_norm=3.0,                   \n",
    "            norm_type=2.0                   \n",
    "        )\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计指标\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_loss = total_loss / len(dataloader)\n",
    "    train_accuracy = correct / total\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device, criterion, config, image_cluster, use_distributed=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    correct_2way = 0\n",
    "    correct_4way = 0\n",
    "    correct_10way = 0\n",
    "    total = 0\n",
    "    \n",
    "    # 从 DataFrame 创建聚类映射字典\n",
    "    cluster_2_map = torch.tensor(image_cluster[\"2_cluster\"].values, device=device)\n",
    "    cluster_4_map = torch.tensor(image_cluster[\"4_cluster\"].values, device=device)\n",
    "    cluster_10_map = torch.tensor(image_cluster[\"10_cluster\"].values, device=device)\n",
    "    \n",
    "    for neuro, labels, img_feature in dataloader:\n",
    "        neuro = neuro.to(device)\n",
    "        labels = labels.to(device)\n",
    "        img_feature = img_feature.to(device)\n",
    "        \n",
    "        # 如果是DDP模型，使用module属性\n",
    "        if hasattr(model, 'module'):\n",
    "            logits, features = model.module(neuro)\n",
    "        else:\n",
    "            logits, features = model(neuro)\n",
    "        \n",
    "        loss = criterion(logits, labels, img_feature, features)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # （Top-1 和 Top-5）\n",
    "        _, predicted_top1 = torch.max(logits, 1)\n",
    "        correct_top1 += (predicted_top1 == labels).sum().item()\n",
    "        _, predicted_top5 = logits.topk(5, dim=1)\n",
    "        correct_top5 += torch.sum(predicted_top5.eq(labels.view(-1, 1))).item()\n",
    "        \n",
    "        # (2-way, 4-way, 10-way)\n",
    "        cluster_2_pred = cluster_2_map[predicted_top1]\n",
    "        cluster_4_pred = cluster_4_map[predicted_top1]\n",
    "        cluster_10_pred = cluster_10_map[predicted_top1]\n",
    "        \n",
    "        cluster_2_true = cluster_2_map[labels]\n",
    "        cluster_4_true = cluster_4_map[labels]\n",
    "        cluster_10_true = cluster_10_map[labels]\n",
    "        \n",
    "        correct_2way += (cluster_2_pred == cluster_2_true).sum().item()\n",
    "        correct_4way += (cluster_4_pred == cluster_4_true).sum().item()\n",
    "        correct_10way += (cluster_10_pred == cluster_10_true).sum().item()\n",
    "        \n",
    "        total += labels.size(0)\n",
    "    \n",
    "    # 如果是分布式训练，需要同步所有进程的结果\n",
    "    if use_distributed:\n",
    "        import torch.distributed as dist\n",
    "        # 创建tensor来收集所有进程的结果\n",
    "        metrics = torch.tensor([total_loss, correct_top1, correct_top5, correct_2way, correct_4way, correct_10way, total], device=device)\n",
    "        dist.all_reduce(metrics, op=dist.ReduceOp.SUM)\n",
    "        total_loss, correct_top1, correct_top5, correct_2way, correct_4way, correct_10way, total = metrics.tolist()\n",
    "    \n",
    "    test_loss = total_loss / len(dataloader)\n",
    "    test_accuracy = correct_top1 / total\n",
    "    top5_accuracy = correct_top5 / total\n",
    "    accuracy_2way = correct_2way / total\n",
    "    accuracy_4way = correct_4way / total\n",
    "    accuracy_10way = correct_10way / total\n",
    "    \n",
    "    return test_loss, test_accuracy, top5_accuracy, accuracy_2way, accuracy_4way, accuracy_10way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_loop(config, model, train_loader, test_loader, device, image_cluster):\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "    criterion = MultitaskLoss(alpha=0.7, temp=0.07)\n",
    "    \n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs, test_top5, acc_2way, acc_4way, acc_10way = [], [], [], [], [], []\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss, train_acc = train_model(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        test_loss, test_acc, top5_acc, accuracy_2way, accuracy_4way, accuracy_10way = evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=test_loader,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            config=config,\n",
    "            image_cluster = image_cluster\n",
    "        )\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        test_top5.append(top5_acc)\n",
    "        acc_2way.append(accuracy_2way)\n",
    "        acc_4way.append(accuracy_4way)\n",
    "        acc_10way.append(accuracy_10way)\n",
    "        \n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        # 打印日志\n",
    "        #print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        #print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%}\")\n",
    "        #print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2%} | Top-5 Acc: {top5_acc:.2%}\")\n",
    "        #print(f\"2-way Acc: {accuracy_2way:.2%} | 4-way Acc: {accuracy_4way:.2%} | 10-way Acc: {accuracy_10way:.2%}\")\n",
    "        #print(\"-\" * 60)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(test_losses, label=\"Test\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label=\"Train Acc\")\n",
    "    plt.plot(test_accs, label=\"Test Acc\")\n",
    "    plt.plot(test_top5, label=\"Test Top-5\")\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        \"best_test_acc\": best_acc,\n",
    "        \"final_top5_acc\": test_top5[-1],\n",
    "        \"train_history\": {\n",
    "            \"loss\": train_losses,\n",
    "            \"accuracy\": train_accs\n",
    "        },\n",
    "        \"test_history\": {\n",
    "            \"loss\": test_losses,\n",
    "            \"accuracy\": test_accs,\n",
    "            \"top5_accuracy\": test_top5,\n",
    "            \"acc_2way\": acc_2way,\n",
    "            \"acc_4way\": acc_4way,\n",
    "            \"acc_10way\": acc_10way\n",
    "        },\n",
    "        \"best_epoch\": best_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_order = ['021322', '022522', '031722', '042422', \n",
    "              '052422', '062422', '072322', '082322', \n",
    "              '092422', '102122', '112022', '122022', \n",
    "              #'012123', \n",
    "              '022223', '032123', '042323']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_feature.pkl not found, using dummy features\n",
      "Loaded 18456 trials\n",
      "Spike data shape: (18, 40)\n",
      "Date range: 21322 to 122022\n",
      "Image range: 1 to 117\n"
     ]
    }
   ],
   "source": [
    "# 新的数据加载代码 - 适配mouse6数据格式\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_mouse6_data():\n",
    "    \"\"\"加载mouse6的数据\"\"\"\n",
    "    # 加载spike rate数据\n",
    "    with open(\"/media/ubuntu/sda/data/paper_architecture/02_consistency/data_for_train/mouse6/trial_spike_rate_data.pkl\", 'rb') as f:\n",
    "        trial_spike_rate_data = pickle.load(f)\n",
    "    \n",
    "    # 加载日期和图像标签\n",
    "    with open(\"/media/ubuntu/sda/data/paper_architecture/02_consistency/data_for_train/mouse6/trial_date.pkl\", 'rb') as f:\n",
    "        trial_date = pickle.load(f)\n",
    "    \n",
    "    with open(\"/media/ubuntu/sda/data/paper_architecture/02_consistency/data_for_train/mouse6/trial_image.pkl\", 'rb') as f:\n",
    "        trial_image = pickle.load(f)\n",
    "    \n",
    "    # 加载图像特征（如果存在）\n",
    "    try:\n",
    "        with open(\"image_feature.pkl\", 'rb') as f:\n",
    "            image_feature_list = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"image_feature.pkl not found, using dummy features\")\n",
    "        # 创建虚拟特征（117个图像，每个图像512维特征）\n",
    "        image_feature_list = [np.random.randn(512) for _ in range(117)]\n",
    "    \n",
    "    return trial_spike_rate_data, trial_date, trial_image, image_feature_list\n",
    "\n",
    "def prepare_train_test_data(trial_spike_rate_data, trial_date, trial_image, image_feature_list, test_dates):\n",
    "    \"\"\"准备训练和测试数据\"\"\"\n",
    "    # 创建DataFrame\n",
    "    data_df = pd.DataFrame({\n",
    "        'spike_data': trial_spike_rate_data,\n",
    "        'date': trial_date,\n",
    "        'image': trial_image\n",
    "    })\n",
    "    \n",
    "    # 添加特征\n",
    "    data_df['feature'] = [image_feature_list[img] for img in data_df['image']]\n",
    "    \n",
    "    # 分割训练和测试数据\n",
    "    train_data = data_df[~data_df['date'].isin(test_dates)]\n",
    "    test_data = data_df[data_df['date'].isin(test_dates)]\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# 定义日期顺序（从consistency notebook复制）\n",
    "date_order = ['21322', '22522', '31722', '42422', \n",
    "              '52422', '062422', '72322', '82322', \n",
    "              '92422', '102122', '112022', '122022', \n",
    "              '22223', '32123', '42323']\n",
    "\n",
    "# 加载数据\n",
    "trial_spike_rate_data, trial_date, trial_image, image_feature_list = load_mouse6_data()\n",
    "\n",
    "print(f\"Loaded {len(trial_spike_rate_data)} trials\")\n",
    "print(f\"Spike data shape: {trial_spike_rate_data[0].shape}\")\n",
    "print(f\"Date range: {min(trial_date)} to {max(trial_date)}\")\n",
    "print(f\"Image range: {min(trial_image)} to {max(trial_image)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on month: [22522]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results_dict\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 运行训练\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m results_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_mouse6_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# # 保存结果\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# import pickle\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# with open(\"/media/ubuntu/sda/data/paper_architecture/02_consistency/mouse6_training_results.pkl\", \"wb\") as f:\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#     pickle.dump(results_dict, f)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# print(\"Training completed and results saved!\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m, in \u001b[0;36mtrain_with_mouse6_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting on month: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_month\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 准备训练和测试数据\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_train_test_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_spike_rate_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_feature_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_month\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trials\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trials\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 41\u001b[0m, in \u001b[0;36mprepare_train_test_data\u001b[0;34m(trial_spike_rate_data, trial_date, trial_image, image_feature_list, test_dates)\u001b[0m\n\u001b[1;32m     34\u001b[0m data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspike_data\u001b[39m\u001b[38;5;124m'\u001b[39m: trial_spike_rate_data,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: trial_date,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: trial_image\n\u001b[1;32m     38\u001b[0m })\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 添加特征\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mimage_feature_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 分割训练和测试数据\u001b[39;00m\n\u001b[1;32m     44\u001b[0m train_data \u001b[38;5;241m=\u001b[39m data_df[\u001b[38;5;241m~\u001b[39mdata_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(test_dates)]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 主训练循环 - 适配新数据格式\n",
    "def train_with_mouse6_data():\n",
    "    \"\"\"使用mouse6数据进行训练\"\"\"\n",
    "    results_dict = {}\n",
    "    \n",
    "    # 遍历每个测试月份\n",
    "    for slide in range(1, len(date_order)): \n",
    "        test_month = [int(date_order[slide])]  # 转换为整数以匹配数据格式\n",
    "        \n",
    "        print(f\"Testing on month: {test_month}\")\n",
    "        \n",
    "        # 准备训练和测试数据\n",
    "        train_data, test_data = prepare_train_test_data(\n",
    "            trial_spike_rate_data, trial_date, trial_image, image_feature_list, test_month\n",
    "        )\n",
    "        \n",
    "        print(f\"Train data: {len(train_data)} trials\")\n",
    "        print(f\"Test data: {len(test_data)} trials\")\n",
    "        \n",
    "        if len(train_data) == 0 or len(test_data) == 0:\n",
    "            print(f\"Skipping slide {slide} due to insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        # 创建数据集\n",
    "        current_input_neuron = train_data['spike_data'].iloc[0].shape[0]\n",
    "        current_time_bins = train_data['spike_data'].iloc[0].shape[1]\n",
    "        \n",
    "        train_dataset = EPDataset(\n",
    "            train_data['spike_data'].tolist(), \n",
    "            train_data['image'].tolist(), \n",
    "            train_data['feature'].tolist()\n",
    "        )\n",
    "        test_dataset = EPDataset(\n",
    "            test_data['spike_data'].tolist(), \n",
    "            test_data['image'].tolist(), \n",
    "            test_data['feature'].tolist()\n",
    "        )\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "        \n",
    "        # 创建模型配置\n",
    "        config = ModelConfig(\n",
    "            input_neuron=current_input_neuron,\n",
    "            time_bins=current_time_bins\n",
    "        )\n",
    "        \n",
    "        # 设置设备\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = TimeTransformerConvModel(config).to(device)\n",
    "        \n",
    "        # 加载图像聚类信息（如果存在）\n",
    "        try:\n",
    "            image_cluster = pd.read_csv(\"image_cluster.csv\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"image_cluster.csv not found, using default clustering\")\n",
    "            # 创建默认聚类（每个图像一个类别）\n",
    "            image_cluster = pd.DataFrame({\n",
    "                'image': range(117),\n",
    "                'cluster': range(117)\n",
    "            })\n",
    "        \n",
    "        print(f\"Training with {current_input_neuron} neurons, {current_time_bins} time bins\")\n",
    "        \n",
    "        # 开始训练\n",
    "        results = main_train_loop(\n",
    "            config=config,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            device=device,\n",
    "            image_cluster=image_cluster\n",
    "        )\n",
    "        \n",
    "        results_dict[slide] = results\n",
    "        print(f\"Completed training for slide {slide}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "# 运行训练\n",
    "results_dict = train_with_mouse6_data()\n",
    "\n",
    "# # 保存结果\n",
    "# import pickle\n",
    "# with open(\"/media/ubuntu/sda/data/paper_architecture/02_consistency/mouse6_training_results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(results_dict, f)\n",
    "\n",
    "# print(\"Training completed and results saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 调试数据加载 ===\n",
      "✓ Loaded spike data: 18456 trials\n",
      "✓ Loaded dates: 18456 entries\n",
      "✓ Loaded images: 18456 entries\n",
      "Date range: 21322 to 122022\n",
      "Image range: 1 to 117\n",
      "Unique images: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]\n",
      "Spike data shape: (18, 40)\n",
      "⚠ image_feature.pkl not found, creating dummy features\n",
      "\\n=== 测试特征索引 ===\n",
      "Trial 0: Image ID = 27\n",
      "  ✓ Valid index, feature shape: (512,)\n",
      "Trial 1: Image ID = 42\n",
      "  ✓ Valid index, feature shape: (512,)\n",
      "Trial 2: Image ID = 5\n",
      "  ✓ Valid index, feature shape: (512,)\n",
      "Trial 3: Image ID = 101\n",
      "  ✓ Valid index, feature shape: (512,)\n",
      "Trial 4: Image ID = 84\n",
      "  ✓ Valid index, feature shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "# 修复索引错误的调试代码\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def debug_data_loading():\n",
    "    \"\"\"调试数据加载过程\"\"\"\n",
    "    print(\"=== 调试数据加载 ===\")\n",
    "    \n",
    "    # 加载数据\n",
    "    try:\n",
    "        with open(\"/media/ubuntu/sda/data/paper_architecture/02_consistency/data_for_train/mouse6/trial_spike_rate_data.pkl\", 'rb') as f:\n",
    "            trial_spike_rate_data = pickle.load(f)\n",
    "        print(f\"✓ Loaded spike data: {len(trial_spike_rate_data)} trials\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading spike data: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(\"/media/ubuntu/sda/data/paper_architecture/02_consistency/data_for_train/mouse6/trial_date.pkl\", 'rb') as f:\n",
    "            trial_date = pickle.load(f)\n",
    "        print(f\"✓ Loaded dates: {len(trial_date)} entries\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading dates: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(\"/media/ubuntu/sda/data/paper_architecture/02_consistency/data_for_train/mouse6/trial_image.pkl\", 'rb') as f:\n",
    "            trial_image = pickle.load(f)\n",
    "        print(f\"✓ Loaded images: {len(trial_image)} entries\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading images: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 检查数据一致性\n",
    "    if len(trial_spike_rate_data) != len(trial_date) or len(trial_date) != len(trial_image):\n",
    "        print(\"⚠ Warning: Data lengths inconsistent!\")\n",
    "        min_len = min(len(trial_spike_rate_data), len(trial_date), len(trial_image))\n",
    "        print(f\"Truncating to minimum length: {min_len}\")\n",
    "        trial_spike_rate_data = trial_spike_rate_data[:min_len]\n",
    "        trial_date = trial_date[:min_len]\n",
    "        trial_image = trial_image[:min_len]\n",
    "    \n",
    "    # 检查数据内容\n",
    "    print(f\"Date range: {min(trial_date)} to {max(trial_date)}\")\n",
    "    print(f\"Image range: {min(trial_image)} to {max(trial_image)}\")\n",
    "    print(f\"Unique images: {sorted(set(trial_image))}\")\n",
    "    \n",
    "    if len(trial_spike_rate_data) > 0:\n",
    "        print(f\"Spike data shape: {trial_spike_rate_data[0].shape}\")\n",
    "    \n",
    "    # 检查图像特征\n",
    "    try:\n",
    "        with open(\"image_feature.pkl\", 'rb') as f:\n",
    "            image_feature_list = pickle.load(f)\n",
    "        print(f\"✓ Loaded image features: {len(image_feature_list)} features\")\n",
    "        if len(image_feature_list) > 0:\n",
    "            print(f\"Feature dimension: {len(image_feature_list[0])}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠ image_feature.pkl not found, creating dummy features\")\n",
    "        image_feature_list = [np.random.randn(512) for _ in range(117)]\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading image features: {e}\")\n",
    "        image_feature_list = [np.random.randn(512) for _ in range(117)]\n",
    "    \n",
    "    # 测试特征索引\n",
    "    print(\"\\\\n=== 测试特征索引 ===\")\n",
    "    for i, img_id in enumerate(trial_image[:5]):  # 测试前5个\n",
    "        print(f\"Trial {i}: Image ID = {img_id}\")\n",
    "        if img_id >= 0 and img_id < len(image_feature_list):\n",
    "            print(f\"  ✓ Valid index, feature shape: {image_feature_list[img_id].shape}\")\n",
    "        else:\n",
    "            print(f\"  ✗ Invalid index! Image ID {img_id} not in range [0, {len(image_feature_list)-1}]\")\n",
    "    \n",
    "    return trial_spike_rate_data, trial_date, trial_image, image_feature_list\n",
    "\n",
    "# 运行调试\n",
    "trial_spike_rate_data, trial_date, trial_image, image_feature_list = debug_data_loading()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据准备函数已定义完成！\n"
     ]
    }
   ],
   "source": [
    "# 修复版本的数据准备函数\n",
    "def safe_prepare_train_test_data(trial_spike_rate_data, trial_date, trial_image, image_feature_list, test_dates):\n",
    "    \"\"\"安全准备训练和测试数据，避免索引错误\"\"\"\n",
    "    print(f\"Preparing data with test dates: {test_dates}\")\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    data_df = pd.DataFrame({\n",
    "        'spike_data': trial_spike_rate_data,\n",
    "        'date': trial_date,\n",
    "        'image': trial_image\n",
    "    })\n",
    "    \n",
    "    print(f\"Created DataFrame with {len(data_df)} rows\")\n",
    "    print(f\"Image ID range: {data_df['image'].min()} to {data_df['image'].max()}\")\n",
    "    print(f\"Available features: {len(image_feature_list)}\")\n",
    "    \n",
    "    # 安全添加特征\n",
    "    features = []\n",
    "    for i, img_id in enumerate(data_df['image']):\n",
    "        # 处理不同的图像ID格式\n",
    "        if isinstance(img_id, (int, float)):\n",
    "            # 如果是1-117范围的ID，转换为0-116\n",
    "            if 1 <= img_id <= 117:\n",
    "                img_idx = int(img_id) - 1\n",
    "            # 如果已经是0-116范围的ID\n",
    "            elif 0 <= img_id < 117:\n",
    "                img_idx = int(img_id)\n",
    "            else:\n",
    "                print(f\"Warning: Invalid image ID {img_id} at index {i}, using index 0\")\n",
    "                img_idx = 0\n",
    "        else:\n",
    "            print(f\"Warning: Non-numeric image ID {img_id} at index {i}, using index 0\")\n",
    "            img_idx = 0\n",
    "        \n",
    "        # 安全获取特征\n",
    "        if 0 <= img_idx < len(image_feature_list):\n",
    "            feature = image_feature_list[img_idx]\n",
    "        else:\n",
    "            print(f\"Warning: Image index {img_idx} out of range, using dummy feature\")\n",
    "            feature = np.random.randn(512)\n",
    "        \n",
    "        features.append(feature)\n",
    "    \n",
    "    data_df['feature'] = features\n",
    "    \n",
    "    # 分割训练和测试数据\n",
    "    train_data = data_df[~data_df['date'].isin(test_dates)]\n",
    "    test_data = data_df[data_df['date'].isin(test_dates)]\n",
    "    \n",
    "    print(f\"Train data: {len(train_data)} trials\")\n",
    "    print(f\"Test data: {len(test_data)} trials\")\n",
    "    \n",
    "    if len(train_data) > 0:\n",
    "        print(f\"Train data image range: {train_data['image'].min()} to {train_data['image'].max()}\")\n",
    "    if len(test_data) > 0:\n",
    "        print(f\"Test data image range: {test_data['image'].min()} to {test_data['image'].max()}\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# 定义日期顺序\n",
    "date_order = ['021322', '022522', '031722', '042422', \n",
    "              '052422', '062422', '072322', '082322', \n",
    "              '092422', '102122', '112022', '122022', \n",
    "              '022223', '032123', '042323']\n",
    "\n",
    "print(\"数据准备函数已定义完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备运行安全快速测试...\n",
      "=== 开始安全快速测试 ===\n",
      "Testing on month: [22522]\n",
      "Preparing data with test dates: [22522]\n",
      "Created DataFrame with 18456 rows\n",
      "Image ID range: 1 to 117\n",
      "Available features: 117\n",
      "Train data: 17286 trials\n",
      "Test data: 1170 trials\n",
      "Train data image range: 1 to 117\n",
      "Test data image range: 1 to 117\n",
      "Data shape: 18 neurons x 40 time bins\n",
      "Created datasets - Train: 17286, Test: 1170\n",
      "Created data loaders\n",
      "Using device: cuda\n",
      "Created model successfully\n",
      "Starting training...\n",
      "Error during training: mat1 and mat2 shapes cannot be multiplied (16x1024 and 512x16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3363688/3666048438.py\", line 89, in safe_quick_test_training\n",
      "    results = main_train_loop(\n",
      "        config=config,\n",
      "    ...<4 lines>...\n",
      "        image_cluster=image_cluster\n",
      "    )\n",
      "  File \"/tmp/ipykernel_3363688/2739153585.py\", line 10, in main_train_loop\n",
      "    train_loss, train_acc = train_model(\n",
      "                            ~~~~~~~~~~~^\n",
      "        model=model,\n",
      "        ^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        config=config\n",
      "        ^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/tmp/ipykernel_3363688/1792460012.py\", line 24, in train_model\n",
      "    loss = criterion(logits, labels, img_feature, features)\n",
      "  File \"/home/ubuntu/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3363688/2486964316.py\", line 26, in forward\n",
      "    loss_cont = self.contrastive_loss(features, img_feature)\n",
      "  File \"/tmp/ipykernel_3363688/2486964316.py\", line 15, in contrastive_loss\n",
      "    logits_ab = torch.matmul(h_neuro, h_img.T) / self.temp\n",
      "                ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: mat1 and mat2 shapes cannot be multiplied (16x1024 and 512x16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    }
   ],
   "source": [
    "# 修复版本的快速测试训练函数\n",
    "def safe_quick_test_training():\n",
    "    \"\"\"安全快速测试训练流程\"\"\"\n",
    "    print(\"=== 开始安全快速测试 ===\")\n",
    "    \n",
    "    # 选择第一个测试月份\n",
    "    test_month = [int(date_order[1])]  # 使用第二个月份作为测试\n",
    "    \n",
    "    print(f\"Testing on month: {test_month}\")\n",
    "    \n",
    "    # 准备数据\n",
    "    try:\n",
    "        train_data, test_data = safe_prepare_train_test_data(\n",
    "            trial_spike_rate_data, trial_date, trial_image, image_feature_list, test_month\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if len(train_data) == 0 or len(test_data) == 0:\n",
    "        print(\"No data available for testing\")\n",
    "        return None\n",
    "    \n",
    "    # 创建数据集\n",
    "    try:\n",
    "        current_input_neuron = train_data['spike_data'].iloc[0].shape[0]\n",
    "        current_time_bins = train_data['spike_data'].iloc[0].shape[1]\n",
    "        \n",
    "        print(f\"Data shape: {current_input_neuron} neurons x {current_time_bins} time bins\")\n",
    "        \n",
    "        train_dataset = EPDataset(\n",
    "            train_data['spike_data'].tolist(), \n",
    "            train_data['image'].tolist(), \n",
    "            train_data['feature'].tolist()\n",
    "        )\n",
    "        test_dataset = EPDataset(\n",
    "            test_data['spike_data'].tolist(), \n",
    "            test_data['image'].tolist(), \n",
    "            test_data['feature'].tolist()\n",
    "        )\n",
    "        \n",
    "        print(f\"Created datasets - Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating datasets: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    try:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)  # 减小batch size\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, drop_last=True)\n",
    "        \n",
    "        print(\"Created data loaders\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating data loaders: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 创建模型配置\n",
    "    try:\n",
    "        config = ModelConfig(\n",
    "            input_neuron=current_input_neuron,\n",
    "            time_bins=current_time_bins,\n",
    "            epochs=5  # 进一步减少训练轮数\n",
    "        )\n",
    "        \n",
    "        # 设置设备\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        model = TimeTransformerConvModel(config).to(device)\n",
    "        \n",
    "        print(\"Created model successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 创建默认图像聚类\n",
    "    try:\n",
    "        image_cluster = pd.DataFrame({\n",
    "            'image': range(117),\n",
    "            'cluster': range(117)\n",
    "        })\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        # 开始训练\n",
    "        results = main_train_loop(\n",
    "            config=config,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            device=device,\n",
    "            image_cluster=image_cluster\n",
    "        )\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "        print(f\"Final test accuracy: {results.get('test_accuracy', 'N/A')}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# 运行安全快速测试\n",
    "print(\"准备运行安全快速测试...\")\n",
    "safe_results = safe_quick_test_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 调试模型维度 ===\n",
      "Sample spike data shape: (18, 40)\n",
      "Input tensor shape: torch.Size([16, 40, 18])\n",
      "Model config:\n",
      "  input_neuron: 18\n",
      "  time_bins: 40\n",
      "  d_model: 150\n",
      "  conv_channels: 64\n",
      "  num_classes: 117\n",
      "\\n=== 模型结构调试 ===\n",
      "1. Input: torch.Size([16, 40, 18])\n",
      "2. After masker: torch.Size([16, 40, 18])\n",
      "3. After input_proj: torch.Size([16, 40, 150])\n",
      "4. After pos_encoder: torch.Size([16, 40, 150])\n",
      "5. After transformer: torch.Size([16, 40, 150])\n",
      "6. After unsqueeze: torch.Size([16, 1, 40, 150])\n",
      "7. After conv_blocks: torch.Size([16, 64, 5, 18])\n",
      "8. After adaptive_pool: torch.Size([16, 64, 1, 1])\n",
      "9. After flatten: torch.Size([16, 64])\n",
      "10. After classifier: torch.Size([16, 117])\n",
      "11. After residual_layers: torch.Size([16, 1024])\n",
      "\\n✓ 模型维度调试完成，没有发现错误\n",
      "\\n=== 图像特征维度检查 ===\n",
      "Image feature dimension: 512\n",
      "h_neuro shape: torch.Size([16, 1024])\n",
      "h_img shape: torch.Size([16, 512])\n",
      "✗ Error in contrastive loss: mat1 and mat2 shapes cannot be multiplied (16x1024 and 512x16)\n",
      "    h_neuro: torch.Size([16, 1024])\n",
      "    h_img.T: torch.Size([512, 16])\n"
     ]
    }
   ],
   "source": [
    "# 调试模型维度问题\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def debug_model_dimensions():\n",
    "    \"\"\"调试模型维度问题\"\"\"\n",
    "    print(\"=== 调试模型维度 ===\")\n",
    "    \n",
    "    # 检查数据形状\n",
    "    if len(trial_spike_rate_data) > 0:\n",
    "        sample_shape = trial_spike_rate_data[0].shape\n",
    "        print(f\"Sample spike data shape: {sample_shape}\")\n",
    "        \n",
    "        # 模拟一个batch的数据\n",
    "        batch_size = 16\n",
    "        sample_data = torch.randn(batch_size, sample_shape[1], sample_shape[0])  # [B, T, D]\n",
    "        print(f\"Input tensor shape: {sample_data.shape}\")\n",
    "        \n",
    "        # 创建模型配置\n",
    "        config = ModelConfig(\n",
    "            input_neuron=sample_shape[0],\n",
    "            time_bins=sample_shape[1],\n",
    "            d_model=150,\n",
    "            conv_channels=64,\n",
    "            num_classes=117\n",
    "        )\n",
    "        \n",
    "        print(f\"Model config:\")\n",
    "        print(f\"  input_neuron: {config.input_dim}\")\n",
    "        print(f\"  time_bins: {config.time_steps}\")\n",
    "        print(f\"  d_model: {config.transformer['d_model']}\")\n",
    "        print(f\"  conv_channels: {config.convolution['channels']}\")\n",
    "        print(f\"  num_classes: {config.num_classes}\")\n",
    "        \n",
    "        # 创建模型\n",
    "        device = torch.device(\"cpu\")  # 使用CPU进行调试\n",
    "        model = TimeTransformerConvModel(config).to(device)\n",
    "        \n",
    "        print(f\"\\\\n=== 模型结构调试 ===\")\n",
    "        \n",
    "        # 逐步调试forward过程\n",
    "        x = sample_data.to(device)\n",
    "        print(f\"1. Input: {x.shape}\")\n",
    "        \n",
    "        # 通过masker\n",
    "        x = model.masker(x)\n",
    "        print(f\"2. After masker: {x.shape}\")\n",
    "        \n",
    "        # 通过input_proj\n",
    "        x = model.input_proj(x)\n",
    "        print(f\"3. After input_proj: {x.shape}\")\n",
    "        \n",
    "        # 通过positional encoding\n",
    "        x = model.pos_encoder(x)\n",
    "        print(f\"4. After pos_encoder: {x.shape}\")\n",
    "        \n",
    "        # 通过transformer\n",
    "        x = model.transformer(x)\n",
    "        print(f\"5. After transformer: {x.shape}\")\n",
    "        \n",
    "        # 添加channel维度\n",
    "        x = x.unsqueeze(1)\n",
    "        print(f\"6. After unsqueeze: {x.shape}\")\n",
    "        \n",
    "        # 通过conv blocks\n",
    "        try:\n",
    "            x = model.conv_blocks(x)\n",
    "            print(f\"7. After conv_blocks: {x.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"7. Error in conv_blocks: {e}\")\n",
    "            return\n",
    "        \n",
    "        # 通过adaptive pool\n",
    "        x = model.adaptive_pool(x)\n",
    "        print(f\"8. After adaptive_pool: {x.shape}\")\n",
    "        \n",
    "        # flatten\n",
    "        x = x.flatten(1)\n",
    "        print(f\"9. After flatten: {x.shape}\")\n",
    "        \n",
    "        # 检查classifier\n",
    "        try:\n",
    "            logits = model.classifier(x)\n",
    "            print(f\"10. After classifier: {logits.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"10. Error in classifier: {e}\")\n",
    "            print(f\"    classifier input dim: {model.classifier.in_features}\")\n",
    "            print(f\"    classifier output dim: {model.classifier.out_features}\")\n",
    "            print(f\"    actual input shape: {x.shape}\")\n",
    "            return\n",
    "        \n",
    "        # 检查residual layers\n",
    "        try:\n",
    "            features = model.residual_layers(x)\n",
    "            print(f\"11. After residual_layers: {features.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"11. Error in residual_layers: {e}\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\\\n✓ 模型维度调试完成，没有发现错误\")\n",
    "        \n",
    "        # 检查图像特征维度\n",
    "        print(f\"\\\\n=== 图像特征维度检查 ===\")\n",
    "        if len(image_feature_list) > 0:\n",
    "            feature_dim = len(image_feature_list[0])\n",
    "            print(f\"Image feature dimension: {feature_dim}\")\n",
    "            \n",
    "            # 检查对比学习中的矩阵乘法\n",
    "            h_neuro = features  # [batch_size, 1024]\n",
    "            h_img = torch.randn(batch_size, feature_dim)  # [batch_size, feature_dim]\n",
    "            \n",
    "            print(f\"h_neuro shape: {h_neuro.shape}\")\n",
    "            print(f\"h_img shape: {h_img.shape}\")\n",
    "            \n",
    "            # 检查矩阵乘法\n",
    "            try:\n",
    "                logits_ab = torch.matmul(h_neuro, h_img.T) / 0.07\n",
    "                print(f\"✓ Contrastive logits shape: {logits_ab.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error in contrastive loss: {e}\")\n",
    "                print(f\"    h_neuro: {h_neuro.shape}\")\n",
    "                print(f\"    h_img.T: {h_img.T.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No spike data available for debugging\")\n",
    "\n",
    "# 运行调试\n",
    "debug_model_dimensions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始修复维度的安全训练 ===\n",
      "Data shape: 18 neurons x 40 time bins\n",
      "Created config:\n",
      "  input_dim: 18\n",
      "  time_steps: 40\n",
      "  d_model: 128\n",
      "  nhead: 8\n",
      "  conv_channels: 64\n",
      "Testing on month: [22522]\n",
      "Preparing data with test dates: [22522]\n",
      "Created DataFrame with 18456 rows\n",
      "Image ID range: 1 to 117\n",
      "Available features: 117\n",
      "Train data: 17286 trials\n",
      "Test data: 1170 trials\n",
      "Train data image range: 1 to 117\n",
      "Test data image range: 1 to 117\n",
      "Using device: cuda\n",
      "Error during training: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3363688/1546890164.py\", line 81, in safe_train_with_fixed_dimensions\n",
      "    model = TimeTransformerConvModel(config).to(device)\n",
      "  File \"/home/ubuntu/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1343, in to\n",
      "    return self._apply(convert)\n",
      "           ~~~~~~~~~~~^^^^^^^^^\n",
      "  File \"/home/ubuntu/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
      "    module._apply(fn)\n",
      "    ~~~~~~~~~~~~~^^^^\n",
      "  File \"/home/ubuntu/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/ubuntu/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
      "    return t.to(\n",
      "           ~~~~^\n",
      "        device,\n",
      "        ^^^^^^^\n",
      "        dtype if t.is_floating_point() or t.is_complex() else None,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        non_blocking,\n",
      "        ^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 修复模型维度问题的训练函数\n",
    "def create_fixed_model_config(trial_spike_rate_data):\n",
    "    \"\"\"创建修复了维度问题的模型配置\"\"\"\n",
    "    if len(trial_spike_rate_data) == 0:\n",
    "        raise ValueError(\"No spike data available\")\n",
    "    \n",
    "    sample_shape = trial_spike_rate_data[0].shape\n",
    "    n_neurons = sample_shape[0]\n",
    "    n_time_bins = sample_shape[1]\n",
    "    \n",
    "    print(f\"Data shape: {n_neurons} neurons x {n_time_bins} time bins\")\n",
    "    \n",
    "    # 创建修复的配置\n",
    "    config = ModelConfig(\n",
    "        input_neuron=n_neurons,\n",
    "        time_bins=n_time_bins,\n",
    "        d_model=128,  # 减小d_model以避免维度问题\n",
    "        nhead=8,      # 确保d_model能被nhead整除\n",
    "        conv_channels=64,\n",
    "        num_conv_blocks=2,  # 减少卷积层数\n",
    "        num_classes=117,\n",
    "        residual_dims=[256, 512],  # 调整residual维度\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # 验证配置\n",
    "    assert config.transformer['d_model'] % config.transformer['nhead'] == 0, \\\n",
    "        f\"d_model ({config.transformer['d_model']}) must be divisible by nhead ({config.transformer['nhead']})\"\n",
    "    \n",
    "    print(f\"Created config:\")\n",
    "    print(f\"  input_dim: {config.input_dim}\")\n",
    "    print(f\"  time_steps: {config.time_steps}\")\n",
    "    print(f\"  d_model: {config.transformer['d_model']}\")\n",
    "    print(f\"  nhead: {config.transformer['nhead']}\")\n",
    "    print(f\"  conv_channels: {config.convolution['channels']}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "def safe_train_with_fixed_dimensions():\n",
    "    \"\"\"使用修复维度的安全训练\"\"\"\n",
    "    print(\"=== 开始修复维度的安全训练 ===\")\n",
    "    \n",
    "    try:\n",
    "        # 创建修复的模型配置\n",
    "        config = create_fixed_model_config(trial_spike_rate_data)\n",
    "        \n",
    "        # 选择测试月份\n",
    "        test_month = [int(date_order[1])]\n",
    "        print(f\"Testing on month: {test_month}\")\n",
    "        \n",
    "        # 准备数据\n",
    "        train_data, test_data = safe_prepare_train_test_data(\n",
    "            trial_spike_rate_data, trial_date, trial_image, image_feature_list, test_month\n",
    "        )\n",
    "        \n",
    "        if len(train_data) == 0 or len(test_data) == 0:\n",
    "            print(\"No data available for testing\")\n",
    "            return None\n",
    "        \n",
    "        # 创建数据集\n",
    "        train_dataset = EPDataset(\n",
    "            train_data['spike_data'].tolist(), \n",
    "            train_data['image'].tolist(), \n",
    "            train_data['feature'].tolist()\n",
    "        )\n",
    "        test_dataset = EPDataset(\n",
    "            test_data['spike_data'].tolist(), \n",
    "            test_data['image'].tolist(), \n",
    "            test_data['feature'].tolist()\n",
    "        )\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)  # 减小batch size\n",
    "        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, drop_last=True)\n",
    "        \n",
    "        # 设置设备\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # 创建模型\n",
    "        model = TimeTransformerConvModel(config).to(device)\n",
    "        \n",
    "        # 测试模型前向传播\n",
    "        print(\"Testing model forward pass...\")\n",
    "        with torch.no_grad():\n",
    "            sample_batch = next(iter(train_loader))\n",
    "            EP_tensor, label, feature = sample_batch\n",
    "            EP_tensor = EP_tensor.to(device)\n",
    "            feature = torch.stack(feature).to(device)\n",
    "            \n",
    "            print(f\"Input shapes:\")\n",
    "            print(f\"  EP_tensor: {EP_tensor.shape}\")\n",
    "            print(f\"  feature: {feature.shape}\")\n",
    "            \n",
    "            try:\n",
    "                logits, features = model(EP_tensor)\n",
    "                print(f\"✓ Model forward pass successful!\")\n",
    "                print(f\"  logits: {logits.shape}\")\n",
    "                print(f\"  features: {features.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Model forward pass failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                return None\n",
    "        \n",
    "        # 创建图像聚类\n",
    "        image_cluster = pd.DataFrame({\n",
    "            'image': range(117),\n",
    "            'cluster': range(117)\n",
    "        })\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        # 开始训练\n",
    "        results = main_train_loop(\n",
    "            config=config,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            device=device,\n",
    "            image_cluster=image_cluster\n",
    "        )\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "        print(f\"Final test accuracy: {results.get('test_accuracy', 'N/A')}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# 运行修复维度的训练\n",
    "fixed_results = safe_train_with_fixed_dimensions()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

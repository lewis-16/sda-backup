{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384e9b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import spikeinterface as si\n",
    "import matplotlib.pyplot as plt\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.widgets as sw\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import json\n",
    "import probeinterface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5cedfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /media/ubuntu/sda/Monkey/TVSD/monkeyF/20240115/Block_3/Hub1-instance1_B003.ns6\n",
      "Processing chunk 0-500000 (500000 samples)\n",
      "Processing chunk 500000-1000000 (500000 samples)\n",
      "Processing chunk 1000000-1500000 (500000 samples)\n",
      "Processing chunk 1500000-2000000 (500000 samples)\n",
      "Processing chunk 2000000-2500000 (500000 samples)\n",
      "Processing chunk 2500000-3000000 (500000 samples)\n",
      "Processing chunk 3000000-3500000 (500000 samples)\n",
      "Processing chunk 3500000-4000000 (500000 samples)\n",
      "Processing chunk 4000000-4500000 (500000 samples)\n",
      "Processing chunk 4500000-5000000 (500000 samples)\n",
      "Processing chunk 5000000-5500000 (500000 samples)\n",
      "Processing chunk 5500000-6000000 (500000 samples)\n",
      "Processing chunk 6000000-6500000 (500000 samples)\n",
      "Processing chunk 6500000-7000000 (500000 samples)\n",
      "Processing chunk 7000000-7500000 (500000 samples)\n",
      "Processing chunk 7500000-8000000 (500000 samples)\n",
      "Processing chunk 8000000-8500000 (500000 samples)\n",
      "Processing chunk 8500000-9000000 (500000 samples)\n",
      "Processing chunk 9000000-9500000 (500000 samples)\n",
      "Processing chunk 9500000-10000000 (500000 samples)\n",
      "Processing chunk 10000000-10500000 (500000 samples)\n",
      "Processing chunk 10500000-11000000 (500000 samples)\n",
      "Processing chunk 11000000-11500000 (500000 samples)\n",
      "Processing chunk 11500000-12000000 (500000 samples)\n",
      "Processing chunk 12000000-12500000 (500000 samples)\n",
      "Processing chunk 12500000-13000000 (500000 samples)\n",
      "Processing chunk 13000000-13500000 (500000 samples)\n",
      "Processing chunk 13500000-14000000 (500000 samples)\n",
      "Processing chunk 14000000-14500000 (500000 samples)\n",
      "Processing chunk 14500000-15000000 (500000 samples)\n",
      "Processing chunk 15000000-15500000 (500000 samples)\n",
      "Processing chunk 15500000-16000000 (500000 samples)\n",
      "Processing chunk 16000000-16500000 (500000 samples)\n",
      "Processing chunk 16500000-17000000 (500000 samples)\n",
      "Processing chunk 17000000-17500000 (500000 samples)\n",
      "Processing chunk 17500000-18000000 (500000 samples)\n",
      "Processing chunk 18000000-18500000 (500000 samples)\n",
      "Processing chunk 18500000-19000000 (500000 samples)\n",
      "Processing chunk 19000000-19500000 (500000 samples)\n",
      "Processing chunk 19500000-20000000 (500000 samples)\n",
      "Processing chunk 20000000-20500000 (500000 samples)\n",
      "Processing chunk 20500000-21000000 (500000 samples)\n",
      "Processing chunk 21000000-21500000 (500000 samples)\n",
      "Processing chunk 21500000-22000000 (500000 samples)\n",
      "Processing chunk 22000000-22500000 (500000 samples)\n",
      "Processing chunk 22500000-23000000 (500000 samples)\n",
      "Processing chunk 23000000-23500000 (500000 samples)\n",
      "Processing chunk 23500000-24000000 (500000 samples)\n",
      "Processing chunk 24000000-24500000 (500000 samples)\n",
      "Processing chunk 24500000-25000000 (500000 samples)\n",
      "Processing chunk 25000000-25500000 (500000 samples)\n",
      "Processing chunk 25500000-26000000 (500000 samples)\n",
      "Processing chunk 26000000-26500000 (500000 samples)\n",
      "Processing chunk 26500000-27000000 (500000 samples)\n",
      "Processing chunk 27000000-27500000 (500000 samples)\n",
      "Processing chunk 27500000-28000000 (500000 samples)\n",
      "Processing chunk 28000000-28500000 (500000 samples)\n",
      "Processing chunk 28500000-29000000 (500000 samples)\n",
      "Processing chunk 29000000-29500000 (500000 samples)\n",
      "Processing chunk 29500000-30000000 (500000 samples)\n",
      "Processing chunk 30000000-30500000 (500000 samples)\n",
      "Processing chunk 30500000-31000000 (500000 samples)\n",
      "Processing chunk 31000000-31500000 (500000 samples)\n",
      "Processing chunk 31500000-32000000 (500000 samples)\n",
      "Processing chunk 32000000-32500000 (500000 samples)\n",
      "Processing chunk 32500000-33000000 (500000 samples)\n",
      "Processing chunk 33000000-33500000 (500000 samples)\n",
      "Processing chunk 33500000-34000000 (500000 samples)\n",
      "Processing chunk 34000000-34500000 (500000 samples)\n",
      "Processing chunk 34500000-35000000 (500000 samples)\n",
      "Processing chunk 35000000-35500000 (500000 samples)\n",
      "Processing chunk 35500000-36000000 (500000 samples)\n",
      "Processing chunk 36000000-36500000 (500000 samples)\n",
      "Processing chunk 36500000-36679890 (179890 samples)\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub1-instance1_B003/array_01_V1.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub1-instance1_B003/array_02_V1.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub1-instance1_B003/array_03_V1.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub1-instance1_B003/array_04_V1.npy\n",
      "Completed processing Hub1-instance1_B003.ns6\n",
      "Processing /media/ubuntu/sda/Monkey/TVSD/monkeyF/20240115/Block_3/Hub1-instance2_B003.ns6\n",
      "Processing chunk 0-500000 (500000 samples)\n",
      "Processing chunk 500000-1000000 (500000 samples)\n",
      "Processing chunk 1000000-1500000 (500000 samples)\n",
      "Processing chunk 1500000-2000000 (500000 samples)\n",
      "Processing chunk 2000000-2500000 (500000 samples)\n",
      "Processing chunk 2500000-3000000 (500000 samples)\n",
      "Processing chunk 3000000-3500000 (500000 samples)\n",
      "Processing chunk 3500000-4000000 (500000 samples)\n",
      "Processing chunk 4000000-4500000 (500000 samples)\n",
      "Processing chunk 4500000-5000000 (500000 samples)\n",
      "Processing chunk 5000000-5500000 (500000 samples)\n",
      "Processing chunk 5500000-6000000 (500000 samples)\n",
      "Processing chunk 6000000-6500000 (500000 samples)\n",
      "Processing chunk 6500000-7000000 (500000 samples)\n",
      "Processing chunk 7000000-7500000 (500000 samples)\n",
      "Processing chunk 7500000-8000000 (500000 samples)\n",
      "Processing chunk 8000000-8500000 (500000 samples)\n",
      "Processing chunk 8500000-9000000 (500000 samples)\n",
      "Processing chunk 9000000-9500000 (500000 samples)\n",
      "Processing chunk 9500000-10000000 (500000 samples)\n",
      "Processing chunk 10000000-10500000 (500000 samples)\n",
      "Processing chunk 10500000-11000000 (500000 samples)\n",
      "Processing chunk 11000000-11500000 (500000 samples)\n",
      "Processing chunk 11500000-12000000 (500000 samples)\n",
      "Processing chunk 12000000-12500000 (500000 samples)\n",
      "Processing chunk 12500000-13000000 (500000 samples)\n",
      "Processing chunk 13000000-13500000 (500000 samples)\n",
      "Processing chunk 13500000-14000000 (500000 samples)\n",
      "Processing chunk 14000000-14500000 (500000 samples)\n",
      "Processing chunk 14500000-15000000 (500000 samples)\n",
      "Processing chunk 15000000-15500000 (500000 samples)\n",
      "Processing chunk 15500000-16000000 (500000 samples)\n",
      "Processing chunk 16000000-16500000 (500000 samples)\n",
      "Processing chunk 16500000-17000000 (500000 samples)\n",
      "Processing chunk 17000000-17500000 (500000 samples)\n",
      "Processing chunk 17500000-18000000 (500000 samples)\n",
      "Processing chunk 18000000-18500000 (500000 samples)\n",
      "Processing chunk 18500000-19000000 (500000 samples)\n",
      "Processing chunk 19000000-19500000 (500000 samples)\n",
      "Processing chunk 19500000-20000000 (500000 samples)\n",
      "Processing chunk 20000000-20500000 (500000 samples)\n",
      "Processing chunk 20500000-21000000 (500000 samples)\n",
      "Processing chunk 21000000-21500000 (500000 samples)\n",
      "Processing chunk 21500000-22000000 (500000 samples)\n",
      "Processing chunk 22000000-22500000 (500000 samples)\n",
      "Processing chunk 22500000-23000000 (500000 samples)\n",
      "Processing chunk 23000000-23500000 (500000 samples)\n",
      "Processing chunk 23500000-24000000 (500000 samples)\n",
      "Processing chunk 24000000-24500000 (500000 samples)\n",
      "Processing chunk 24500000-25000000 (500000 samples)\n",
      "Processing chunk 25000000-25500000 (500000 samples)\n",
      "Processing chunk 25500000-26000000 (500000 samples)\n",
      "Processing chunk 26000000-26500000 (500000 samples)\n",
      "Processing chunk 26500000-27000000 (500000 samples)\n",
      "Processing chunk 27000000-27500000 (500000 samples)\n",
      "Processing chunk 27500000-28000000 (500000 samples)\n",
      "Processing chunk 28000000-28500000 (500000 samples)\n",
      "Processing chunk 28500000-29000000 (500000 samples)\n",
      "Processing chunk 29000000-29500000 (500000 samples)\n",
      "Processing chunk 29500000-30000000 (500000 samples)\n",
      "Processing chunk 30000000-30500000 (500000 samples)\n",
      "Processing chunk 30500000-31000000 (500000 samples)\n",
      "Processing chunk 31000000-31500000 (500000 samples)\n",
      "Processing chunk 31500000-32000000 (500000 samples)\n",
      "Processing chunk 32000000-32500000 (500000 samples)\n",
      "Processing chunk 32500000-33000000 (500000 samples)\n",
      "Processing chunk 33000000-33500000 (500000 samples)\n",
      "Processing chunk 33500000-34000000 (500000 samples)\n",
      "Processing chunk 34000000-34500000 (500000 samples)\n",
      "Processing chunk 34500000-35000000 (500000 samples)\n",
      "Processing chunk 35000000-35500000 (500000 samples)\n",
      "Processing chunk 35500000-36000000 (500000 samples)\n",
      "Processing chunk 36000000-36500000 (500000 samples)\n",
      "Processing chunk 36500000-36578160 (78160 samples)\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub1-instance2_B003/array_09_IT.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub1-instance2_B003/array_10_IT.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub1-instance2_B003/array_11_IT.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub1-instance2_B003/array_12_IT.npy\n",
      "Completed processing Hub1-instance2_B003.ns6\n",
      "Processing /media/ubuntu/sda/Monkey/TVSD/monkeyF/20240115/Block_3/Hub2-instance1_B003.ns6\n",
      "Processing chunk 0-500000 (500000 samples)\n",
      "Processing chunk 500000-1000000 (500000 samples)\n",
      "Processing chunk 1000000-1500000 (500000 samples)\n",
      "Processing chunk 1500000-2000000 (500000 samples)\n",
      "Processing chunk 2000000-2500000 (500000 samples)\n",
      "Processing chunk 2500000-3000000 (500000 samples)\n",
      "Processing chunk 3000000-3500000 (500000 samples)\n",
      "Processing chunk 3500000-4000000 (500000 samples)\n",
      "Processing chunk 4000000-4500000 (500000 samples)\n",
      "Processing chunk 4500000-5000000 (500000 samples)\n",
      "Processing chunk 5000000-5500000 (500000 samples)\n",
      "Processing chunk 5500000-6000000 (500000 samples)\n",
      "Processing chunk 6000000-6500000 (500000 samples)\n",
      "Processing chunk 6500000-7000000 (500000 samples)\n",
      "Processing chunk 7000000-7500000 (500000 samples)\n",
      "Processing chunk 7500000-8000000 (500000 samples)\n",
      "Processing chunk 8000000-8500000 (500000 samples)\n",
      "Processing chunk 8500000-9000000 (500000 samples)\n",
      "Processing chunk 9000000-9500000 (500000 samples)\n",
      "Processing chunk 9500000-10000000 (500000 samples)\n",
      "Processing chunk 10000000-10500000 (500000 samples)\n",
      "Processing chunk 10500000-11000000 (500000 samples)\n",
      "Processing chunk 11000000-11500000 (500000 samples)\n",
      "Processing chunk 11500000-12000000 (500000 samples)\n",
      "Processing chunk 12000000-12500000 (500000 samples)\n",
      "Processing chunk 12500000-13000000 (500000 samples)\n",
      "Processing chunk 13000000-13500000 (500000 samples)\n",
      "Processing chunk 13500000-14000000 (500000 samples)\n",
      "Processing chunk 14000000-14500000 (500000 samples)\n",
      "Processing chunk 14500000-15000000 (500000 samples)\n",
      "Processing chunk 15000000-15500000 (500000 samples)\n",
      "Processing chunk 15500000-16000000 (500000 samples)\n",
      "Processing chunk 16000000-16500000 (500000 samples)\n",
      "Processing chunk 16500000-17000000 (500000 samples)\n",
      "Processing chunk 17000000-17500000 (500000 samples)\n",
      "Processing chunk 17500000-18000000 (500000 samples)\n",
      "Processing chunk 18000000-18500000 (500000 samples)\n",
      "Processing chunk 18500000-19000000 (500000 samples)\n",
      "Processing chunk 19000000-19500000 (500000 samples)\n",
      "Processing chunk 19500000-20000000 (500000 samples)\n",
      "Processing chunk 20000000-20500000 (500000 samples)\n",
      "Processing chunk 20500000-21000000 (500000 samples)\n",
      "Processing chunk 21000000-21500000 (500000 samples)\n",
      "Processing chunk 21500000-22000000 (500000 samples)\n",
      "Processing chunk 22000000-22500000 (500000 samples)\n",
      "Processing chunk 22500000-23000000 (500000 samples)\n",
      "Processing chunk 23000000-23500000 (500000 samples)\n",
      "Processing chunk 23500000-24000000 (500000 samples)\n",
      "Processing chunk 24000000-24500000 (500000 samples)\n",
      "Processing chunk 24500000-25000000 (500000 samples)\n",
      "Processing chunk 25000000-25500000 (500000 samples)\n",
      "Processing chunk 25500000-26000000 (500000 samples)\n",
      "Processing chunk 26000000-26500000 (500000 samples)\n",
      "Processing chunk 26500000-27000000 (500000 samples)\n",
      "Processing chunk 27000000-27500000 (500000 samples)\n",
      "Processing chunk 27500000-28000000 (500000 samples)\n",
      "Processing chunk 28000000-28500000 (500000 samples)\n",
      "Processing chunk 28500000-29000000 (500000 samples)\n",
      "Processing chunk 29000000-29500000 (500000 samples)\n",
      "Processing chunk 29500000-30000000 (500000 samples)\n",
      "Processing chunk 30000000-30500000 (500000 samples)\n",
      "Processing chunk 30500000-31000000 (500000 samples)\n",
      "Processing chunk 31000000-31500000 (500000 samples)\n",
      "Processing chunk 31500000-32000000 (500000 samples)\n",
      "Processing chunk 32000000-32500000 (500000 samples)\n",
      "Processing chunk 32500000-33000000 (500000 samples)\n",
      "Processing chunk 33000000-33500000 (500000 samples)\n",
      "Processing chunk 33500000-34000000 (500000 samples)\n",
      "Processing chunk 34000000-34500000 (500000 samples)\n",
      "Processing chunk 34500000-35000000 (500000 samples)\n",
      "Processing chunk 35000000-35500000 (500000 samples)\n",
      "Processing chunk 35500000-36000000 (500000 samples)\n",
      "Processing chunk 36000000-36500000 (500000 samples)\n",
      "Processing chunk 36500000-36679859 (179859 samples)\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub2-instance1_B003/array_05_V1.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub2-instance1_B003/array_06_V1.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub2-instance1_B003/array_07_V1.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub2-instance1_B003/array_08_V1.npy\n",
      "Completed processing Hub2-instance1_B003.ns6\n",
      "Processing /media/ubuntu/sda/Monkey/TVSD/monkeyF/20240115/Block_3/Hub2-instance2_B003.ns6\n",
      "Processing chunk 0-500000 (500000 samples)\n",
      "Processing chunk 500000-1000000 (500000 samples)\n",
      "Processing chunk 1000000-1500000 (500000 samples)\n",
      "Processing chunk 1500000-2000000 (500000 samples)\n",
      "Processing chunk 2000000-2500000 (500000 samples)\n",
      "Processing chunk 2500000-3000000 (500000 samples)\n",
      "Processing chunk 3000000-3500000 (500000 samples)\n",
      "Processing chunk 3500000-4000000 (500000 samples)\n",
      "Processing chunk 4000000-4500000 (500000 samples)\n",
      "Processing chunk 4500000-5000000 (500000 samples)\n",
      "Processing chunk 5000000-5500000 (500000 samples)\n",
      "Processing chunk 5500000-6000000 (500000 samples)\n",
      "Processing chunk 6000000-6500000 (500000 samples)\n",
      "Processing chunk 6500000-7000000 (500000 samples)\n",
      "Processing chunk 7000000-7500000 (500000 samples)\n",
      "Processing chunk 7500000-8000000 (500000 samples)\n",
      "Processing chunk 8000000-8500000 (500000 samples)\n",
      "Processing chunk 8500000-9000000 (500000 samples)\n",
      "Processing chunk 9000000-9500000 (500000 samples)\n",
      "Processing chunk 9500000-10000000 (500000 samples)\n",
      "Processing chunk 10000000-10500000 (500000 samples)\n",
      "Processing chunk 10500000-11000000 (500000 samples)\n",
      "Processing chunk 11000000-11500000 (500000 samples)\n",
      "Processing chunk 11500000-12000000 (500000 samples)\n",
      "Processing chunk 12000000-12500000 (500000 samples)\n",
      "Processing chunk 12500000-13000000 (500000 samples)\n",
      "Processing chunk 13000000-13500000 (500000 samples)\n",
      "Processing chunk 13500000-14000000 (500000 samples)\n",
      "Processing chunk 14000000-14500000 (500000 samples)\n",
      "Processing chunk 14500000-15000000 (500000 samples)\n",
      "Processing chunk 15000000-15500000 (500000 samples)\n",
      "Processing chunk 15500000-16000000 (500000 samples)\n",
      "Processing chunk 16000000-16500000 (500000 samples)\n",
      "Processing chunk 16500000-17000000 (500000 samples)\n",
      "Processing chunk 17000000-17500000 (500000 samples)\n",
      "Processing chunk 17500000-18000000 (500000 samples)\n",
      "Processing chunk 18000000-18500000 (500000 samples)\n",
      "Processing chunk 18500000-19000000 (500000 samples)\n",
      "Processing chunk 19000000-19500000 (500000 samples)\n",
      "Processing chunk 19500000-20000000 (500000 samples)\n",
      "Processing chunk 20000000-20500000 (500000 samples)\n",
      "Processing chunk 20500000-21000000 (500000 samples)\n",
      "Processing chunk 21000000-21500000 (500000 samples)\n",
      "Processing chunk 21500000-22000000 (500000 samples)\n",
      "Processing chunk 22000000-22500000 (500000 samples)\n",
      "Processing chunk 22500000-23000000 (500000 samples)\n",
      "Processing chunk 23000000-23500000 (500000 samples)\n",
      "Processing chunk 23500000-24000000 (500000 samples)\n",
      "Processing chunk 24000000-24500000 (500000 samples)\n",
      "Processing chunk 24500000-25000000 (500000 samples)\n",
      "Processing chunk 25000000-25500000 (500000 samples)\n",
      "Processing chunk 25500000-26000000 (500000 samples)\n",
      "Processing chunk 26000000-26500000 (500000 samples)\n",
      "Processing chunk 26500000-27000000 (500000 samples)\n",
      "Processing chunk 27000000-27500000 (500000 samples)\n",
      "Processing chunk 27500000-28000000 (500000 samples)\n",
      "Processing chunk 28000000-28500000 (500000 samples)\n",
      "Processing chunk 28500000-29000000 (500000 samples)\n",
      "Processing chunk 29000000-29500000 (500000 samples)\n",
      "Processing chunk 29500000-30000000 (500000 samples)\n",
      "Processing chunk 30000000-30500000 (500000 samples)\n",
      "Processing chunk 30500000-31000000 (500000 samples)\n",
      "Processing chunk 31000000-31500000 (500000 samples)\n",
      "Processing chunk 31500000-32000000 (500000 samples)\n",
      "Processing chunk 32000000-32500000 (500000 samples)\n",
      "Processing chunk 32500000-33000000 (500000 samples)\n",
      "Processing chunk 33000000-33500000 (500000 samples)\n",
      "Processing chunk 33500000-34000000 (500000 samples)\n",
      "Processing chunk 34000000-34500000 (500000 samples)\n",
      "Processing chunk 34500000-35000000 (500000 samples)\n",
      "Processing chunk 35000000-35500000 (500000 samples)\n",
      "Processing chunk 35500000-36000000 (500000 samples)\n",
      "Processing chunk 36000000-36500000 (500000 samples)\n",
      "Processing chunk 36500000-36578253 (78253 samples)\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub2-instance2_B003/array_13_IT.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub2-instance2_B003/array_14_V4.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub2-instance2_B003/array_15_V4.npy\n",
      "Saved /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/processed_data/Hub2-instance2_B003/array_16_V4.npy\n",
      "Completed processing Hub2-instance2_B003.ns6\n",
      "All files processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface as si\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "date = 20240115\n",
    "block = 3\n",
    "monkey = 'monkeyF'\n",
    "datadir_gen = f'/media/ubuntu/sda/Monkey/sorted_result/{date}/Block_{block}/'\n",
    "mapping_file = f'/media/ubuntu/sda/Monkey/TVSD/monkeyF/_logs/1024chns_mapping_20220105.mat'\n",
    "\n",
    "os.makedirs(f'/media/ubuntu/sda/Monkey/sorted_result/{date}/Block_{block}/', exist_ok=True)\n",
    "os.makedirs(f'/media/ubuntu/sda/Monkey/sorted_result/{date}/Block_{block}/processed_data', exist_ok=True)\n",
    "\n",
    "file_list = [\n",
    "    f'Hub1-instance1_B00{block}.ns6',\n",
    "    f'Hub1-instance2_B00{block}.ns6',\n",
    "    f'Hub2-instance1_B00{block}.ns6',\n",
    "    f'Hub2-instance2_B00{block}.ns6'\n",
    "]\n",
    "\n",
    "# 加载映射文件\n",
    "mapping_data = sio.loadmat(mapping_file)\n",
    "mapping = mapping_data['mapping'].flatten() - 1  # 转换为0-based索引\n",
    "\n",
    "# 定义脑区映射\n",
    "if monkey == 'monkeyN':\n",
    "    rois = np.ones(1024)  # V1\n",
    "    rois[512:768] = 2  # V4 (513-768)\n",
    "    rois[768:1024] = 3  # IT (769-1024)\n",
    "else:\n",
    "    rois = np.ones(1024)  # V1\n",
    "    rois[512:832] = 3  # IT (513-832)\n",
    "    rois[832:1024] = 2  # V4 (833-1024)\n",
    "\n",
    "output_dir = Path(datadir_gen) / 'processed_data'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 设置块大小（根据可用内存调整）\n",
    "chunk_size = 500000  # 每次处理的样本数\n",
    "\n",
    "# 处理每个文件\n",
    "for file_idx, file_name in enumerate(file_list):\n",
    "    file_path = f'/media/ubuntu/sda/Monkey/TVSD/monkeyF/{date}/Block_{block}/{file_name}'\n",
    "    print(f'Processing {file_path}')\n",
    "    \n",
    "    # 读取文件\n",
    "    recording = se.read_blackrock(file_path)\n",
    "    \n",
    "    # 处理多段数据\n",
    "    if recording.get_num_segments() > 1:\n",
    "        recording_list = []\n",
    "        for i in range(recording.get_num_segments()):\n",
    "            recording_list.append(recording.select_segments(i))\n",
    "        recording = si.concatenate_recordings(recording_list)\n",
    "    \n",
    "    # 获取采样率和样本数\n",
    "    sample_rate = recording.get_sampling_frequency()\n",
    "    n_samples = recording.get_num_samples()\n",
    "    \n",
    "    # 获取通道ID列表（字符串类型）\n",
    "    channel_ids = np.array([str(i) for i in range(1, 257)])\n",
    "    \n",
    "    # 确定当前文件在映射中的位置\n",
    "    if 'Hub1-instance1' in file_name:\n",
    "        file_start_idx = 0\n",
    "    elif 'Hub2-instance1' in file_name:\n",
    "        file_start_idx = 256\n",
    "    elif 'Hub1-instance2' in file_name:\n",
    "        file_start_idx = 512\n",
    "    elif 'Hub2-instance2' in file_name:\n",
    "        file_start_idx = 768\n",
    "    else:\n",
    "        raise ValueError(f'Unknown file type: {file_name}')\n",
    "    \n",
    "    # 创建文件输出目录\n",
    "    file_output_dir = output_dir / file_name.replace('.ns6', '')\n",
    "    file_output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 为每个阵列创建内存映射文件（每个文件有4个阵列）\n",
    "    array_files = []\n",
    "    array_info = []\n",
    "    \n",
    "    # 每个文件有256个通道，分成4组，每组64个通道\n",
    "    for array_idx in range(4):\n",
    "        # 确定阵列的主要脑区\n",
    "        start_chan = file_start_idx + array_idx * 64\n",
    "        end_chan = start_chan + 64\n",
    "        \n",
    "        array_roi_counts = np.bincount(rois[start_chan:end_chan].astype(int))\n",
    "        primary_roi = np.argmax(array_roi_counts)\n",
    "        \n",
    "        if primary_roi == 1:\n",
    "            roi_name = 'V1'\n",
    "        elif primary_roi == 2:\n",
    "            roi_name = 'V4'\n",
    "        else:\n",
    "            roi_name = 'IT'\n",
    "        \n",
    "        output_file = file_output_dir / f'array_{file_start_idx//64 + array_idx + 1:02d}_{roi_name}.npy'\n",
    "        \n",
    "        # 创建内存映射文件\n",
    "        mmap_array = np.lib.format.open_memmap(\n",
    "            output_file, mode='w+', dtype=np.float32, shape=(64, n_samples)\n",
    "        )\n",
    "        array_files.append(mmap_array)\n",
    "        array_info.append({'roi_name': roi_name, 'output_file': output_file})\n",
    "    \n",
    "    # 分块处理数据\n",
    "    for start in range(0, n_samples, chunk_size):\n",
    "        end = min(start + chunk_size, n_samples)\n",
    "        chunk_size_actual = end - start\n",
    "        \n",
    "        print(f'Processing chunk {start}-{end} ({chunk_size_actual} samples)')\n",
    "        \n",
    "        # 获取当前块的数据\n",
    "        chunk_data = recording.get_traces(start_frame=start, end_frame=end)\n",
    "        \n",
    "        # 处理当前文件的每个通道\n",
    "        for i in range(256):\n",
    "            # 使用正确的通道ID获取数据\n",
    "            channel_id = str(i + 1)  # 转换为字符串，因为Recording使用字符串ID\n",
    "            channel_idx_in_recording = np.where(channel_ids == channel_id)[0][0]\n",
    "            \n",
    "            # 确定通道属于哪个阵列（在当前文件的4个阵列中）\n",
    "            array_idx = i // 64\n",
    "            channel_in_array = i % 64\n",
    "            \n",
    "            # 将数据写入对应阵列的内存映射文件\n",
    "            array_files[array_idx][channel_in_array, start:end] = chunk_data[:, channel_idx_in_recording]\n",
    "    \n",
    "    # 保存并关闭内存映射文件\n",
    "    for array_idx, mmap_array in enumerate(array_files):\n",
    "        mmap_array.flush()\n",
    "        del mmap_array  # 释放内存映射\n",
    "        print(f'Saved {array_info[array_idx][\"output_file\"]}')\n",
    "    \n",
    "    print(f'Completed processing {file_name}')\n",
    "\n",
    "print('All files processed successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "442bf6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/spikeinterface/core/base.py:967: UserWarning: The extractor is not serializable to file. The provenance will not be saved.\n",
      "  warnings.warn(\"The extractor is not serializable to file. The provenance will not be saved.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BandpassFilterRecording: 64 channels - 30.0kHz - 1 segments - 36,578,160 samples \n",
      "                         1,219.27s (20.32 minutes) - float32 dtype - 8.72 GiB\n",
      "CommonReferenceRecording: 64 channels - 30.0kHz - 1 segments - 36,578,160 samples \n",
      "                          1,219.27s (20.32 minutes) - float32 dtype - 8.72 GiB\n",
      "Use cache_folder=/tmp/spikeinterface_cache/tmpwce3q708/014PDXYL\n",
      "write_binary_recording \n",
      "engine=process - n_jobs=1 - samples_per_chunk=30,000 - chunk_memory=7.32 MiB - total_memory=7.32 MiB - chunk_duration=1.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording (no parallelization): 100%|██████████| 1220/1220 [01:05<00:00, 18.55it/s]\n",
      "/tmp/ipykernel_3534540/3934467296.py:27: DeprecationWarning: `output_folder` is deprecated and will be removed in version 0.103.0 Please use folder instead\n",
      "  sorting_kilosort4 = ss.run_sorter(sorter_name=\"kilosort4\", recording=recording_preprocessed, output_folder=output_folder + \"/kilosort4\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryFolderRecording: 64 channels - 30.0kHz - 1 segments - 36,578,160 samples \n",
      "                       1,219.27s (20.32 minutes) - float32 dtype - 8.72 GiB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Folder /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/sort/array_09_IT.npy/kilosort4 already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/ubuntu/sda/Monkey/sorted_result/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Block_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sort/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/ubuntu/sda/Monkey/sorted_result/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Block_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/sort/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m sorting_kilosort4 \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_sorter\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkilosort4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecording\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecording_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kilosort4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m analyzer_kilosort4 \u001b[38;5;241m=\u001b[39m si\u001b[38;5;241m.\u001b[39mcreate_sorting_analyzer(sorting\u001b[38;5;241m=\u001b[39msorting_kilosort4, recording\u001b[38;5;241m=\u001b[39mrecording_preprocessed, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_folder\u001b[39m\u001b[38;5;124m'\u001b[39m, folder\u001b[38;5;241m=\u001b[39moutput_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/analyzer_kilosort4_binary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m extensions_to_compute \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_spikes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaveforms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         ]\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/spikeinterface/sorters/runsorter.py:198\u001b[0m, in \u001b[0;36mrun_sorter\u001b[0;34m(sorter_name, recording, folder, remove_existing_folder, delete_output_folder, verbose, raise_error, docker_image, singularity_image, delete_container_files, with_output, output_folder, **sorter_params)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe python `spython` package must be installed to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun singularity. Install with `pip install spython`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m             )\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m run_sorter_container(\n\u001b[1;32m    193\u001b[0m         container_image\u001b[38;5;241m=\u001b[39mcontainer_image,\n\u001b[1;32m    194\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_kwargs,\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_sorter_local\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcommon_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/spikeinterface/sorters/runsorter.py:255\u001b[0m, in \u001b[0;36mrun_sorter_local\u001b[0;34m(sorter_name, recording, folder, remove_existing_folder, delete_output_folder, verbose, raise_error, with_output, output_folder, **sorter_params)\u001b[0m\n\u001b[1;32m    252\u001b[0m SorterClass \u001b[38;5;241m=\u001b[39m sorter_dict[sorter_name]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# only classmethod call not instance (stateless at instance level but state is in folder)\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m folder \u001b[38;5;241m=\u001b[39m \u001b[43mSorterClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecording\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_existing_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m SorterClass\u001b[38;5;241m.\u001b[39mset_params_to_folder(recording, folder, sorter_params, verbose)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# This writes parameters and recording to binary and could ideally happen in the host\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/spikeinterface/sorters/external/kilosort4.py:127\u001b[0m, in \u001b[0;36mKilosort4Sorter.initialize_folder\u001b[0;34m(cls, recording, output_folder, verbose, remove_existing_folder)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sorter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msorter_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not installed. Please install it with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39minstallation_mesg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_sorter_version()\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKilosort4Sorter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecording\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_existing_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/spikeinterface/sorters/basesorter.py:131\u001b[0m, in \u001b[0;36mBaseSorter.initialize_folder\u001b[0;34m(cls, recording, output_folder, verbose, remove_existing_folder)\u001b[0m\n\u001b[1;32m    129\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;28mstr\u001b[39m(output_folder))\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFolder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m output_folder\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    134\u001b[0m sorter_output_folder\u001b[38;5;241m.\u001b[39mmkdir()\n",
      "\u001b[0;31mValueError\u001b[0m: Folder /media/ubuntu/sda/Monkey/sorted_result/20240115/Block_3/sort/array_09_IT.npy/kilosort4 already exists"
     ]
    }
   ],
   "source": [
    "import os\n",
    "date = 20240115\n",
    "block = 3\n",
    "for file in os.listdir(f\"/media/ubuntu/sda/Monkey/sorted_result/{date}/Block_{block}/processed_data\"):\n",
    "    for array in os.listdir(f\"/media/ubuntu/sda/Monkey/sorted_result/{date}/Block_{block}/processed_data/{file}\"):\n",
    "        recording = np.load(f\"/media/ubuntu/sda/Monkey/sorted_result/{date}/Block_{block}/processed_data/{file}/{array}\")\n",
    "        recording = se.NumpyRecording(recording.T, sampling_frequency=30000)\n",
    "        from probeinterface import write_probeinterface, read_probeinterface\n",
    "\n",
    "        probe_30channel = read_probeinterface('/media/ubuntu/sda/Monkey/scripts/probe.json')\n",
    "        probe_30channel.set_global_device_channel_indices([i for i in range(64)])\n",
    "        recording_recorded = recording.set_probegroup(probe_30channel)\n",
    "\n",
    "        recording_cmr = recording_recorded\n",
    "        recording_f = spre.bandpass_filter(recording_recorded, freq_min=300, freq_max=3000)\n",
    "        print(recording_f)\n",
    "        recording_cmr = spre.common_reference(recording_f, reference=\"global\", operator=\"median\")\n",
    "        print(recording_cmr)\n",
    "\n",
    "        # this computes and saves the recording after applying the preprocessing chain\n",
    "        recording_preprocessed = recording_cmr.save(format=\"binary\")\n",
    "        print(recording_preprocessed)\n",
    "        os.makedirs(f\"/media/ubuntu/sda/Monkey/sorted_result/{date}/Block_{block}/sort\", exist_ok=True)\n",
    "\n",
    "        os.makedirs(f\"/media/ubuntu/sda/Monkey/sorted_result/{date}/Block_{block}/sort/{array}\", exist_ok=True)\n",
    "        output_folder = f\"/media/ubuntu/sda/Monkey/sorted_result/{date}/Block_{block}/sort/{array}\"\n",
    "        sorting_kilosort4 = ss.run_sorter(sorter_name=\"kilosort4\", recording=recording_preprocessed, output_folder=output_folder + \"/kilosort4\")\n",
    "        analyzer_kilosort4 = si.create_sorting_analyzer(sorting=sorting_kilosort4, recording=recording_preprocessed, format='binary_folder', folder=output_folder + '/analyzer_kilosort4_binary')\n",
    "\n",
    "        extensions_to_compute = [\n",
    "                    \"random_spikes\",\n",
    "                    \"waveforms\",\n",
    "                    \"noise_levels\",\n",
    "                    \"templates\",\n",
    "                    \"spike_amplitudes\",\n",
    "                    \"unit_locations\",\n",
    "                    \"spike_locations\",\n",
    "                    \"correlograms\",\n",
    "                    \"template_similarity\"\n",
    "                ]\n",
    "\n",
    "        extension_params = {\n",
    "            \"unit_locations\": {\"method\": \"center_of_mass\"},\n",
    "            \"spike_locations\": {\"ms_before\": 0.1},\n",
    "            \"correlograms\": {\"bin_ms\": 0.1},\n",
    "            \"template_similarity\": {\"method\": \"cosine_similarity\"}\n",
    "        }\n",
    "\n",
    "        analyzer_kilosort4.compute(extensions_to_compute, extension_params=extension_params)\n",
    "\n",
    "        qm_params = sqm.get_default_qm_params()\n",
    "        analyzer_kilosort4.compute(\"quality_metrics\", qm_params)\n",
    "\n",
    "        import spikeinterface.exporters as sexp\n",
    "        sexp.export_to_phy(analyzer_kilosort4, output_folder + \"/phy_folder_for_kilosort\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = np.load(\"/media/ubuntu/sda/Monkey/TVSD/monkeyF/20240112/Block_1/processed_data/Hub1-instance1_B001/array_01_V1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = se.NumpyRecording(recording.T, sampling_frequency=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14635242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probeinterface import write_probeinterface, read_probeinterface\n",
    "\n",
    "probe_30channel = read_probeinterface('/media/ubuntu/sda/Monkey/probe.json')\n",
    "probe_30channel.set_global_device_channel_indices([i for i in range(64)])\n",
    "recording_recorded = recording.set_probegroup(probe_30channel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f71a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/spikeinterface/core/base.py:967: UserWarning: The extractor is not serializable to file. The provenance will not be saved.\n",
      "  warnings.warn(\"The extractor is not serializable to file. The provenance will not be saved.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BandpassFilterRecording: 64 channels - 30.0kHz - 1 segments - 7,031,518 samples \n",
      "                         234.38s (3.91 minutes) - float32 dtype - 1.68 GiB\n",
      "CommonReferenceRecording: 64 channels - 30.0kHz - 1 segments - 7,031,518 samples \n",
      "                          234.38s (3.91 minutes) - float32 dtype - 1.68 GiB\n",
      "Use cache_folder=/tmp/spikeinterface_cache/tmpq3oq4adi/UC8PBUQH\n",
      "write_binary_recording \n",
      "engine=process - n_jobs=1 - samples_per_chunk=30,000 - chunk_memory=7.32 MiB - total_memory=7.32 MiB - chunk_duration=1.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording (no parallelization): 100%|██████████| 235/235 [00:11<00:00, 21.15it/s]\n",
      "/tmp/ipykernel_1458373/1382817715.py:11: DeprecationWarning: `output_folder` is deprecated and will be removed in version 0.103.0 Please use folder instead\n",
      "  sorting_kilosort4 = ss.run_sorter(sorter_name=\"kilosort4\", recording=recording_preprocessed, output_folder=output_folder + \"/kilosort4\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryFolderRecording: 64 channels - 30.0kHz - 1 segments - 7,031,518 samples \n",
      "                       234.38s (3.91 minutes) - float32 dtype - 1.68 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:09<00:00, 11.83it/s]\n",
      "100%|██████████| 8/8 [00:21<00:00,  2.66s/it]\n",
      "100%|██████████| 118/118 [00:03<00:00, 31.27it/s]\n",
      "100%|██████████| 8/8 [00:06<00:00,  1.21it/s]\n",
      "estimate_sparsity (no parallelization): 100%|██████████| 235/235 [00:00<00:00, 5650.40it/s]\n",
      "/home/ubuntu/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/spikeinterface/core/basesorting.py:261: UserWarning: The registered recording will not be persistent on disk, but only available in memory\n",
      "  warnings.warn(\"The registered recording will not be persistent on disk, but only available in memory\")\n"
     ]
    }
   ],
   "source": [
    "recording_cmr = recording_recorded\n",
    "recording_f = spre.bandpass_filter(recording_recorded, freq_min=300, freq_max=3000)\n",
    "print(recording_f)\n",
    "recording_cmr = spre.common_reference(recording_f, reference=\"global\", operator=\"median\")\n",
    "print(recording_cmr)\n",
    "\n",
    "# this computes and saves the recording after applying the preprocessing chain\n",
    "recording_preprocessed = recording_cmr.save(format=\"binary\")\n",
    "print(recording_preprocessed)\n",
    "output_folder = '/media/ubuntu/sda/Monkey/test'\n",
    "sorting_kilosort4 = ss.run_sorter(sorter_name=\"kilosort4\", recording=recording_preprocessed, output_folder=output_folder + \"/kilosort4\")\n",
    "analyzer_kilosort4 = si.create_sorting_analyzer(sorting=sorting_kilosort4, recording=recording_preprocessed, format='binary_folder', folder=output_folder + '/analyzer_kilosort4_binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e35e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute_waveforms (no parallelization): 100%|██████████| 235/235 [00:10<00:00, 22.62it/s]\n",
      "noise_level (no parallelization): 100%|██████████| 20/20 [00:00<00:00, 50.80it/s]\n",
      "Compute : spike_amplitudes + spike_locations (no parallelization): 100%|██████████| 235/235 [00:01<00:00, 231.21it/s]\n",
      "/home/ubuntu/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/spikeinterface/qualitymetrics/misc_metrics.py:910: UserWarning: Some units have too few spikes : amplitude_cutoff is set to NaN\n",
      "  warnings.warn(f\"Some units have too few spikes : amplitude_cutoff is set to NaN\")\n",
      "noise_level (no parallelization): 100%|██████████| 20/20 [00:00<00:00, 884.63it/s]\n",
      "write_binary_recording (no parallelization): 100%|██████████| 235/235 [00:24<00:00,  9.40it/s]\n",
      "Fitting PCA: 100%|██████████| 64/64 [00:12<00:00,  5.21it/s]\n",
      "Projecting waveforms: 100%|██████████| 64/64 [00:00<00:00, 2211.66it/s]\n",
      "extract PCs (no parallelization): 100%|██████████| 235/235 [00:16<00:00, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:\n",
      "phy template-gui  /media/ubuntu/sda/Monkey/test/phy_folder_for_kilosort/params.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extensions_to_compute = [\n",
    "            \"random_spikes\",\n",
    "            \"waveforms\",\n",
    "            \"noise_levels\",\n",
    "            \"templates\",\n",
    "            \"spike_amplitudes\",\n",
    "            \"unit_locations\",\n",
    "            \"spike_locations\",\n",
    "            \"correlograms\",\n",
    "            \"template_similarity\"\n",
    "        ]\n",
    "\n",
    "extension_params = {\n",
    "    \"unit_locations\": {\"method\": \"center_of_mass\"},\n",
    "    \"spike_locations\": {\"ms_before\": 0.1},\n",
    "    \"correlograms\": {\"bin_ms\": 0.1},\n",
    "    \"template_similarity\": {\"method\": \"cosine_similarity\"}\n",
    "}\n",
    "\n",
    "analyzer_kilosort4.compute(extensions_to_compute, extension_params=extension_params)\n",
    "\n",
    "qm_params = sqm.get_default_qm_params()\n",
    "analyzer_kilosort4.compute(\"quality_metrics\", qm_params)\n",
    "\n",
    "import spikeinterface.exporters as sexp\n",
    "sexp.export_to_phy(analyzer_kilosort4, output_folder + \"/phy_folder_for_kilosort\", verbose=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting_jct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

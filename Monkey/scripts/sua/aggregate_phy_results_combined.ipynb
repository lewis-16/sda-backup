{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde9abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/spike_sorting/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface as si\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.postprocessing as spost\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f960a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_SORT_DIR = \"/media/ubuntu/sda/Monkey/sorted_result_combined/20240112/Hub1-instance1_V1/phy_folder_for_kilosort/\"\n",
    "DATE_STR = \"20240112\"\n",
    "\n",
    "# 需要的 cluster 指标文件（来自 phy_folder_for_kilosort）\n",
    "CLUSTER_INFO_FILENAME = \"cluster_info.tsv\"\n",
    "\n",
    "# spike 层面\n",
    "SPIKE_CLUSTERS_FILENAME = \"spike_clusters.npy\"\n",
    "SPIKE_TIMES_FILENAME = \"spike_times.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7c6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cluster_info(phy_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"读取 phy_folder_for_kilosort/cluster_info.tsv 为 DataFrame。\n",
    "    该表包含所有需要的 cluster 级指标。\n",
    "    如果cluster_info.tsv不存在，则尝试从其他文件构建基本信息。\n",
    "    \"\"\"\n",
    "    path = os.path.join(phy_dir, CLUSTER_INFO_FILENAME)\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path, sep='\\t')\n",
    "        # 标准化主键列名\n",
    "        if 'cluster_id' not in df.columns:\n",
    "            raise ValueError(f\"{path} 中缺少 cluster_id 列\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"警告: {path} 不存在，尝试从其他文件构建cluster信息\")\n",
    "        \n",
    "        # 尝试从cluster_group.tsv构建基本信息\n",
    "        cluster_group_path = os.path.join(phy_dir, \"cluster_group.tsv\")\n",
    "        if os.path.exists(cluster_group_path):\n",
    "            df = pd.read_csv(cluster_group_path, sep='\\t')\n",
    "            if 'cluster_id' not in df.columns:\n",
    "                raise ValueError(f\"{cluster_group_path} 中缺少 cluster_id 列\")\n",
    "            return df\n",
    "        else:\n",
    "            # 如果都没有，从spike_clusters.npy中提取唯一的cluster_id\n",
    "            spike_clusters_path = os.path.join(phy_dir, SPIKE_CLUSTERS_FILENAME)\n",
    "            if os.path.exists(spike_clusters_path):\n",
    "                spike_clusters = np.load(spike_clusters_path)\n",
    "                unique_clusters = np.unique(spike_clusters)\n",
    "                df = pd.DataFrame({\n",
    "                    'cluster_id': unique_clusters,\n",
    "                    'group': 'unsorted'  # 默认分组\n",
    "                })\n",
    "                return df\n",
    "            else:\n",
    "                raise ValueError(f\"无法找到任何cluster信息文件: {phy_dir}\")\n",
    "\n",
    "\n",
    "def load_spike_level(phy_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"读取 spike 层面的 numpy 文件并返回 DataFrame: [cluster, time]\n",
    "    time 使用原始采样点，不做单位转换。\n",
    "    \"\"\"\n",
    "    spike_clusters = np.load(os.path.join(phy_dir, SPIKE_CLUSTERS_FILENAME))\n",
    "    spike_times = np.load(os.path.join(phy_dir, SPIKE_TIMES_FILENAME))\n",
    "    # 展平为一维\n",
    "    spike_clusters = np.asarray(spike_clusters).reshape(-1)\n",
    "    spike_times = np.asarray(spike_times).reshape(-1)\n",
    "    if spike_clusters.shape[0] != spike_times.shape[0]:\n",
    "        raise ValueError(f\"spike_clusters 与 spike_times 行数不一致: {phy_dir}\")\n",
    "    df = pd.DataFrame({\n",
    "        'cluster_id': spike_clusters.astype(int),\n",
    "        'time': spike_times.astype(int),\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f038f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_SORT_DIR = \"/media/ubuntu/sda/Monkey/sorted_result_combined/20240112/Hub1-instance1_V1/phy_folder_for_kilosort/\"\n",
    "cluster_inf = load_cluster_info(phy_dir=ROOT_SORT_DIR)\n",
    "spike_inf = load_spike_level(phy_dir=ROOT_SORT_DIR)\n",
    "\n",
    "cluster_inf = cluster_inf[cluster_inf['group'] == 'good']\n",
    "spike_inf = spike_inf[spike_inf['cluster_id'].isin(cluster_inf['cluster_id'].unique())]\n",
    "\n",
    "\n",
    "from numpy import record\n",
    "time_mapping = {}\n",
    "\n",
    "for block in os.listdir(\"/media/ubuntu/sda/Monkey/TVSD/monkeyF/20240112\"):\n",
    "    record_temp = se.read_blackrock(f\"/media/ubuntu/sda/Monkey/TVSD/monkeyF/20240112/{block}/Hub1-instance1_B00{block[-1]}.ns6\")\n",
    "    time_mapping[block] = int(record_temp.get_total_duration() * 30000)\n",
    "\n",
    "\n",
    "spike_inf['block'] = None\n",
    "\n",
    "cumulative_time = 0\n",
    "time_boundaries = {}\n",
    "\n",
    "for i in [1, 2, 3, 4]:\n",
    "    block_key = f'Block_{i}'\n",
    "    if block_key in time_mapping:\n",
    "        time_boundaries[i] = (cumulative_time, cumulative_time + time_mapping[block_key])\n",
    "        cumulative_time += time_mapping[block_key]\n",
    "\n",
    "for block_id, (start_time, end_time) in time_boundaries.items():\n",
    "    mask = (spike_inf['time'] >= start_time) & (spike_inf['time'] < end_time)\n",
    "    spike_inf.loc[mask, 'block'] = block_id\n",
    "\n",
    "for block_id in spike_inf['block'].dropna().unique():\n",
    "    block_mask = spike_inf['block'] == block_id\n",
    "    block_spikes = spike_inf[block_mask]\n",
    "    \n",
    "    if len(block_spikes) > 0:\n",
    "        min_time = block_spikes['time'].min()\n",
    "        spike_inf.loc[block_mask, 'time'] = spike_inf.loc[block_mask, 'time'] - min_time\n",
    "\n",
    "\n",
    "spike_inf['array'] = 'Hub1_instance1'\n",
    "cluster_inf['array'] = 'Hub1_instance1'\n",
    "\n",
    "cluster_inf.to_csv(\"/media/ubuntu/sda/Monkey/sorted_result_combined/20240112/cluster_inf_Hub1-instance1_V1.csv\", index = False)\n",
    "spike_inf.to_csv(\"/media/ubuntu/sda/Monkey/sorted_result_combined/20240112/spike_inf_Hub1-instance1_V1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7492be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_SORT_DIR = \"/media/ubuntu/sda/Monkey/sorted_result_combined/20240112/Hub2-instance1_V1/phy_folder_for_kilosort/\"\n",
    "cluster_inf = load_cluster_info(phy_dir=ROOT_SORT_DIR)\n",
    "spike_inf = load_spike_level(phy_dir=ROOT_SORT_DIR)\n",
    "\n",
    "cluster_inf = cluster_inf[cluster_inf['group'] == 'good']\n",
    "spike_inf = spike_inf[spike_inf['cluster_id'].isin(cluster_inf['cluster_id'].unique())]\n",
    "\n",
    "\n",
    "from numpy import record\n",
    "time_mapping = {}\n",
    "\n",
    "for block in os.listdir(\"/media/ubuntu/sda/Monkey/TVSD/monkeyF/20240112\"):\n",
    "    record_temp = se.read_blackrock(f\"/media/ubuntu/sda/Monkey/TVSD/monkeyF/20240112/{block}/Hub2-instance1_B00{block[-1]}.ns6\")\n",
    "    time_mapping[block] = int(record_temp.get_total_duration() * 30000)\n",
    "\n",
    "\n",
    "spike_inf['block'] = None\n",
    "\n",
    "cumulative_time = 0\n",
    "time_boundaries = {}\n",
    "\n",
    "for i in [1, 2, 3, 4]:\n",
    "    block_key = f'Block_{i}'\n",
    "    if block_key in time_mapping:\n",
    "        time_boundaries[i] = (cumulative_time, cumulative_time + time_mapping[block_key])\n",
    "        cumulative_time += time_mapping[block_key]\n",
    "\n",
    "for block_id, (start_time, end_time) in time_boundaries.items():\n",
    "    mask = (spike_inf['time'] >= start_time) & (spike_inf['time'] < end_time)\n",
    "    spike_inf.loc[mask, 'block'] = block_id\n",
    "\n",
    "for block_id in spike_inf['block'].dropna().unique():\n",
    "    block_mask = spike_inf['block'] == block_id\n",
    "    block_spikes = spike_inf[block_mask]\n",
    "    \n",
    "    if len(block_spikes) > 0:\n",
    "        min_time = block_spikes['time'].min()\n",
    "        spike_inf.loc[block_mask, 'time'] = spike_inf.loc[block_mask, 'time'] - min_time\n",
    "\n",
    "\n",
    "spike_inf['array'] = 'Hub2_instance1'\n",
    "cluster_inf['array'] = 'Hub2_instance1'\n",
    "\n",
    "cluster_inf.to_csv(\"/media/ubuntu/sda/Monkey/sorted_result_combined/20240112/cluster_inf_Hub2-instance1_V1.csv\", index = False)\n",
    "spike_inf.to_csv(\"/media/ubuntu/sda/Monkey/sorted_result_combined/20240112/spike_inf_Hub2-instance1_V1.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

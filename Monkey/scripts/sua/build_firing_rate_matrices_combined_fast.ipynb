{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ neo和elephant库已安装\n",
            "可用CPU核心数: 32\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "import multiprocessing as mp\n",
        "from functools import partial\n",
        "import time\n",
        "\n",
        "# 检查是否安装了必要的库\n",
        "try:\n",
        "    import neo\n",
        "    import quantities as pq\n",
        "    from elephant.statistics import instantaneous_rate\n",
        "    from elephant.kernels import GaussianKernel\n",
        "    print(\"✅ neo和elephant库已安装\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ 警告: 无法导入neo和elephant库: {e}\")\n",
        "    neo = None\n",
        "    pq = None\n",
        "    instantaneous_rate = None\n",
        "    GaussianKernel = None\n",
        "\n",
        "print(f\"可用CPU核心数: {mp.cpu_count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "处理日期: 20240112\n",
            "合并的arrays: ['Hub1-instance1_V1', 'Hub2-instance1_V1']\n",
            "采样率: 30000.0 Hz\n",
            "采样周期: 0.01 秒\n",
            "高斯核标准差: 0.02 秒\n"
          ]
        }
      ],
      "source": [
        "# 配置参数\n",
        "DATE_STR = \"20240112\"\n",
        "SAMPLE_RATE = 30000.0  # Hz\n",
        "SAMPLING_PERIOD_SEC = 0.01  # 20ms采样周期\n",
        "KERNEL_SIGMA_SEC = 0.02  # 40ms高斯核\n",
        "\n",
        "# 文件路径\n",
        "BASE_DIR = \"/media/ubuntu/sda/Monkey/sorted_result_combined\"\n",
        "TRIGGER_DIR = \"/media/ubuntu/sda/Monkey/trigger\"\n",
        "TRAIN_IMAGE_CSV = \"/media/ubuntu/sda/Monkey/scripts/test_image.csv\"\n",
        "\n",
        "# 要合并的array数据\n",
        "ARRAYS = [\n",
        "    'Hub1-instance1_V1',\n",
        "    'Hub2-instance1_V1'\n",
        "]\n",
        "\n",
        "print(f\"处理日期: {DATE_STR}\")\n",
        "print(f\"合并的arrays: {ARRAYS}\")\n",
        "print(f\"采样率: {SAMPLE_RATE} Hz\")\n",
        "print(f\"采样周期: {SAMPLING_PERIOD_SEC} 秒\")\n",
        "print(f\"高斯核标准差: {KERNEL_SIGMA_SEC} 秒\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "加载图像类别映射...\n",
            "加载了 11 个图像类别映射\n",
            "类别示例: ['bulldozer', 'helicopter', 'wig', 'monkey', 'iguana', 'whip', 'tamale', 'dough', 'wasp', 'key']\n"
          ]
        }
      ],
      "source": [
        "def load_image_class_mapping() -> Dict[int, str]:\n",
        "    \"\"\"加载图像ID到类别的映射\"\"\"\n",
        "    print(\"加载图像类别映射...\")\n",
        "    \n",
        "    # 读取train_image.csv\n",
        "    train_df = pd.read_csv(TRAIN_IMAGE_CSV)\n",
        "    \n",
        "    # 从文件路径中提取图像ID\n",
        "    image_id_to_class = {}\n",
        "    \n",
        "    # 方法1: 如果things_path包含ID信息\n",
        "    for _, row in train_df.iterrows():\n",
        "        things_path = row['things_path']\n",
        "        class_name = row['class']\n",
        "        \n",
        "        # 尝试从路径中提取数字作为图像ID\n",
        "        import re\n",
        "        numbers = re.findall(r'\\d+', things_path)\n",
        "        if numbers:\n",
        "            # 使用路径中的最后一个数字作为可能的图像ID\n",
        "            image_id = int(numbers[-1])\n",
        "            image_id_to_class[image_id] = class_name\n",
        "    \n",
        "    print(f\"加载了 {len(image_id_to_class)} 个图像类别映射\")\n",
        "    print(f\"类别示例: {list(set(list(image_id_to_class.values())[:10]))}\")\n",
        "    \n",
        "    return image_id_to_class\n",
        "\n",
        "# 加载图像类别映射\n",
        "image_class_mapping = load_image_class_mapping()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 简化的快速处理方法 - 使用binning而不是instantaneous rate\n",
        "def compute_firing_rate_matrix_binned(\n",
        "    start_sec: float,\n",
        "    stop_sec: float,\n",
        "    neuron_order: pd.DataFrame,\n",
        "    spike_times_by_neuron: Dict[Tuple[str, int], np.ndarray],\n",
        "    bin_width_sec: float\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"使用binning方法快速计算发放率矩阵\"\"\"\n",
        "    duration = max(0.0, float(stop_sec) - float(start_sec))\n",
        "    if duration <= 0:\n",
        "        edges = np.array([start_sec, start_sec + bin_width_sec], dtype=float)\n",
        "        n_bins = 1\n",
        "    else:\n",
        "        n_bins = int(np.ceil(duration / bin_width_sec))\n",
        "        edges = start_sec + np.arange(n_bins + 1, dtype=float) * bin_width_sec\n",
        "        if edges[-1] < stop_sec:\n",
        "            edges = np.append(edges, stop_sec)\n",
        "            n_bins = len(edges) - 1\n",
        "    \n",
        "    n_neurons = len(neuron_order)\n",
        "    fr = np.zeros((n_neurons, n_bins), dtype=float)\n",
        "    \n",
        "    # 对每个神经元计算binned发放率\n",
        "    for _, row in neuron_order.iterrows():\n",
        "        idx = int(row['neuron_index'])\n",
        "        key = (row['array'], int(row['cluster_id']))\n",
        "        t = spike_times_by_neuron.get(key)\n",
        "        \n",
        "        if t is None or t.size == 0:\n",
        "            continue\n",
        "        \n",
        "        # 过滤时间窗口内的spikes\n",
        "        mask = (t >= start_sec) & (t < stop_sec)\n",
        "        rel_t = t[mask] - start_sec\n",
        "        \n",
        "        if rel_t.size == 0:\n",
        "            continue\n",
        "        \n",
        "        # 使用numpy的histogram进行快速binning\n",
        "        hist, _ = np.histogram(rel_t, bins=edges - start_sec)\n",
        "        fr[idx, :] = hist / bin_width_sec  # 转换为Hz\n",
        "    \n",
        "    return fr, edges\n",
        "\n",
        "def build_firing_rate_matrices_fast(\n",
        "    triggers_df: pd.DataFrame,\n",
        "    neuron_order: pd.DataFrame,\n",
        "    spikes_by_neuron: Dict[Tuple[str, int], np.ndarray],\n",
        "    bin_width_sec: float,\n",
        "    image_class_mapping: Dict[int, str]\n",
        ") -> Dict[str, Dict[str, object]]:\n",
        "    \"\"\"快速构建所有trigger窗口的发放率矩阵\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(f\"开始快速构建发放率矩阵，共 {len(triggers_df)} 个试次...\")\n",
        "    \n",
        "    fr_dict: Dict[str, Dict[str, object]] = {}\n",
        "    \n",
        "    for idx, (_, row) in enumerate(triggers_df.iterrows()):\n",
        "        if idx % 100 == 0:\n",
        "            print(f\"处理进度: {idx}/{len(triggers_df)} ({idx/len(triggers_df)*100:.1f}%)\")\n",
        "        \n",
        "        start_sec = float(row['start_time'])\n",
        "        stop_sec = float(row['stop_time'])\n",
        "        \n",
        "        # 生成唯一键\n",
        "        train_val = int(row['train_image']) if not pd.isna(row['train_image']) else 0\n",
        "        test_val = int(row['test_image']) if not pd.isna(row['test_image']) else 0\n",
        "        \n",
        "        is_train = train_val != 0\n",
        "        if is_train:\n",
        "            image_id = train_val\n",
        "            phase = 'train'\n",
        "        else:\n",
        "            image_id = test_val\n",
        "            phase = 'test'\n",
        "        \n",
        "        rep_num = int(row['image_rep_num']) if not pd.isna(row['image_rep_num']) else 0\n",
        "        single_rep = int(row['single_train_rep']) if not pd.isna(row['single_train_rep']) else 0\n",
        "        key = f\"{phase}_{image_id}_{rep_num}_{single_rep}\"\n",
        "        \n",
        "        try:\n",
        "            # 使用快速binning方法\n",
        "            fr_mat, edges = compute_firing_rate_matrix_binned(\n",
        "                start_sec=start_sec,\n",
        "                stop_sec=stop_sec,\n",
        "                neuron_order=neuron_order,\n",
        "                spike_times_by_neuron=spikes_by_neuron,\n",
        "                bin_width_sec=bin_width_sec,\n",
        "            )\n",
        "            \n",
        "            # 获取图像类别\n",
        "            image_class = image_class_mapping.get(image_id, 'unknown')\n",
        "            \n",
        "            fr_dict[key] = {\n",
        "                'phase': phase,\n",
        "                'image_id': image_id,\n",
        "                'image_class': image_class,\n",
        "                'image_rep_num': rep_num,\n",
        "                'single_train_rep': single_rep,\n",
        "                'edges_sec': edges,\n",
        "                'bin_width_sec': bin_width_sec,\n",
        "                'neuron_order': neuron_order.copy(),\n",
        "                'firing_rate': fr_mat,\n",
        "                'start_time': start_sec,\n",
        "                'stop_time': stop_sec,\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"警告: 处理试次 {key} 时出错: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n完成！成功构建了 {len(fr_dict)} 个发放率矩阵\")\n",
        "    print(f\"总耗时: {time.time() - start_time:.2f} 秒\")\n",
        "    print(f\"平均每个试次: {(time.time() - start_time) / len(triggers_df):.3f} 秒\")\n",
        "    \n",
        "    return fr_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 快速加载合并数据\n",
        "def load_combined_data_fast(date_str: str, arrays: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"快速加载合并的cluster和spike数据\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    cluster_frames = []\n",
        "    spike_frames = []\n",
        "    \n",
        "    for array_name in arrays:\n",
        "        cluster_path = os.path.join(BASE_DIR, date_str, f\"cluster_inf_{array_name}.csv\")\n",
        "        spike_path = os.path.join(BASE_DIR, date_str, f\"spike_inf_{array_name}.csv\")\n",
        "        \n",
        "        if os.path.exists(cluster_path) and os.path.exists(spike_path):\n",
        "            print(f\"加载 {array_name} 数据...\")\n",
        "            \n",
        "            cluster_df = pd.read_csv(cluster_path)\n",
        "            spike_df = pd.read_csv(spike_path)\n",
        "            \n",
        "            cluster_frames.append(cluster_df)\n",
        "            spike_frames.append(spike_df)\n",
        "            \n",
        "            print(f\"  - cluster数据: {len(cluster_df)} 个神经元\")\n",
        "            print(f\"  - spike数据: {len(spike_df)} 个spikes\")\n",
        "        else:\n",
        "            print(f\"警告: {array_name} 数据文件不存在\")\n",
        "    \n",
        "    if not cluster_frames or not spike_frames:\n",
        "        raise ValueError(\"没有找到有效的数据文件\")\n",
        "    \n",
        "    # 合并数据\n",
        "    combined_cluster = pd.concat(cluster_frames, ignore_index=True)\n",
        "    combined_spike = pd.concat(spike_frames, ignore_index=True)\n",
        "    \n",
        "    print(f\"\\n合并后数据:\")\n",
        "    print(f\"- 总神经元数: {len(combined_cluster)}\")\n",
        "    print(f\"- 总spikes数: {len(combined_spike)}\")\n",
        "    print(f\"- 加载耗时: {time.time() - start_time:.2f} 秒\")\n",
        "    \n",
        "    return combined_cluster, combined_spike\n",
        "\n",
        "# 加载合并数据\n",
        "cluster_df, spike_df = load_combined_data_fast(DATE_STR, ARRAYS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 快速加载trigger数据（排除Block1）\n",
        "def load_triggers_fast(date_str: str) -> pd.DataFrame:\n",
        "    \"\"\"快速加载trigger数据，排除Block1\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    trigger_files = []\n",
        "    \n",
        "    # 获取所有相关的trigger文件（排除Block1）\n",
        "    for block_num in [2, 3, 4]:  # 排除Block1\n",
        "        for instance_num in [1, 2]:\n",
        "            trigger_file = os.path.join(TRIGGER_DIR, f\"trigger_df_monkyF_{date_str}_B{block_num}_instance{instance_num}.csv\")\n",
        "            if os.path.exists(trigger_file):\n",
        "                trigger_files.append(trigger_file)\n",
        "                print(f\"找到trigger文件: Block{block_num}_instance{instance_num}\")\n",
        "            else:\n",
        "                print(f\"警告: trigger文件不存在: {trigger_file}\")\n",
        "    \n",
        "    if not trigger_files:\n",
        "        raise ValueError(\"没有找到有效的trigger文件\")\n",
        "    \n",
        "    # 加载并合并trigger数据\n",
        "    trigger_frames = []\n",
        "    for trigger_file in trigger_files:\n",
        "        df = pd.read_csv(trigger_file)\n",
        "        trigger_frames.append(df)\n",
        "    \n",
        "    combined_triggers = pd.concat(trigger_frames, ignore_index=True)\n",
        "    \n",
        "    # 数据清理\n",
        "    required_cols = ['start_time', 'stop_time', 'train_image', 'test_image', 'image_rep_num', 'single_train_rep']\n",
        "    for col in required_cols:\n",
        "        if col not in combined_triggers.columns:\n",
        "            raise ValueError(f\"Trigger文件缺少列: {col}\")\n",
        "    \n",
        "    # 仅保留有效图像试次\n",
        "    if 'valid_image' in combined_triggers.columns:\n",
        "        combined_triggers = combined_triggers[combined_triggers['valid_image'] == 1].copy()\n",
        "    \n",
        "    # 填充NaN值\n",
        "    combined_triggers['train_image'] = combined_triggers['train_image'].fillna(0).astype(int)\n",
        "    combined_triggers['test_image'] = combined_triggers['test_image'].fillna(0).astype(int)\n",
        "    combined_triggers['image_rep_num'] = combined_triggers['image_rep_num'].fillna(0).astype(int)\n",
        "    combined_triggers['single_train_rep'] = combined_triggers['single_train_rep'].fillna(0).astype(int)\n",
        "    \n",
        "    print(f\"\\n加载的trigger数据:\")\n",
        "    print(f\"- 总试次数: {len(combined_triggers)}\")\n",
        "    print(f\"- 时间范围: {combined_triggers['start_time'].min():.2f} - {combined_triggers['stop_time'].max():.2f} 秒\")\n",
        "    print(f\"- 加载耗时: {time.time() - start_time:.2f} 秒\")\n",
        "    \n",
        "    return combined_triggers\n",
        "\n",
        "# 加载trigger数据（排除Block1）\n",
        "triggers_df = load_triggers_fast(DATE_STR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 构建神经元索引和分组spike时间\n",
        "def build_neuron_index_and_group_spikes(cluster_df: pd.DataFrame, spike_df: pd.DataFrame, sample_rate: float) -> Tuple[pd.DataFrame, Dict[Tuple[str, int], np.ndarray]]:\n",
        "    \"\"\"一次性构建神经元索引和分组spike时间\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # 构建神经元索引\n",
        "    if 'group' in cluster_df.columns:\n",
        "        cluster_df = cluster_df[cluster_df['group'] == 'good'].copy()\n",
        "    \n",
        "    neurons = cluster_df[['array', 'cluster_id']].copy()\n",
        "    neurons['array'] = neurons['array'].astype(str)\n",
        "    neurons['cluster_id'] = neurons['cluster_id'].astype(int)\n",
        "    neurons = neurons.drop_duplicates().reset_index(drop=True)\n",
        "    neurons['neuron_index'] = np.arange(len(neurons), dtype=int)\n",
        "    \n",
        "    # 创建映射字典\n",
        "    neuron_mapping: Dict[Tuple[str, int], int] = {\n",
        "        (row['array'], row['cluster_id']): int(row['neuron_index'])\n",
        "        for _, row in neurons.iterrows()\n",
        "    }\n",
        "    \n",
        "    # 分组spike时间\n",
        "    s = spike_df[['array', 'cluster_id', 'time']].copy()\n",
        "    s['array'] = s['array'].astype(str)\n",
        "    s['cluster_id'] = s['cluster_id'].astype(int)\n",
        "    s['t_sec'] = s['time'].astype(float) / float(sample_rate)\n",
        "    \n",
        "    # 快速分组\n",
        "    spikes_by_neuron: Dict[Tuple[str, int], np.ndarray] = {}\n",
        "    for (arr, clu), g in s.groupby(['array', 'cluster_id']):\n",
        "        spikes_by_neuron[(arr, int(clu))] = g['t_sec'].to_numpy()\n",
        "    \n",
        "    print(f\"构建神经元索引和分组spike时间:\")\n",
        "    print(f\"- 总神经元数: {len(neurons)}\")\n",
        "    print(f\"- 总spike数: {sum(len(spikes) for spikes in spikes_by_neuron.values())}\")\n",
        "    print(f\"- 各array的神经元数:\")\n",
        "    for array_name in neurons['array'].unique():\n",
        "        count = len(neurons[neurons['array'] == array_name])\n",
        "        print(f\"  - {array_name}: {count}\")\n",
        "    print(f\"- 处理耗时: {time.time() - start_time:.2f} 秒\")\n",
        "    \n",
        "    return neurons, spikes_by_neuron\n",
        "\n",
        "# 构建神经元索引和分组spike时间\n",
        "neuron_order, spikes_by_neuron = build_neuron_index_and_group_spikes(cluster_df, spike_df, SAMPLE_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 构建发放率矩阵（快速版本）\n",
        "firing_rate_dict = build_firing_rate_matrices_fast(\n",
        "    triggers_df,\n",
        "    neuron_order,\n",
        "    spikes_by_neuron,\n",
        "    SAMPLING_PERIOD_SEC,\n",
        "    image_class_mapping\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分析结果（包含类别信息）\n",
        "print(\"=== 发放率矩阵分析（含类别信息）===\")\n",
        "print(f\"总试次数: {len(firing_rate_dict)}\")\n",
        "\n",
        "# 统计各phase的试次数\n",
        "phase_counts = {}\n",
        "class_counts = {}\n",
        "image_counts = {}\n",
        "\n",
        "for key, data in firing_rate_dict.items():\n",
        "    phase = data['phase']\n",
        "    image_id = data['image_id']\n",
        "    image_class = data['image_class']\n",
        "    \n",
        "    phase_counts[phase] = phase_counts.get(phase, 0) + 1\n",
        "    class_counts[image_class] = class_counts.get(image_class, 0) + 1\n",
        "    image_counts[image_id] = image_counts.get(image_id, 0) + 1\n",
        "\n",
        "print(f\"\\n各phase的试次数:\")\n",
        "for phase, count in sorted(phase_counts.items()):\n",
        "    print(f\"  {phase}: {count}\")\n",
        "\n",
        "print(f\"\\n各图像类别的试次数（前20个）:\")\n",
        "sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "for class_name, count in sorted_classes[:20]:\n",
        "    print(f\"  {class_name}: {count}\")\n",
        "\n",
        "print(f\"\\n总类别数: {len(class_counts)}\")\n",
        "print(f\"唯一图像数: {len(image_counts)}\")\n",
        "\n",
        "# 检查矩阵维度\n",
        "if firing_rate_dict:\n",
        "    sample_key = list(firing_rate_dict.keys())[0]\n",
        "    sample_data = firing_rate_dict[sample_key]\n",
        "    fr_shape = sample_data['firing_rate'].shape\n",
        "    print(f\"\\n发放率矩阵维度: {fr_shape}\")\n",
        "    print(f\"  - 神经元数: {fr_shape[0]}\")\n",
        "    print(f\"  - 时间bins: {fr_shape[1]}\")\n",
        "    print(f\"  - 时间窗口长度: {sample_data['stop_time'] - sample_data['start_time']:.3f} 秒\")\n",
        "    print(f\"  - 每个bin长度: {sample_data['bin_width_sec']:.3f} 秒\")\n",
        "    print(f\"  - 示例类别: {sample_data['image_class']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存结果（包含类别信息）\n",
        "def save_firing_rate_dict_with_classes(fr_dict: Dict[str, Dict[str, object]], output_dir: str, date_str: str):\n",
        "    \"\"\"保存发放率字典到文件（包含类别信息）\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # 保存为npz格式\n",
        "    npz_path = os.path.join(output_dir, f\"firing_rate_matrices_{date_str}_combined_with_classes.npz\")\n",
        "    pack = {}\n",
        "    \n",
        "    for k, v in fr_dict.items():\n",
        "        pack[f'{k}__firing_rate'] = v['firing_rate']\n",
        "        pack[f'{k}__edges_sec'] = v['edges_sec']\n",
        "        pack[f'{k}__meta'] = np.array([\n",
        "            v['phase'],\n",
        "            str(v['image_id']),\n",
        "            v['image_class'],\n",
        "            str(v['image_rep_num']),\n",
        "            str(v['single_train_rep']),\n",
        "            str(v['bin_width_sec']),\n",
        "            str(v['start_time']),\n",
        "            str(v['stop_time'])\n",
        "        ], dtype=object)\n",
        "    \n",
        "    # 保存神经元顺序信息\n",
        "    if fr_dict:\n",
        "        sample_data = list(fr_dict.values())[0]\n",
        "        neuron_order = sample_data['neuron_order']\n",
        "        pack['neuron_order_csv'] = neuron_order.to_csv(index=False).encode('utf-8')\n",
        "    \n",
        "    np.savez_compressed(npz_path, **pack)\n",
        "    print(f\"发放率矩阵已保存到: {npz_path}\")\n",
        "    \n",
        "    # 保存汇总信息（包含类别）\n",
        "    summary_path = os.path.join(output_dir, f\"firing_rate_summary_{date_str}_combined_with_classes.csv\")\n",
        "    summary_data = []\n",
        "    \n",
        "    for key, data in fr_dict.items():\n",
        "        summary_data.append({\n",
        "            'key': key,\n",
        "            'phase': data['phase'],\n",
        "            'image_id': data['image_id'],\n",
        "            'image_class': data['image_class'],\n",
        "            'image_rep_num': data['image_rep_num'],\n",
        "            'single_train_rep': data['single_train_rep'],\n",
        "            'start_time': data['start_time'],\n",
        "            'stop_time': data['stop_time'],\n",
        "            'duration': data['stop_time'] - data['start_time'],\n",
        "            'n_neurons': data['firing_rate'].shape[0],\n",
        "            'n_bins': data['firing_rate'].shape[1]\n",
        "        })\n",
        "    \n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    print(f\"汇总信息已保存到: {summary_path}\")\n",
        "    \n",
        "    # 保存类别统计信息\n",
        "    class_stats_path = os.path.join(output_dir, f\"class_statistics_{date_str}_combined.csv\")\n",
        "    class_stats = []\n",
        "    \n",
        "    for class_name, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        class_stats.append({\n",
        "            'class_name': class_name,\n",
        "            'trial_count': count,\n",
        "            'percentage': count / len(fr_dict) * 100\n",
        "        })\n",
        "    \n",
        "    class_stats_df = pd.DataFrame(class_stats)\n",
        "    class_stats_df.to_csv(class_stats_path, index=False)\n",
        "    print(f\"类别统计信息已保存到: {class_stats_path}\")\n",
        "    \n",
        "    return npz_path, summary_path, class_stats_path\n",
        "\n",
        "# 保存结果\n",
        "output_dir = \"/media/ubuntu/sda/Monkey/sorted_result_combined/firing_rate_matrices\"\n",
        "npz_path, summary_path, class_stats_path = save_firing_rate_dict_with_classes(firing_rate_dict, output_dir, DATE_STR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化示例（包含类别信息）\n",
        "if firing_rate_dict:\n",
        "    # 选择一个示例进行可视化\n",
        "    sample_key = list(firing_rate_dict.keys())[0]\n",
        "    sample_data = firing_rate_dict[sample_key]\n",
        "    fr_matrix = sample_data['firing_rate']\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. 整体发放率矩阵热图\n",
        "    ax1 = axes[0, 0]\n",
        "    im1 = ax1.imshow(fr_matrix, aspect='auto', cmap='viridis', interpolation='nearest')\n",
        "    ax1.set_title(f'发放率矩阵热图\\n{sample_key}\\n类别: {sample_data[\"image_class\"]}')\n",
        "    ax1.set_xlabel('时间bins')\n",
        "    ax1.set_ylabel('神经元索引')\n",
        "    plt.colorbar(im1, ax=ax1, label='发放率 (Hz)')\n",
        "    \n",
        "    # 2. 平均发放率随时间变化\n",
        "    ax2 = axes[0, 1]\n",
        "    mean_fr = np.mean(fr_matrix, axis=0)\n",
        "    time_bins = np.arange(len(mean_fr)) * sample_data['bin_width_sec']\n",
        "    ax2.plot(time_bins, mean_fr, 'b-', linewidth=2)\n",
        "    ax2.set_title('平均发放率随时间变化')\n",
        "    ax2.set_xlabel('时间 (秒)')\n",
        "    ax2.set_ylabel('平均发放率 (Hz)')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. 神经元平均发放率分布\n",
        "    ax3 = axes[1, 0]\n",
        "    neuron_mean_fr = np.mean(fr_matrix, axis=1)\n",
        "    ax3.hist(neuron_mean_fr, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax3.set_title('神经元平均发放率分布')\n",
        "    ax3.set_xlabel('平均发放率 (Hz)')\n",
        "    ax3.set_ylabel('神经元数量')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. 发放率统计和类别信息\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.axis('off')\n",
        "    stats_text = f\"\"\"\n",
        "    统计信息:\n",
        "    神经元数: {fr_matrix.shape[0]}\n",
        "    时间bins: {fr_matrix.shape[1]}\n",
        "    时间窗口: {sample_data['stop_time'] - sample_data['start_time']:.3f} 秒\n",
        "    平均发放率: {np.mean(fr_matrix):.2f} Hz\n",
        "    最大发放率: {np.max(fr_matrix):.2f} Hz\n",
        "    最小发放率: {np.min(fr_matrix):.2f} Hz\n",
        "    \n",
        "    试次信息:\n",
        "    Phase: {sample_data['phase']}\n",
        "    Image ID: {sample_data['image_id']}\n",
        "    图像类别: {sample_data['image_class']}\n",
        "    Rep Num: {sample_data['image_rep_num']}\n",
        "    \"\"\"\n",
        "    ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=11,\n",
        "             verticalalignment='top', fontfamily='monospace')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n可视化完成！示例数据来自试次: {sample_key}\")\n",
        "    print(f\"图像类别: {sample_data['image_class']}\")\n",
        "    \n",
        "    # 类别分布饼图\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "    class_names = [item[0] for item in top_classes]\n",
        "    class_values = [item[1] for item in top_classes]\n",
        "    \n",
        "    plt.pie(class_values, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
        "    plt.title('前15个图像类别的试次分布')\n",
        "    plt.axis('equal')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n类别分布统计完成！\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spike_sorting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

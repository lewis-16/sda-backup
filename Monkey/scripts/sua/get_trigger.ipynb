{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e336ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/spike_sorting/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import spikeinterface as si\n",
    "import matplotlib.pyplot as plt\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.widgets as sw\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import json\n",
    "import probeinterface\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71910d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:227: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Considerscipy.io.matlab._mio5.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    }
   ],
   "source": [
    "block = 1\n",
    "instance = 1\n",
    "\n",
    "recording = se.read_blackrock(f'/media/ubuntu/sda/Monkey/TVSD/monkeyF/20240115/Block_{block}/NSP-instance{instance}_B00{block}.ns6')\n",
    "if recording.get_num_segments() > 1:\n",
    "        recording_list = []\n",
    "        for i in range(recording.get_num_segments()):\n",
    "            recording_list.append(recording.select_segments(i))\n",
    "        recording = si.concatenate_recordings(recording_list)\n",
    "\n",
    "channel_data = recording.get_traces(channel_ids=['527']).flatten()\n",
    "sampling_rate = recording.get_sampling_frequency()\n",
    "\n",
    "threshold = -20000\n",
    "threshold_crossings = np.where(channel_data < threshold)[0]\n",
    "\n",
    "window_size = 500\n",
    "padding = 50\n",
    "segments = []\n",
    "current_segment = []\n",
    "\n",
    "for i in range(len(threshold_crossings)):\n",
    "    if not current_segment:\n",
    "        current_segment = [threshold_crossings[i]]\n",
    "    else:\n",
    "        if threshold_crossings[i] - current_segment[-1] <= window_size:\n",
    "            start_idx = max(0, threshold_crossings[i] - padding)\n",
    "            end_idx = min(len(channel_data), threshold_crossings[i] + padding)\n",
    "            if any(channel_data[start_idx:end_idx] < threshold):\n",
    "                current_segment.append(threshold_crossings[i])\n",
    "        else:\n",
    "            segments.append(current_segment)\n",
    "            current_segment = [threshold_crossings[i]]\n",
    "\n",
    "if current_segment:\n",
    "    segments.append(current_segment)\n",
    "\n",
    "start_times = []\n",
    "stop_times = []\n",
    "\n",
    "for segment in segments:\n",
    "    start_time = segment[0] / sampling_rate\n",
    "    stop_time = segment[-1] / sampling_rate\n",
    "    start_times.append(start_time)\n",
    "    stop_times.append(stop_time)\n",
    "\n",
    "trigger_df = pd.DataFrame({\n",
    "    'start_time': start_times,\n",
    "    'stop_time': stop_times\n",
    "})\n",
    "data = loadmat(f'/media/ubuntu/sda/Monkey/TVSD/monkeyF/_logs/THINGS_monkeyF_20240115_B{block}.mat')\n",
    "trigger_df = pd.concat((trigger_df, pd.DataFrame(data['MAT'])), axis=1)\n",
    "trigger_df.columns = ['start_time', 'stop_time', 'id', 'train_image', 'test_image', 'image_rep_num', 'single_train_rep', 'valid_image']\n",
    "trigger_df.to_csv(f\"/media/ubuntu/sda/Monkey/trigger/trigger_df_monkyF_20240115_B{block}_instance{instance}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32dae148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/spike_sorting/lib/python3.11/site-packages/scipy/io/matlab/_mio.py:235: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Considerscipy.io.matlab.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    }
   ],
   "source": [
    "block = 1\n",
    "instance = 1\n",
    "\n",
    "recording = se.read_blackrock(f'/media/ubuntu/sda/Monkey/TVSD/monkeyF/20240115/Block_{block}/NSP-instance{instance}_B00{block}.ns6')\n",
    "if recording.get_num_segments() > 1:\n",
    "        recording_list = []\n",
    "        for i in range(recording.get_num_segments()):\n",
    "            recording_list.append(recording.select_segments(i))\n",
    "        recording = si.concatenate_recordings(recording_list)\n",
    "\n",
    "channel_data = recording.get_traces(channel_ids=['527']).flatten()\n",
    "sampling_rate = recording.get_sampling_frequency()\n",
    "\n",
    "threshold = -20000\n",
    "threshold_crossings = np.where(channel_data < threshold)[0]\n",
    "\n",
    "window_size = 500\n",
    "padding = 50\n",
    "segments = []\n",
    "current_segment = []\n",
    "\n",
    "for i in range(len(threshold_crossings)):\n",
    "    if not current_segment:\n",
    "        current_segment = [threshold_crossings[i]]\n",
    "    else:\n",
    "        if threshold_crossings[i] - current_segment[-1] <= window_size:\n",
    "            start_idx = max(0, threshold_crossings[i] - padding)\n",
    "            end_idx = min(len(channel_data), threshold_crossings[i] + padding)\n",
    "            if any(channel_data[start_idx:end_idx] < threshold):\n",
    "                current_segment.append(threshold_crossings[i])\n",
    "        else:\n",
    "            segments.append(current_segment)\n",
    "            current_segment = [threshold_crossings[i]]\n",
    "\n",
    "if current_segment:\n",
    "    segments.append(current_segment)\n",
    "\n",
    "start_times = []\n",
    "stop_times = []\n",
    "\n",
    "for segment in segments:\n",
    "    start_time = segment[0] / sampling_rate\n",
    "    stop_time = segment[-1] / sampling_rate\n",
    "    start_times.append(start_time)\n",
    "    stop_times.append(stop_time)\n",
    "\n",
    "trigger_df = pd.DataFrame({\n",
    "    'start_time': start_times,\n",
    "    'stop_time': stop_times\n",
    "})\n",
    "data = loadmat(f'/media/ubuntu/sda/Monkey/TVSD/monkeyF/_logs/THINGS_monkeyF_20240115_B{block}.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brpylib import NevFile\n",
    "file = NevFile(\"/media/ubuntu/sda/Monkey/TVSD/monkeyF/20240112/Block_1/NSP-instance1_B001.nev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e2eaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NSP-instance1_B001.nev opened\n"
     ]
    }
   ],
   "source": [
    "file = NevFile(\"/media/ubuntu/sda/Monkey/TVSD/monkeyF/20240112/Block_1/NSP-instance1_B001.nev\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

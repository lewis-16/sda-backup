{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import neo\n",
        "import quantities as pq\n",
        "from elephant.statistics import instantaneous_rate\n",
        "from elephant.kernels import GaussianKernel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "处理日期: 20240112\n",
            "合并的arrays: ['Hub1-instance1_V1', 'Hub2-instance1_V1']\n",
            "采样率: 30000.0 Hz\n"
          ]
        }
      ],
      "source": [
        "# 配置参数\n",
        "DATE_STR = \"20240112\"\n",
        "SAMPLE_RATE = 30000.0  # Hz\n",
        "\n",
        "\n",
        "# 文件路径\n",
        "BASE_DIR = \"/media/ubuntu/sda/Monkey/sorted_result_combined\"\n",
        "TRIGGER_DIR = \"/media/ubuntu/sda/Monkey/trigger\"\n",
        "\n",
        "# 要合并的array数据\n",
        "ARRAYS = [\n",
        "    'Hub1-instance1_V1',\n",
        "    'Hub2-instance1_V1'\n",
        "]\n",
        "\n",
        "print(f\"处理日期: {DATE_STR}\")\n",
        "print(f\"合并的arrays: {ARRAYS}\")\n",
        "print(f\"采样率: {SAMPLE_RATE} Hz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "加载 Hub1-instance1_V1 数据...\n",
            "  - cluster数据: 120 个神经元\n",
            "  - spike数据: 13625231 个spikes\n",
            "加载 Hub2-instance1_V1 数据...\n",
            "  - cluster数据: 124 个神经元\n",
            "  - spike数据: 16888023 个spikes\n",
            "\n",
            "合并后数据:\n",
            "- 总神经元数: 244\n",
            "- 总spikes数: 30513254\n"
          ]
        }
      ],
      "source": [
        "def load_combined_data(date_str: str, arrays: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"加载合并的cluster和spike数据\"\"\"\n",
        "    cluster_frames = []\n",
        "    spike_frames = []\n",
        "    \n",
        "    for array_name in arrays:\n",
        "        cluster_path = os.path.join(BASE_DIR, date_str, f\"cluster_inf_{array_name}.csv\")\n",
        "        spike_path = os.path.join(BASE_DIR, date_str, f\"spike_inf_{array_name}.csv\")\n",
        "        \n",
        "        if os.path.exists(cluster_path) and os.path.exists(spike_path):\n",
        "            print(f\"加载 {array_name} 数据...\")\n",
        "            cluster_df = pd.read_csv(cluster_path)\n",
        "            spike_df = pd.read_csv(spike_path)\n",
        "            \n",
        "            cluster_frames.append(cluster_df)\n",
        "            spike_frames.append(spike_df)\n",
        "            \n",
        "            print(f\"  - cluster数据: {len(cluster_df)} 个神经元\")\n",
        "            print(f\"  - spike数据: {len(spike_df)} 个spikes\")\n",
        "        else:\n",
        "            print(f\"警告: {array_name} 数据文件不存在\")\n",
        "    \n",
        "    if not cluster_frames or not spike_frames:\n",
        "        raise ValueError(\"没有找到有效的数据文件\")\n",
        "    \n",
        "    # 合并数据\n",
        "    combined_cluster = pd.concat(cluster_frames, ignore_index=True)\n",
        "    combined_spike = pd.concat(spike_frames, ignore_index=True)\n",
        "    \n",
        "    print(f\"\\n合并后数据:\")\n",
        "    print(f\"- 总神经元数: {len(combined_cluster)}\")\n",
        "    print(f\"- 总spikes数: {len(combined_spike)}\")\n",
        "    \n",
        "    return combined_cluster, combined_spike\n",
        "\n",
        "# 加载合并数据\n",
        "cluster_df, spike_df = load_combined_data(DATE_STR, ARRAYS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "找到trigger文件: Block2_instance1\n",
            "找到trigger文件: Block3_instance1\n",
            "找到trigger文件: Block4_instance1\n",
            "\n",
            "加载的trigger数据:\n",
            "- 总试次数: 3422\n",
            "- 时间范围: 8.64 - 1245.65 秒\n"
          ]
        }
      ],
      "source": [
        "def load_triggers_exclude_block1(date_str: str) -> pd.DataFrame:\n",
        "    \"\"\"加载trigger数据，排除Block1\"\"\"\n",
        "    trigger_files = []\n",
        "    \n",
        "    # 获取所有相关的trigger文件（排除Block1）\n",
        "    for block_num in [2, 3, 4]:  # 排除Block1\n",
        "        trigger_file = os.path.join(TRIGGER_DIR, f\"trigger_df_monkyF_{date_str}_B{block_num}_instance1.csv\")\n",
        "        if os.path.exists(trigger_file):\n",
        "            trigger_files.append(trigger_file)\n",
        "            print(f\"找到trigger文件: Block{block_num}_instance1\")\n",
        "        else:\n",
        "            print(f\"警告: trigger文件不存在: {trigger_file}\")\n",
        "    \n",
        "    if not trigger_files:\n",
        "        raise ValueError(\"没有找到有效的trigger文件\")\n",
        "    \n",
        "    # 加载并合并trigger数据\n",
        "    trigger_frames = []\n",
        "    for trigger_file in trigger_files:\n",
        "        df = pd.read_csv(trigger_file)\n",
        "        trigger_frames.append(df)\n",
        "    \n",
        "    combined_triggers = pd.concat(trigger_frames, ignore_index=True)\n",
        "    \n",
        "    # 数据清理\n",
        "    required_cols = ['start_time', 'stop_time', 'train_image', 'test_image', 'image_rep_num', 'single_train_rep']\n",
        "    for col in required_cols:\n",
        "        if col not in combined_triggers.columns:\n",
        "            raise ValueError(f\"Trigger文件缺少列: {col}\")\n",
        "    \n",
        "    # 仅保留有效图像试次\n",
        "    if 'valid_image' in combined_triggers.columns:\n",
        "        combined_triggers = combined_triggers[combined_triggers['valid_image'] == 1].copy()\n",
        "    \n",
        "    # 填充NaN值\n",
        "    combined_triggers['train_image'] = combined_triggers['train_image'].fillna(0)\n",
        "    combined_triggers['test_image'] = combined_triggers['test_image'].fillna(0)\n",
        "    combined_triggers['image_rep_num'] = combined_triggers['image_rep_num'].fillna(0)\n",
        "    combined_triggers['single_train_rep'] = combined_triggers['single_train_rep'].fillna(0)\n",
        "    \n",
        "    print(f\"\\n加载的trigger数据:\")\n",
        "    print(f\"- 总试次数: {len(combined_triggers)}\")\n",
        "    print(f\"- 时间范围: {combined_triggers['start_time'].min():.2f} - {combined_triggers['stop_time'].max():.2f} 秒\")\n",
        "    \n",
        "    return combined_triggers\n",
        "\n",
        "# 加载trigger数据（排除Block1）\n",
        "triggers_df = load_triggers_exclude_block1(DATE_STR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "构建神经元索引:\n",
            "- 总神经元数: 244\n",
            "- 各array的神经元数:\n",
            "  - Hub1_instance1: 120\n",
            "  - Hub2_instance1: 124\n"
          ]
        }
      ],
      "source": [
        "def build_neuron_index_combined(cluster_df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[Tuple[str, int], int]]:\n",
        "    \"\"\"构建合并数据的神经元索引\"\"\"\n",
        "    # 仅选择good neurons\n",
        "    if 'group' in cluster_df.columns:\n",
        "        cluster_df = cluster_df[cluster_df['group'] == 'good'].copy()\n",
        "    \n",
        "    # 检查必要的列\n",
        "    if 'array' not in cluster_df.columns or 'cluster_id' not in cluster_df.columns:\n",
        "        raise ValueError('cluster数据缺少array或cluster_id列')\n",
        "    \n",
        "    # 构建神经元索引\n",
        "    neurons = cluster_df[['array', 'cluster_id']].copy()\n",
        "    neurons['array'] = neurons['array'].astype(str)\n",
        "    neurons['cluster_id'] = neurons['cluster_id'].astype(int)\n",
        "    neurons = neurons.drop_duplicates().reset_index(drop=True)\n",
        "    neurons['neuron_index'] = np.arange(len(neurons), dtype=int)\n",
        "    \n",
        "    # 创建映射字典\n",
        "    mapping: Dict[Tuple[str, int], int] = {\n",
        "        (row['array'], row['cluster_id']): int(row['neuron_index'])\n",
        "        for _, row in neurons.iterrows()\n",
        "    }\n",
        "    \n",
        "    print(f\"构建神经元索引:\")\n",
        "    print(f\"- 总神经元数: {len(neurons)}\")\n",
        "    print(f\"- 各array的神经元数:\")\n",
        "    for array_name in neurons['array'].unique():\n",
        "        count = len(neurons[neurons['array'] == array_name])\n",
        "        print(f\"  - {array_name}: {count}\")\n",
        "    \n",
        "    return neurons, mapping\n",
        "\n",
        "# 构建神经元索引\n",
        "neuron_order, neuron_mapping = build_neuron_index_combined(cluster_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "分组spike时间:\n",
            "- 总神经元数: 244\n",
            "- 总spike数: 30513254\n"
          ]
        }
      ],
      "source": [
        "def group_spike_times_combined(spike_df: pd.DataFrame, sample_rate: float) -> Dict[Tuple[str, int], np.ndarray]:\n",
        "    \"\"\"按神经元分组spike时间，转换为秒\"\"\"\n",
        "    required_cols = ['array', 'cluster_id', 'time']\n",
        "    for col in required_cols:\n",
        "        if col not in spike_df.columns:\n",
        "            raise ValueError(f'spike数据缺少列: {col}')\n",
        "    \n",
        "    s = spike_df[['array', 'cluster_id', 'time']].copy()\n",
        "    s['array'] = s['array'].astype(str)\n",
        "    s['cluster_id'] = s['cluster_id'].astype(int)\n",
        "    \n",
        "    # 转换为秒\n",
        "    s['t_sec'] = s['time'].astype(float) / float(sample_rate)\n",
        "    \n",
        "    # 按神经元分组\n",
        "    grouped: Dict[Tuple[str, int], np.ndarray] = {}\n",
        "    for (arr, clu), g in s.groupby(['array', 'cluster_id']):\n",
        "        grouped[(arr, int(clu))] = g['t_sec'].to_numpy()\n",
        "    \n",
        "    print(f\"分组spike时间:\")\n",
        "    print(f\"- 总神经元数: {len(grouped)}\")\n",
        "    print(f\"- 总spike数: {sum(len(spikes) for spikes in grouped.values())}\")\n",
        "    \n",
        "    return grouped\n",
        "\n",
        "# 分组spike时间\n",
        "spikes_by_neuron = group_spike_times_combined(spike_df, SAMPLE_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_trigger_key(row: pd.Series) -> str:\n",
        "    \"\"\"为trigger行生成唯一键\"\"\"\n",
        "    train_val = int(row['train_image']) if not pd.isna(row['train_image']) else 0\n",
        "    test_val = int(row['test_image']) if not pd.isna(row['test_image']) else 0\n",
        "    \n",
        "    is_train = train_val != 0\n",
        "    if is_train:\n",
        "        image_id = train_val\n",
        "        phase = 'train'\n",
        "    else:\n",
        "        image_id = test_val\n",
        "        phase = 'test'\n",
        "    \n",
        "    rep_num = int(row['image_rep_num']) if not pd.isna(row['image_rep_num']) else 0\n",
        "    single_rep = int(row['single_train_rep']) if not pd.isna(row['single_train_rep']) else 0\n",
        "    \n",
        "    key = f\"{phase}_{image_id}_{rep_num}_{single_rep}\"\n",
        "    return key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_firing_rate_matrix_for_window_instant(\n",
        "    start_sec: float,\n",
        "    stop_sec: float,\n",
        "    neuron_order: pd.DataFrame,\n",
        "    spike_times_by_neuron: Dict[Tuple[str, int], np.ndarray],\n",
        "    sampling_period_sec: float,\n",
        "    kernel_sigma_sec: float\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"计算瞬时发放率矩阵\"\"\"\n",
        "    if neo is None or instantaneous_rate is None or GaussianKernel is None or pq is None:\n",
        "        raise RuntimeError('需要安装neo和elephant库以使用instantaneous_rate计算发放率')\n",
        "    \n",
        "    duration = max(0.0, float(stop_sec) - float(start_sec))\n",
        "    if duration <= 0:\n",
        "        edges = np.array([start_sec, start_sec + sampling_period_sec], dtype=float)\n",
        "        n_bins = 1\n",
        "    else:\n",
        "        n_bins = int(np.ceil(duration / sampling_period_sec))\n",
        "        edges = start_sec + np.arange(n_bins + 1, dtype=float) * sampling_period_sec\n",
        "        if edges[-1] < stop_sec:\n",
        "            edges = np.append(edges, stop_sec)\n",
        "            n_bins = len(edges) - 1\n",
        "    \n",
        "    n_neurons = len(neuron_order)\n",
        "    fr = np.zeros((n_neurons, n_bins), dtype=float)\n",
        "    \n",
        "    kernel = GaussianKernel(sigma=kernel_sigma_sec * pq.s)\n",
        "    sampling_period = sampling_period_sec * pq.s\n",
        "    effective_duration = max(duration, sampling_period_sec)\n",
        "    t_stop = effective_duration * pq.s\n",
        "    \n",
        "    # 对每个神经元计算瞬时发放率\n",
        "    for _, row in neuron_order.iterrows():\n",
        "        idx = int(row['neuron_index'])\n",
        "        key = (row['array'], int(row['cluster_id']))\n",
        "        t = spike_times_by_neuron.get(key)\n",
        "        \n",
        "        if t is None or t.size == 0:\n",
        "            continue\n",
        "        \n",
        "        # 相对时间（秒）\n",
        "        rel_t = t[(t >= start_sec) & (t < stop_sec)] - start_sec\n",
        "        if rel_t.size == 0:\n",
        "            continue\n",
        "        \n",
        "        st = neo.SpikeTrain(rel_t * pq.s, t_start=0 * pq.s, t_stop=t_stop)\n",
        "        rates = instantaneous_rate(st, sampling_period=sampling_period, kernel=kernel)\n",
        "        r = np.asarray(rates.magnitude).reshape(-1)  # Hz\n",
        "        \n",
        "        # 截断或填充到n_bins\n",
        "        if r.size >= n_bins:\n",
        "            fr[idx, :] = r[:n_bins]\n",
        "        else:\n",
        "            fr[idx, :r.size] = r\n",
        "    \n",
        "    return fr, edges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始构建发放率矩阵，共 3422 个试次...\n",
            "处理进度: 0/3422 (0.0%)\n",
            "处理进度: 100/3422 (2.9%)\n",
            "处理进度: 200/3422 (5.8%)\n",
            "处理进度: 300/3422 (8.8%)\n",
            "处理进度: 400/3422 (11.7%)\n",
            "处理进度: 500/3422 (14.6%)\n",
            "处理进度: 600/3422 (17.5%)\n",
            "处理进度: 700/3422 (20.5%)\n",
            "处理进度: 800/3422 (23.4%)\n",
            "处理进度: 900/3422 (26.3%)\n",
            "处理进度: 1000/3422 (29.2%)\n",
            "处理进度: 1100/3422 (32.1%)\n",
            "处理进度: 1200/3422 (35.1%)\n",
            "处理进度: 1300/3422 (38.0%)\n",
            "处理进度: 1400/3422 (40.9%)\n",
            "处理进度: 1500/3422 (43.8%)\n",
            "处理进度: 1600/3422 (46.8%)\n",
            "处理进度: 1700/3422 (49.7%)\n",
            "处理进度: 1800/3422 (52.6%)\n",
            "处理进度: 1900/3422 (55.5%)\n",
            "处理进度: 2000/3422 (58.4%)\n",
            "处理进度: 2100/3422 (61.4%)\n",
            "处理进度: 2200/3422 (64.3%)\n",
            "处理进度: 2300/3422 (67.2%)\n",
            "处理进度: 2400/3422 (70.1%)\n",
            "处理进度: 2500/3422 (73.1%)\n",
            "处理进度: 2600/3422 (76.0%)\n",
            "处理进度: 2700/3422 (78.9%)\n",
            "处理进度: 2800/3422 (81.8%)\n",
            "处理进度: 2900/3422 (84.7%)\n",
            "处理进度: 3000/3422 (87.7%)\n",
            "处理进度: 3100/3422 (90.6%)\n",
            "处理进度: 3200/3422 (93.5%)\n",
            "处理进度: 3300/3422 (96.4%)\n",
            "处理进度: 3400/3422 (99.4%)\n",
            "\n",
            "完成！成功构建了 3422 个发放率矩阵\n"
          ]
        }
      ],
      "source": [
        "def build_firing_rate_matrices_combined(\n",
        "    triggers_df: pd.DataFrame,\n",
        "    neuron_order: pd.DataFrame,\n",
        "    spikes_by_neuron: Dict[Tuple[str, int], np.ndarray],\n",
        "    sampling_period_sec: float,\n",
        "    kernel_sigma_sec: float\n",
        ") -> Dict[str, Dict[str, object]]:\n",
        "    \"\"\"构建所有trigger窗口的发放率矩阵\"\"\"\n",
        "    fr_dict: Dict[str, Dict[str, object]] = {}\n",
        "    \n",
        "    print(f\"开始构建发放率矩阵，共 {len(triggers_df)} 个试次...\")\n",
        "    \n",
        "    for idx, (_, row) in enumerate(triggers_df.iterrows()):\n",
        "        if idx % 100 == 0:\n",
        "            print(f\"处理进度: {idx}/{len(triggers_df)} ({idx/len(triggers_df)*100:.1f}%)\")\n",
        "        \n",
        "        start_sec = float(row['start_time'])\n",
        "        stop_sec = float(row['stop_time'])\n",
        "        key = make_trigger_key(row)\n",
        "        \n",
        "        try:\n",
        "            fr_mat, edges = compute_firing_rate_matrix_for_window_instant(\n",
        "                start_sec=start_sec,\n",
        "                stop_sec=stop_sec,\n",
        "                neuron_order=neuron_order,\n",
        "                spike_times_by_neuron=spikes_by_neuron,\n",
        "                sampling_period_sec=sampling_period_sec,\n",
        "                kernel_sigma_sec=kernel_sigma_sec,\n",
        "            )\n",
        "            \n",
        "            train_val = int(row['train_image']) if not pd.isna(row['train_image']) else 0\n",
        "            test_val = int(row['test_image']) if not pd.isna(row['test_image']) else 0\n",
        "            \n",
        "            if train_val != 0:\n",
        "                phase = 'train'\n",
        "                image_id = train_val\n",
        "            else:\n",
        "                phase = 'test'\n",
        "                image_id = test_val\n",
        "            \n",
        "            fr_dict[key] = {\n",
        "                'phase': phase,\n",
        "                'image_id': image_id,\n",
        "                'firing_rate': fr_mat[:, :-1],\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"警告: 处理试次 {key} 时出错: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n完成！成功构建了 {len(fr_dict)} 个发放率矩阵\")\n",
        "    return fr_dict\n",
        "\n",
        "SAMPLING_PERIOD_SEC = 0.01  \n",
        "KERNEL_SIGMA_SEC = 0.02  \n",
        "\n",
        "firing_rate_dict = build_firing_rate_matrices_combined(\n",
        "    triggers_df,\n",
        "    neuron_order,\n",
        "    spikes_by_neuron,\n",
        "    SAMPLING_PERIOD_SEC,\n",
        "    KERNEL_SIGMA_SEC\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle \n",
        "with open(\"firing_rate_summary_0112.pkl\", 'wb') as f:\n",
        "    pickle.dump(firing_rate_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "发放率矩阵已保存到: /media/ubuntu/sda/Monkey/sorted_result_combined/firing_rate_matrices/firing_rate_matrices_20240112_combined.npz\n",
            "汇总信息已保存到: /media/ubuntu/sda/Monkey/sorted_result_combined/firing_rate_matrices/firing_rate_summary_20240112_combined.csv\n"
          ]
        }
      ],
      "source": [
        "def save_firing_rate_dict(fr_dict: Dict[str, Dict[str, object]], output_dir: str, date_str: str):\n",
        "    \"\"\"保存发放率字典到文件\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # 保存为npz格式\n",
        "    npz_path = os.path.join(output_dir, f\"firing_rate_matrices_{date_str}_combined.npz\")\n",
        "    pack = {}\n",
        "    \n",
        "    for k, v in fr_dict.items():\n",
        "        pack[f'{k}__firing_rate'] = v['firing_rate']\n",
        "    \n",
        "    if fr_dict:\n",
        "        sample_data = list(fr_dict.values())[0]\n",
        "        neuron_order = sample_data['neuron_order']\n",
        "        pack['neuron_order_csv'] = neuron_order.to_csv(index=False).encode('utf-8')\n",
        "    \n",
        "    np.savez_compressed(npz_path, **pack)\n",
        "    print(f\"发放率矩阵已保存到: {npz_path}\")\n",
        "    \n",
        "    # 保存汇总信息\n",
        "    summary_path = os.path.join(output_dir, f\"firing_rate_summary_{date_str}_combined.csv\")\n",
        "    summary_data = []\n",
        "    \n",
        "    for key, data in fr_dict.items():\n",
        "        summary_data.append({\n",
        "            'key': key,\n",
        "            'phase': data['phase'],\n",
        "            'image_id': data['image_id'],\n",
        "            'image_rep_num': data['image_rep_num'],\n",
        "            'single_train_rep': data['single_train_rep'],\n",
        "            'start_time': data['start_time'],\n",
        "            'stop_time': data['stop_time'],\n",
        "            'duration': data['stop_time'] - data['start_time'],\n",
        "            'n_neurons': data['firing_rate'].shape[0],\n",
        "            'n_bins': data['firing_rate'].shape[1]\n",
        "        })\n",
        "    \n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    print(f\"汇总信息已保存到: {summary_path}\")\n",
        "    \n",
        "    return npz_path, summary_path\n",
        "\n",
        "# 保存结果\n",
        "output_dir = \"/media/ubuntu/sda/Monkey/sorted_result_combined/firing_rate_matrices\"\n",
        "npz_path, summary_path = save_firing_rate_dict(firing_rate_dict, output_dir, DATE_STR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化示例\n",
        "if firing_rate_dict:\n",
        "    # 选择一个示例进行可视化\n",
        "    sample_key = list(firing_rate_dict.keys())[0]\n",
        "    sample_data = firing_rate_dict[sample_key]\n",
        "    fr_matrix = sample_data['firing_rate']\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. 整体发放率矩阵热图\n",
        "    ax1 = axes[0, 0]\n",
        "    im1 = ax1.imshow(fr_matrix, aspect='auto', cmap='viridis', interpolation='nearest')\n",
        "    ax1.set_title(f'发放率矩阵热图\\n{sample_key}')\n",
        "    ax1.set_xlabel('时间bins')\n",
        "    ax1.set_ylabel('神经元索引')\n",
        "    plt.colorbar(im1, ax=ax1, label='发放率 (Hz)')\n",
        "    \n",
        "    # 2. 平均发放率随时间变化\n",
        "    ax2 = axes[0, 1]\n",
        "    mean_fr = np.mean(fr_matrix, axis=0)\n",
        "    time_bins = np.arange(len(mean_fr)) * sample_data['bin_width_sec']\n",
        "    ax2.plot(time_bins, mean_fr, 'b-', linewidth=2)\n",
        "    ax2.set_title('平均发放率随时间变化')\n",
        "    ax2.set_xlabel('时间 (秒)')\n",
        "    ax2.set_ylabel('平均发放率 (Hz)')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. 神经元平均发放率分布\n",
        "    ax3 = axes[1, 0]\n",
        "    neuron_mean_fr = np.mean(fr_matrix, axis=1)\n",
        "    ax3.hist(neuron_mean_fr, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax3.set_title('神经元平均发放率分布')\n",
        "    ax3.set_xlabel('平均发放率 (Hz)')\n",
        "    ax3.set_ylabel('神经元数量')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. 发放率统计\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.axis('off')\n",
        "    stats_text = f\"\"\"\n",
        "    统计信息:\n",
        "    神经元数: {fr_matrix.shape[0]}\n",
        "    时间bins: {fr_matrix.shape[1]}\n",
        "    时间窗口: {sample_data['stop_time'] - sample_data['start_time']:.3f} 秒\n",
        "    平均发放率: {np.mean(fr_matrix):.2f} Hz\n",
        "    最大发放率: {np.max(fr_matrix):.2f} Hz\n",
        "    最小发放率: {np.min(fr_matrix):.2f} Hz\n",
        "    Phase: {sample_data['phase']}\n",
        "    Image ID: {sample_data['image_id']}\n",
        "    \"\"\"\n",
        "    ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=12,\n",
        "             verticalalignment='top', fontfamily='monospace')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n可视化完成！示例数据来自试次: {sample_key}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spike_sorting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

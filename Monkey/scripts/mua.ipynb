{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9eb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_test_MUA_by_trails(test_MUA_oracle, image_trail_to_include):\n",
    "    filtered_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for image_idx, trail_indices in image_trail_to_include.items():\n",
    "        image_data = test_MUA_oracle[:, image_idx, :]  \n",
    "        \n",
    "        for trail_idx in trail_indices:\n",
    "            if trail_idx < image_data.shape[0]:\n",
    "                filtered_data.append(image_data[trail_idx, :])  \n",
    "                labels.append(image_idx)  \n",
    "                \n",
    "    filtered_test_MUA = np.array(filtered_data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"筛选后的数据形状: {filtered_test_MUA.shape}\")\n",
    "    print(f\"标签数量: {len(labels)}\")\n",
    "    print(f\"包含的图像数量: {len(image_trail_to_include)}\")\n",
    "    \n",
    "    return filtered_test_MUA, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f4c8e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNR', 'SNR_max', 'lats', 'oracle', 'reliab', 'tb', 'test_MUA', 'test_MUA_reps', 'train_MUA']\n",
      "筛选后的数据形状: (2549, 503)\n",
      "标签数量: 2549\n",
      "包含的图像数量: 91\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/media/ubuntu/sda/Monkey/data/THINGS_normMUA_MonkeyF.mat\"\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    print(list(file.keys()))\n",
    "    oracle = file['oracle'][:]\n",
    "    test_MUA = file['test_MUA_reps'][:]\n",
    "    train_MUA = file['train_MUA'][:]\n",
    "    reliab = file['reliab'][:]\n",
    "    SNR = file['SNR_max'][:]\n",
    "    lats = file['lats'][:]\n",
    "\n",
    "lats = lats.mean(axis=0)\n",
    "channel_inf = pd.DataFrame(oracle, columns=['oracle'])\n",
    "channel_inf['region'] = 'V1'\n",
    "channel_inf.loc[512:832, 'region'] = 'V4'\n",
    "channel_inf.loc[832:, 'region'] = 'IT'\n",
    "\n",
    "channel_inf['reliab'] = True\n",
    "channel_inf.loc[channel_inf['oracle'] < 0.6, 'reliab'] = False\n",
    "channel_inf['SNR'] = SNR\n",
    "channel_inf['lats'] = lats\n",
    "\n",
    "test_MUA_oracle = test_MUA[:, :, channel_inf[channel_inf['reliab'] == True].index]\n",
    "train_MUA_oracle = train_MUA[:, channel_inf[channel_inf['reliab'] == True].index]\n",
    "\n",
    "similarity_matrices = np.zeros((100, 30, 30))\n",
    "similarity_list = []\n",
    "for image_idx in range(100):\n",
    "    image_data = test_MUA_oracle[:, image_idx, :]\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(image_data)\n",
    "    similarity_matrices[image_idx] = similarity_matrix\n",
    "    similarity_list.append((np.sum(similarity_matrix) - 30) / (30 * 29))\n",
    "\n",
    "similarity_list = np.array(similarity_list)\n",
    "\n",
    "image_to_include = np.where(similarity_list >= 0.5)[0]\n",
    "image_trail_to_include = {}\n",
    "for image in image_to_include:\n",
    "    temp = similarity_matrices[image].mean(axis=0)\n",
    "    image_trail_to_include[image] = np.where(temp > 0.5)[0]\n",
    "\n",
    "filtered_test_MUA, filtered_labels = filter_test_MUA_by_trails(test_MUA_oracle, image_trail_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41754cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = channel_inf['oracle'].min()\n",
    "vmax = channel_inf['oracle'].max()\n",
    "\n",
    "groups = channel_inf.groupby('region')\n",
    "\n",
    "with PdfPages('/media/ubuntu/sda/Monkey/figure/region_heatmaps_oracle_monkeyF.pdf') as pdf:\n",
    "    for region_name, group_data in groups:\n",
    "        n_blocks = len(group_data) // 64\n",
    "        \n",
    "        n_cols = min(4, n_blocks) \n",
    "        n_rows = (n_blocks + n_cols - 1) // n_cols \n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2.5*n_rows + 0.5))\n",
    "        fig.suptitle(f'Heatmaps for {region_name}', fontsize=16)\n",
    "        \n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for i in range(n_blocks):\n",
    "            block_data = group_data.iloc[i*64:(i+1)*64]['oracle'].values\n",
    "            heatmap_data = block_data.reshape(8, 8)\n",
    "            \n",
    "            im = axes_flat[i].imshow(heatmap_data, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "            axes_flat[i].set_xticks([])\n",
    "            axes_flat[i].set_yticks([])\n",
    "        \n",
    "        for i in range(n_blocks, len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  \n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('Oracle Value', fontsize=10)\n",
    "        \n",
    "        plt.subplots_adjust(right=0.9, top=0.9 if n_rows > 1 else 0.85)\n",
    "        \n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "vmin = channel_inf['SNR'].min()\n",
    "vmax = channel_inf['SNR'].max()\n",
    "with PdfPages('/media/ubuntu/sda/Monkey/figure/region_heatmaps_SNR_monkeyF.pdf') as pdf:\n",
    "    for region_name, group_data in groups:\n",
    "        n_blocks = len(group_data) // 64\n",
    "        \n",
    "        n_cols = min(4, n_blocks) \n",
    "        n_rows = (n_blocks + n_cols - 1) // n_cols \n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2.5*n_rows + 0.5))\n",
    "        fig.suptitle(f'Heatmaps for {region_name}', fontsize=16)\n",
    "        \n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for i in range(n_blocks):\n",
    "            block_data = group_data.iloc[i*64:(i+1)*64]['SNR'].values\n",
    "            heatmap_data = block_data.reshape(8, 8)\n",
    "            \n",
    "            im = axes_flat[i].imshow(heatmap_data, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "            axes_flat[i].set_xticks([])\n",
    "            axes_flat[i].set_yticks([])\n",
    "        \n",
    "        for i in range(n_blocks, len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  \n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('Oracle Value', fontsize=10)\n",
    "        \n",
    "        plt.subplots_adjust(right=0.9, top=0.9 if n_rows > 1 else 0.85)\n",
    "        \n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "vmin = channel_inf['lats'].min()\n",
    "vmax = channel_inf['lats'].max()\n",
    "with PdfPages('/media/ubuntu/sda/Monkey/figure/region_heatmaps_lats_monkeyF.pdf') as pdf:\n",
    "    for region_name, group_data in groups:\n",
    "        n_blocks = len(group_data) // 64\n",
    "        \n",
    "        n_cols = min(4, n_blocks) \n",
    "        n_rows = (n_blocks + n_cols - 1) // n_cols \n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2.5*n_rows + 0.5))\n",
    "        fig.suptitle(f'Heatmaps for {region_name}', fontsize=16)\n",
    "        \n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for i in range(n_blocks):\n",
    "            block_data = group_data.iloc[i*64:(i+1)*64]['lats'].values\n",
    "            heatmap_data = block_data.reshape(8, 8)\n",
    "            \n",
    "            im = axes_flat[i].imshow(heatmap_data, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "            axes_flat[i].set_xticks([])\n",
    "            axes_flat[i].set_yticks([])\n",
    "        \n",
    "        for i in range(n_blocks, len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  \n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('Oracle Value', fontsize=10)\n",
    "        \n",
    "        plt.subplots_adjust(right=0.9, top=0.9 if n_rows > 1 else 0.85)\n",
    "        \n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c9a0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/media/ubuntu/sda/Monkey/data/filtered_test_MUA_MonkeyF.npy', filtered_test_MUA)\n",
    "np.save('/media/ubuntu/sda/Monkey/data/filtered_labels_MonkeyF.npy', filtered_labels)\n",
    "np.save('/media/ubuntu/sda/Monkey/data/train_MUA_MonkeyF.npy', train_MUA_oracle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b49b3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNR', 'SNR_max', 'lats', 'oracle', 'reliab', 'tb', 'test_MUA', 'test_MUA_reps', 'train_MUA']\n",
      "筛选后的数据形状: (2834, 669)\n",
      "标签数量: 2834\n",
      "包含的图像数量: 98\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/media/ubuntu/sda/Monkey/data/THINGS_normMUA_MonkeyN.mat\"\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    print(list(file.keys()))\n",
    "    oracle = file['oracle'][:]\n",
    "    test_MUA = file['test_MUA_reps'][:]\n",
    "    train_MUA = file['train_MUA'][:]\n",
    "    reliab = file['reliab'][:]\n",
    "    SNR = file['SNR_max'][:]\n",
    "    lats = file['lats'][:]\n",
    "\n",
    "lats = lats.mean(axis=0)\n",
    "channel_inf = pd.DataFrame(oracle, columns=['oracle'])\n",
    "channel_inf['region'] = 'V1'\n",
    "channel_inf.loc[512:768, 'region'] = 'V4'\n",
    "channel_inf.loc[768:, 'region'] = 'IT'\n",
    "\n",
    "channel_inf['reliab'] = True\n",
    "channel_inf.loc[channel_inf['oracle'] < 0.6, 'reliab'] = False\n",
    "channel_inf['SNR'] = SNR\n",
    "channel_inf['lats'] = lats\n",
    "\n",
    "test_MUA_oracle = test_MUA[:, :, channel_inf[channel_inf['reliab'] == True].index]\n",
    "train_MUA_oracle = train_MUA[:, channel_inf[channel_inf['reliab'] == True].index]\n",
    "\n",
    "\n",
    "similarity_matrices = np.zeros((100, 30, 30))\n",
    "similarity_list = []\n",
    "for image_idx in range(100):\n",
    "    image_data = test_MUA_oracle[:, image_idx, :]\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(image_data)\n",
    "    similarity_matrices[image_idx] = similarity_matrix\n",
    "    similarity_list.append((np.sum(similarity_matrix) - 30) / (30 * 29))\n",
    "\n",
    "similarity_list = np.array(similarity_list)\n",
    "\n",
    "image_to_include = np.where(similarity_list >= 0.5)[0]\n",
    "image_trail_to_include = {}\n",
    "for image in image_to_include:\n",
    "    temp = similarity_matrices[image].mean(axis=0)\n",
    "    image_trail_to_include[image] = np.where(temp > 0.5)[0]\n",
    "\n",
    "filtered_test_MUA, filtered_labels = filter_test_MUA_by_trails(test_MUA_oracle, image_trail_to_include)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffa728ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/503 neurons...\n",
      "Processed 100/503 neurons...\n",
      "Processed 150/503 neurons...\n",
      "Processed 200/503 neurons...\n",
      "Processed 250/503 neurons...\n",
      "Processed 300/503 neurons...\n",
      "Processed 350/503 neurons...\n",
      "Processed 400/503 neurons...\n",
      "Processed 450/503 neurons...\n",
      "Processed 500/503 neurons...\n",
      "PDF created with 503 scatter plots\n"
     ]
    }
   ],
   "source": [
    "n_trials, n_images, n_neurons = test_MUA_oracle.shape\n",
    "\n",
    "max_pref_images = [] \n",
    "min_pref_images = []  \n",
    "\n",
    "with PdfPages('/media/ubuntu/sda/Monkey/figure/neuron_scatter_plots_monkeyN.pdf') as pdf:\n",
    "    for neuron_idx in range(n_neurons):\n",
    "        neuron_data = test_MUA_oracle[:, :, neuron_idx]  \n",
    "        image_means = np.mean(neuron_data, axis=0)  \n",
    "        \n",
    "        sorted_indices = np.argsort(image_means)\n",
    "        \n",
    "        min_pref_image = sorted_indices[0]  \n",
    "        max_pref_image = sorted_indices[-1] \n",
    "        \n",
    "        max_pref_images.append(max_pref_image)\n",
    "        min_pref_images.append(min_pref_image)\n",
    "        \n",
    "        sorted_means = image_means[sorted_indices]\n",
    "        \n",
    "        all_values = []\n",
    "        all_ranks = []\n",
    "        \n",
    "        for img_idx in range(n_images):\n",
    "            img_values = neuron_data[:, img_idx]\n",
    "            img_rank = np.where(sorted_indices == img_idx)[0][0]\n",
    "            all_values.extend(img_values)\n",
    "            all_ranks.extend([img_rank] * n_trials)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        scatter = ax.scatter(all_ranks, all_values, alpha=0.6, s=25, color='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.legend('')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)  \n",
    "        \n",
    "        if (neuron_idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {neuron_idx + 1}/{n_neurons} neurons...\")\n",
    "\n",
    "print(f\"PDF created with {n_neurons} scatter plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdce24d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_pref_images[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3edbcd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(20)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pref_images[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5241a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "\n",
    "vmin = channel_inf['oracle'].min()\n",
    "vmax = channel_inf['oracle'].max()\n",
    "\n",
    "groups = channel_inf.groupby('region')\n",
    "\n",
    "with PdfPages('/media/ubuntu/sda/Monkey/figure/region_heatmaps_oracle_monkeyN.pdf') as pdf:\n",
    "    for region_name, group_data in groups:\n",
    "        n_blocks = len(group_data) // 64\n",
    "        \n",
    "        n_cols = min(4, n_blocks) \n",
    "        n_rows = (n_blocks + n_cols - 1) // n_cols \n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2.5*n_rows + 0.5))\n",
    "        fig.suptitle(f'Heatmaps for {region_name}', fontsize=16)\n",
    "        \n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for i in range(n_blocks):\n",
    "            block_data = group_data.iloc[i*64:(i+1)*64]['oracle'].values\n",
    "            heatmap_data = block_data.reshape(8, 8)\n",
    "            \n",
    "            im = axes_flat[i].imshow(heatmap_data, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "            axes_flat[i].set_xticks([])\n",
    "            axes_flat[i].set_yticks([])\n",
    "        \n",
    "        for i in range(n_blocks, len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  \n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('Oracle Value', fontsize=10)\n",
    "        \n",
    "        plt.subplots_adjust(right=0.9, top=0.9 if n_rows > 1 else 0.85)\n",
    "        \n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "vmin = channel_inf['SNR'].min()\n",
    "vmax = channel_inf['SNR'].max()\n",
    "with PdfPages('/media/ubuntu/sda/Monkey/figure/region_heatmaps_SNR_monkeyN.pdf') as pdf:\n",
    "    for region_name, group_data in groups:\n",
    "        n_blocks = len(group_data) // 64\n",
    "        \n",
    "        n_cols = min(4, n_blocks) \n",
    "        n_rows = (n_blocks + n_cols - 1) // n_cols \n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2.5*n_rows + 0.5))\n",
    "        fig.suptitle(f'Heatmaps for {region_name}', fontsize=16)\n",
    "        \n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for i in range(n_blocks):\n",
    "            block_data = group_data.iloc[i*64:(i+1)*64]['SNR'].values\n",
    "            heatmap_data = block_data.reshape(8, 8)\n",
    "            \n",
    "            im = axes_flat[i].imshow(heatmap_data, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "            axes_flat[i].set_xticks([])\n",
    "            axes_flat[i].set_yticks([])\n",
    "        \n",
    "        for i in range(n_blocks, len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  \n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('Oracle Value', fontsize=10)\n",
    "        \n",
    "        plt.subplots_adjust(right=0.9, top=0.9 if n_rows > 1 else 0.85)\n",
    "        \n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "vmin = channel_inf['lats'].min()\n",
    "vmax = channel_inf['lats'].max()\n",
    "with PdfPages('/media/ubuntu/sda/Monkey/figure/region_heatmaps_lats_monkeyN.pdf') as pdf:\n",
    "    for region_name, group_data in groups:\n",
    "        n_blocks = len(group_data) // 64\n",
    "        \n",
    "        n_cols = min(4, n_blocks) \n",
    "        n_rows = (n_blocks + n_cols - 1) // n_cols \n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2.5*n_rows + 0.5))\n",
    "        fig.suptitle(f'Heatmaps for {region_name}', fontsize=16)\n",
    "        \n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = np.array([axes])\n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for i in range(n_blocks):\n",
    "            block_data = group_data.iloc[i*64:(i+1)*64]['lats'].values\n",
    "            heatmap_data = block_data.reshape(8, 8)\n",
    "            \n",
    "            im = axes_flat[i].imshow(heatmap_data, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "            axes_flat[i].set_xticks([])\n",
    "            axes_flat[i].set_yticks([])\n",
    "        \n",
    "        for i in range(n_blocks, len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  \n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('Oracle Value', fontsize=10)\n",
    "        \n",
    "        plt.subplots_adjust(right=0.9, top=0.9 if n_rows > 1 else 0.85)\n",
    "        \n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea17a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/media/ubuntu/sda/Monkey/data/filtered_test_MUA_MonkeyN.npy', filtered_test_MUA)\n",
    "np.save('/media/ubuntu/sda/Monkey/data/filtered_labels_MonkeyN.npy', filtered_labels)\n",
    "np.save('/media/ubuntu/sda/Monkey/data/train_MUA_MonkeyN.npy', train_MUA_oracle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c5ac482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 创建分类数据集 ===\n",
      "原始标签数量: 98\n",
      "映射后标签数量: 98\n",
      "标签映射范围: 0 - 97\n",
      "训练集大小: 2267\n",
      "测试集大小: 567\n",
      "特征维度: 669\n",
      "类别数量: 98\n"
     ]
    }
   ],
   "source": [
    "# 构建分类网络的Dataset类\n",
    "from random import shuffle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "class MUAClassificationDataset(Dataset):\n",
    "    def __init__(self, mua_data, labels, transform=None):\n",
    "        \"\"\"\n",
    "        MUA分类数据集\n",
    "        \n",
    "        Args:\n",
    "            mua_data: MUA数据，形状为 (n_samples, n_channels)\n",
    "            labels: 标签列表\n",
    "            transform: 数据变换（可选）\n",
    "        \"\"\"\n",
    "        self.mua_data = torch.tensor(mua_data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mua_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.mua_data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample, label\n",
    "\n",
    "def create_label_mapping(original_labels, num_classes=91):\n",
    "    \"\"\"\n",
    "    创建标签重新映射\n",
    "    \n",
    "    Args:\n",
    "        original_labels: 原始标签数组\n",
    "        num_classes: 目标类别数量\n",
    "    \n",
    "    Returns:\n",
    "        mapped_labels: 重新映射后的标签\n",
    "        label_mapping: 标签映射字典\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(original_labels)\n",
    "    print(f\"原始标签数量: {len(unique_labels)}\")\n",
    "    \n",
    "    # 创建映射字典\n",
    "    label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
    "    \n",
    "    # 重新映射标签\n",
    "    mapped_labels = np.array([label_mapping[label] for label in original_labels])\n",
    "    \n",
    "    print(f\"映射后标签数量: {len(np.unique(mapped_labels))}\")\n",
    "    print(f\"标签映射范围: {mapped_labels.min()} - {mapped_labels.max()}\")\n",
    "    \n",
    "    return mapped_labels, label_mapping\n",
    "\n",
    "# 创建数据集\n",
    "print(\"=== 创建分类数据集 ===\")\n",
    "mapped_labels, label_mapping = create_label_mapping(filtered_labels, num_classes=91)\n",
    "\n",
    "# 分割训练和测试集\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(filtered_test_MUA)), \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=mapped_labels\n",
    ")\n",
    "\n",
    "train_dataset = MUAClassificationDataset(\n",
    "    filtered_test_MUA[train_indices], \n",
    "    mapped_labels[train_indices]\n",
    ")\n",
    "\n",
    "test_dataset = MUAClassificationDataset(\n",
    "    filtered_test_MUA[test_indices], \n",
    "    mapped_labels[test_indices]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"测试集大小: {len(test_dataset)}\")\n",
    "print(f\"特征维度: {filtered_test_MUA.shape[1]}\")\n",
    "print(f\"类别数量: {len(np.unique(mapped_labels))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a837cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建4层ResNet MLP模型\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"ResNet风格的残差块\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, output_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(output_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.linear2 = nn.Linear(output_dim, output_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(output_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # 残差连接\n",
    "        self.shortcut = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        \n",
    "        out = F.relu(self.bn1(self.linear1(x)))\n",
    "        out = self.dropout1(out)\n",
    "        out = self.bn2(self.linear2(out))\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNetMLPClassifier(nn.Module):\n",
    "    \"\"\"4层ResNet MLP分类器\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 1024, 1024, 1024], num_classes=91, dropout_rate=0.1):\n",
    "        super(ResNetMLPClassifier, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # 输入投影层\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.input_bn = nn.BatchNorm1d(hidden_dims[0])\n",
    "        \n",
    "        # 4个ResNet块\n",
    "        self.resnet_blocks = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.resnet_blocks.append(\n",
    "                ResidualBlock(hidden_dims[i], hidden_dims[i+1], dropout_rate)\n",
    "            )\n",
    "        \n",
    "        # 最终特征层（1024维）\n",
    "        self.feature_layer = nn.Linear(hidden_dims[-1], 1024)\n",
    "        self.feature_bn = nn.BatchNorm1d(1024)\n",
    "        self.feature_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # 分类器层\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"初始化网络权重\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 输入投影\n",
    "        x = F.relu(self.input_bn(self.input_projection(x)))\n",
    "        \n",
    "        # 通过ResNet块\n",
    "        for block in self.resnet_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # 特征提取（1024维）\n",
    "        features = F.relu(self.feature_bn(self.feature_layer(x)))\n",
    "        features = self.feature_dropout(features)\n",
    "        \n",
    "        # 分类\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits, features\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"只返回特征，不进行分类\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = F.relu(self.input_bn(self.input_projection(x)))\n",
    "            \n",
    "            for block in self.resnet_blocks:\n",
    "                x = block(x)\n",
    "            \n",
    "            features = F.relu(self.feature_bn(self.feature_layer(x)))\n",
    "            return features\n",
    "\n",
    "# 创建模型\n",
    "input_dim = filtered_test_MUA.shape[1]  # 特征维度\n",
    "num_classes = len(np.unique(mapped_labels))  # 实际类别数量\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9c82638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        logits, features = model(data)\n",
    "        loss = criterion(logits, target)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_features = []\n",
    "    \n",
    "    \n",
    "    for idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        logits, features = model(data)\n",
    "        loss = criterion(logits, target)\n",
    "        \n",
    "        # 统计\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # 收集预测结果和特征\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "        all_features.extend(features.cpu().numpy())\n",
    "        \n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, all_predictions, all_targets, all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "187447d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26cf0ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "--------------------------------------------------\n",
      "save best model acc: 96.12%\n",
      "Train loss: 1.8695, Train acc: 69.87%\n",
      "Test loss: 0.1454, Test acc: 96.12%\n",
      "\n",
      "Epoch 2/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0846, Train acc: 97.75%\n",
      "Test loss: 0.2042, Test acc: 96.12%\n",
      "\n",
      "Epoch 3/20\n",
      "--------------------------------------------------\n",
      "save best model acc: 98.06%\n",
      "Train loss: 0.0956, Train acc: 97.88%\n",
      "Test loss: 0.0904, Test acc: 98.06%\n",
      "\n",
      "Epoch 4/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0502, Train acc: 99.03%\n",
      "Test loss: 0.2623, Test acc: 96.47%\n",
      "\n",
      "Epoch 5/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0287, Train acc: 99.21%\n",
      "Test loss: 0.1457, Test acc: 96.65%\n",
      "\n",
      "Epoch 6/20\n",
      "--------------------------------------------------\n",
      "save best model acc: 98.59%\n",
      "Train loss: 0.0557, Train acc: 98.99%\n",
      "Test loss: 0.0863, Test acc: 98.59%\n",
      "\n",
      "Epoch 7/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0511, Train acc: 98.90%\n",
      "Test loss: 0.1114, Test acc: 97.35%\n",
      "\n",
      "Epoch 8/20\n",
      "--------------------------------------------------\n",
      "save best model acc: 98.77%\n",
      "Train loss: 0.0282, Train acc: 99.43%\n",
      "Test loss: 0.0292, Test acc: 98.77%\n",
      "\n",
      "Epoch 9/20\n",
      "--------------------------------------------------\n",
      "save best model acc: 99.29%\n",
      "Train loss: 0.0374, Train acc: 99.25%\n",
      "Test loss: 0.0434, Test acc: 99.29%\n",
      "\n",
      "Epoch 10/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0200, Train acc: 99.51%\n",
      "Test loss: 0.0805, Test acc: 98.24%\n",
      "\n",
      "Epoch 11/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0485, Train acc: 98.99%\n",
      "Test loss: 0.1087, Test acc: 98.59%\n",
      "\n",
      "Epoch 12/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0337, Train acc: 99.34%\n",
      "Test loss: 0.0206, Test acc: 99.29%\n",
      "\n",
      "Epoch 13/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0353, Train acc: 99.03%\n",
      "Test loss: 0.1230, Test acc: 97.53%\n",
      "\n",
      "Epoch 14/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0478, Train acc: 98.76%\n",
      "Test loss: 0.0630, Test acc: 98.94%\n",
      "\n",
      "Epoch 15/20\n",
      "--------------------------------------------------\n",
      "save best model acc: 99.47%\n",
      "Train loss: 0.0392, Train acc: 99.16%\n",
      "Test loss: 0.0290, Test acc: 99.47%\n",
      "\n",
      "Epoch 16/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0107, Train acc: 99.60%\n",
      "Test loss: 0.1275, Test acc: 98.24%\n",
      "\n",
      "Epoch 17/20\n",
      "--------------------------------------------------\n",
      "save best model acc: 99.65%\n",
      "Train loss: 0.0127, Train acc: 99.74%\n",
      "Test loss: 0.0065, Test acc: 99.65%\n",
      "\n",
      "Epoch 18/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0135, Train acc: 99.74%\n",
      "Test loss: 0.1349, Test acc: 98.41%\n",
      "\n",
      "Epoch 19/20\n",
      "--------------------------------------------------\n",
      "save best model acc: 99.82%\n",
      "Train loss: 0.0202, Train acc: 99.60%\n",
      "Test loss: 0.0060, Test acc: 99.82%\n",
      "\n",
      "Epoch 20/20\n",
      "--------------------------------------------------\n",
      "Train loss: 0.0178, Train acc: 99.43%\n",
      "Test loss: 0.0951, Test acc: 98.59%\n"
     ]
    }
   ],
   "source": [
    "model = ResNetMLPClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=[512, 1024, 1024, 1024],\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=0.1\n",
    ").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle = True, batch_size= 128)\n",
    "val_loader = DataLoader(test_dataset, shuffle = False, batch_size= 128)\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    val_loss, val_acc, _, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'mua_test_classifier.pth')\n",
    "        print(f\"save best model acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.2f}%\")\n",
    "    print(f\"Test loss: {val_loss:.4f}, Test acc: {val_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

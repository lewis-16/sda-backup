{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from probeinterface import write_prb, read_prb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_local_maxima_in_window(data, window_size=20, std_multiplier=2):\n",
    "\n",
    "    \"\"\"\n",
    "    在每个滑动窗口范围内检测局部最大值的索引，并确保最大值大于两倍的标准差。\n",
    "\n",
    "    参数:\n",
    "    data : numpy.ndarray\n",
    "        输入数据，形状为 (n_rows, n_columns)。\n",
    "    window_size : int\n",
    "        滑动窗口的大小，用于定义局部范围，默认为 20。\n",
    "    std_multiplier : float\n",
    "        标准差的倍数，用于筛选局部最大值，默认为 2。\n",
    "\n",
    "    返回:\n",
    "    local_maxima_indices : list of numpy.ndarray\n",
    "        每行局部最大值的索引列表，每个元素是对应行局部最大值的索引数组。\n",
    "    \"\"\"\n",
    "    local_maxima_indices = []\n",
    "\n",
    "    for row in data:\n",
    "        maxima_indices = []\n",
    "        row_std = np.std(row)\n",
    "        threshold = std_multiplier * row_std\n",
    "\n",
    "        for start in range(0, len(row), window_size):\n",
    "            end = min(start + window_size, len(row))\n",
    "            window = row[start:end]\n",
    "            \n",
    "            if len(window) > 0:\n",
    "                local_max_index = np.argmax(window)\n",
    "                local_max_value = window[local_max_index]\n",
    "                \n",
    "                if local_max_value > threshold:\n",
    "                    maxima_indices.append(start + local_max_index)  \n",
    "        \n",
    "        local_maxima_indices.extend(maxima_indices)\n",
    "        local_maxima_indices = list(set(local_maxima_indices))  \n",
    "\n",
    "    return local_maxima_indices\n",
    "\n",
    "def cluster_label_array1_based_on_array2(array1, array2, threshold=5, \n",
    "                                         cluster_column='cluster'):\n",
    "\n",
    "    \"\"\"\n",
    "    根据 array2 的 'time' 和 'cluster' 对 array1 进行标记。\n",
    "    如果 array1 中的某个值在 threshold 范围内存在于 array2 的 'time' 中，则标记为对应的 'cluster' 值，否则为 0。\n",
    "    \n",
    "    参数:\n",
    "    array1 : numpy.ndarray\n",
    "        要标记的数组。\n",
    "    array2 : numpy.ndarray\n",
    "        包含 'time' 和 'cluster' 的二维数组。\n",
    "        第一列为 'time'，第二列为 'cluster'。\n",
    "    threshold : int\n",
    "        判断范围的阈值。\n",
    "    \n",
    "    返回:\n",
    "    labels : numpy.ndarray\n",
    "        长度为 len(array1) 的标签数组，值为 array2 中的 'cluster' 或 0。\n",
    "    \"\"\"\n",
    "\n",
    "    array2 = np.array((array2['time'], array2[cluster_column])).T\n",
    "    sorted_indices = np.argsort(array2[:, 0])\n",
    "    sorted_array2 = array2[sorted_indices]\n",
    "    \n",
    "    labels = -np.ones(len(array1), dtype=int)\n",
    "    \n",
    "    # 遍历 array1 中的每个元素\n",
    "    for i, value in enumerate(array1):\n",
    "        # 计算当前值的范围\n",
    "        left = value - threshold\n",
    "        right = value + threshold\n",
    "        \n",
    "        left_index = np.searchsorted(sorted_array2[:, 0], left, side='left')\n",
    "        right_index = np.searchsorted(sorted_array2[:, 0], right, side='right')\n",
    "        \n",
    "        # 如果范围内存在值，则标记为对应的 'cluster'\n",
    "        if right_index > left_index:\n",
    "            # 获取范围内的第一个匹配值的 'cluster'\n",
    "            labels[i] = sorted_array2[left_index, 1]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def extract_windows(data, indices, window_size=61):\n",
    "    \"\"\"\n",
    "    根据给定的时间点索引提取窗口。\n",
    "    \n",
    "    参数:\n",
    "    data : numpy.ndarray\n",
    "        输入数据，形状为 (n_channels, time)\n",
    "    indices : numpy.ndarray\n",
    "        时间点索引数组，用于指定需要提取窗口的中心点\n",
    "    window_size : int\n",
    "        窗口长度，默认为61（对应time-30到time+31）\n",
    "    \n",
    "    返回:\n",
    "    windows : numpy.ndarray\n",
    "        提取的窗口数据，形状为 (len(indices), n_channels, window_size)\n",
    "    \"\"\"\n",
    "    n_channels, time_length = data.shape\n",
    "    half_window = window_size // 2\n",
    "\n",
    "    if np.any(indices < half_window) or np.any(indices >= time_length - half_window):\n",
    "        raise ValueError(\"Some indices are out of bounds for the given window size.\")\n",
    "\n",
    "    windows = []\n",
    "    for idx in indices:\n",
    "        window = data[:, idx - half_window:idx + half_window + 1]\n",
    "        windows.append(window)\n",
    "\n",
    "    windows = np.array(windows)\n",
    "    return windows\n",
    "\n",
    "def compute_cluster_average(sample_data, labels):\n",
    "    \"\"\"\n",
    "    计算 potent_spike_inf 中每个 cluster_predicted 对应的 sample_data 的平均值。\n",
    "    \n",
    "    参数:\n",
    "    - sample_data: np.ndarray, 输入的 (n, 30, 61) 矩阵。\n",
    "    - potent_spike_inf: pd.DataFrame, 包含 cluster_predicted 信息的 DataFrame。\n",
    "    - cluster_column: str, cluster 信息所在的列名。\n",
    "    \n",
    "    返回:\n",
    "    - cluster_averages: dict, 每个 cluster 对应的平均值矩阵 (30, 61)。\n",
    "    \"\"\"\n",
    "    cluster_averages = {}\n",
    "    unique_clusters = np.unique(labels)\n",
    "    \n",
    "    for cluster in unique_clusters:\n",
    "        cluster_indices = np.where(labels == cluster)[0]\n",
    "        cluster_average = sample_data[cluster_indices].mean(axis=0) \n",
    "        cluster_averages[cluster] = cluster_average\n",
    "    \n",
    "    return cluster_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cluster_averages(cluster_averages, channel_position, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    处理cluster平均波形，基于最大峰值通道选择最近的n_neighbors个通道\n",
    "    \n",
    "    参数:\n",
    "    - cluster_averages: dict, 每个cluster的平均波形 (channels, time)\n",
    "    - channel_position: 通道位置字典 {channel_id: (x, y)}\n",
    "    - n_neighbors: 邻居通道数量\n",
    "    \n",
    "    返回:\n",
    "    - processed_data: dict, 处理后的数据，键为cluster_id，值为字典包含:\n",
    "        'waveform': 选中的通道波形 (n_neighbors+1, time)\n",
    "        'channel_ids': 选中的通道ID列表\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "    \n",
    "    for cluster, avg_matrix in cluster_averages.items():\n",
    "        # 1. 计算每个通道的峰值（绝对值最大值）\n",
    "        channel_peaks = np.max(np.abs(avg_matrix), axis=1)\n",
    "        \n",
    "        # 2. 找到最大峰值通道\n",
    "        max_peak_idx = np.argmax(channel_peaks)\n",
    "        \n",
    "        # 3. 获取所有通道位置\n",
    "        all_channels = list(range(avg_matrix.shape[0]))\n",
    "        positions = np.array([channel_position.get(ch, (np.nan, np.nan)) for ch in all_channels])\n",
    "        \n",
    "        # 4. 过滤无效位置\n",
    "        valid_mask = ~np.isnan(positions[:, 0])\n",
    "        valid_positions = positions[valid_mask]\n",
    "        valid_indices = np.array(all_channels)[valid_mask]\n",
    "        \n",
    "        if len(valid_positions) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 5. 计算到最大峰值通道的距离\n",
    "        max_pos = valid_positions[valid_indices == max_peak_idx][0].reshape(1, -1)\n",
    "        distances = cdist(valid_positions, max_pos).flatten()\n",
    "        \n",
    "        # 6. 选择距离最近的n_neighbors+1个通道（包括最大峰值通道自身）\n",
    "        closest_indices = np.argsort(distances)[:n_neighbors+1]\n",
    "        selected_indices = valid_indices[closest_indices]\n",
    "        \n",
    "        # 7. 存储处理后的数据\n",
    "        processed_data[cluster] = {\n",
    "            'waveform': avg_matrix[selected_indices, :],\n",
    "            'channel_ids': selected_indices.tolist(),\n",
    "        }\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def calculate_position(row, channel_position):\n",
    "    \"\"\"\n",
    "    计算cluster的位置（质心），使用预选的通道\n",
    "    \n",
    "    参数:\n",
    "    - row: DataFrame行，包含'waveform'和'channel_ids'\n",
    "    - channel_position: 通道位置字典 {channel_id: (x, y)}\n",
    "    \n",
    "    返回:\n",
    "    - pd.Series: 包含position_1和position_2\n",
    "    \"\"\"\n",
    "    waveform = row['waveform']\n",
    "    channel_ids = row['channel_ids']\n",
    "    \n",
    "    # 计算每个通道的波形幅值（平方和）\n",
    "    channel_amplitudes = np.sum(waveform**2, axis=1)\n",
    "    \n",
    "    sum_x_a = 0\n",
    "    sum_y_a = 0\n",
    "    sum_a = 0\n",
    "    \n",
    "    for i, ch_id in enumerate(channel_ids):\n",
    "        # 获取通道位置\n",
    "        x_i, y_i = channel_position.get(ch_id, (0, 0))\n",
    "        \n",
    "        # 使用幅值平方作为权重\n",
    "        a_i_sq = channel_amplitudes[i]\n",
    "        \n",
    "        sum_x_a += x_i * a_i_sq\n",
    "        sum_y_a += y_i * a_i_sq\n",
    "        sum_a += a_i_sq\n",
    "    \n",
    "    if sum_a == 0:\n",
    "        return pd.Series({'position_1': 0, 'position_2': 0})\n",
    "    \n",
    "    x_hat = sum_x_a / sum_a\n",
    "    y_hat = sum_y_a / sum_a\n",
    "    return pd.Series({'position_1': x_hat, 'position_2': y_hat})\n",
    "\n",
    "def calculate_position_waveform(row, channel_position, power=2):\n",
    "    \"\"\"\n",
    "    计算位置波形，使用预选的通道\n",
    "    \n",
    "    参数:\n",
    "    - row: DataFrame行，包含'waveform', 'channel_ids', 'position_1', 'position_2'\n",
    "    - channel_position: 通道位置字典\n",
    "    - power: IDW的幂参数\n",
    "    \n",
    "    返回:\n",
    "    - 合成波形 (31个点的数组)\n",
    "    \"\"\"\n",
    "    x_target = row['position_1']\n",
    "    y_target = row['position_2']\n",
    "    waveform = row['waveform']\n",
    "    channel_ids = row['channel_ids']\n",
    "    \n",
    "    # 计算通道位置\n",
    "    positions = np.array([channel_position.get(ch_id, (np.nan, np.nan)) for ch_id in channel_ids])\n",
    "    \n",
    "    # 计算每个通道到目标位置的距离\n",
    "    target_pos = np.array([[x_target, y_target]])\n",
    "    distances = cdist(positions, target_pos).flatten()\n",
    "    \n",
    "    # IDW反距离加权\n",
    "    weights = 1 / (distances ** power)\n",
    "    \n",
    "    # 处理零距离情况\n",
    "    if np.any(distances == 0):\n",
    "        zero_idx = np.argwhere(distances == 0).flatten()\n",
    "        return waveform[zero_idx[0], :]\n",
    "    \n",
    "    # 归一化权重\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    # 使用权重合成波形\n",
    "    synthesized_waveform = np.zeros(waveform.shape[1])\n",
    "    for t in range(waveform.shape[1]):\n",
    "        weighted_sum = np.dot(waveform[:, t], weights)\n",
    "        synthesized_waveform[t] = weighted_sum\n",
    "    \n",
    "    return synthesized_waveform\n",
    "\n",
    "def predict_new(feature, kmeans):\n",
    "    dists = pairwise_distances(feature, kmeans.cluster_centers_ )\n",
    "    return np.argmin(dists, axis=1)\n",
    "\n",
    "def judge_cluster_reality(row, neuron_inf):\n",
    "    from scipy.stats import pearsonr\n",
    "\n",
    "    position_threshold =10\n",
    "    position_condition = (\n",
    "        (abs(neuron_inf['position_1'] - row['position_1']) <= position_threshold) &\n",
    "        (abs(neuron_inf['position_2'] - row['position_2']) <= position_threshold)\n",
    "    )\n",
    "\n",
    "    candidate_neurons = neuron_inf[position_condition]\n",
    "\n",
    "    if candidate_neurons.empty:\n",
    "        return None\n",
    "\n",
    "    waveform_threshold = 0.95\n",
    "    row_waveform = row['position_waveform']\n",
    "    best_match = None\n",
    "    best_corr = -1 \n",
    "\n",
    "    for _, candidate in candidate_neurons.iterrows():\n",
    "        neuron_inf_waveform = candidate['position_waveform']\n",
    "        corr, _ = pearsonr(row_waveform, neuron_inf_waveform)\n",
    "\n",
    "        if corr > waveform_threshold and corr > best_corr:\n",
    "            best_corr = corr\n",
    "            best_match = candidate['cluster']\n",
    "\n",
    "    return best_match if best_match is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "        return self.data[idx].astype(np.float32), self.labels[idx]\n",
    "\n",
    "class Spike_Classification_MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Spike_Classification_MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 31 * 32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_raw = se.MEArecRecordingExtractor(file_path=f'/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/data_generation/setting_1/Neuronexus_32_50_cell_recordings.h5')\n",
    "recording_f = spre.bandpass_filter(recording_raw, freq_min=300, freq_max=3000)\n",
    "recording_f = spre.common_reference(recording_f, reference=\"global\", operator=\"median\")\n",
    "spike_inf = pd.read_csv(f\"/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/data_generation/setting_1/spike_inf.csv\")\n",
    "spike_inf['label'] = spike_inf['Neuron']\n",
    "unique_clusters = np.unique(spike_inf['label'])\n",
    "cluster_to_index = {cluster: idx for idx, cluster in enumerate(unique_clusters)}\n",
    "spike_inf['label'] = np.array([cluster_to_index[cluster] for cluster in spike_inf['label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = int(1000 * 10000)\n",
    "chunk_size = 100000  \n",
    "window_size = 31\n",
    "half_window = window_size // 2\n",
    "\n",
    "all_labels = []\n",
    "all_windows = []\n",
    "\n",
    "for start_frame in range(0, total_frames, chunk_size):\n",
    "    end_frame = min(start_frame + chunk_size, total_frames)\n",
    "    \n",
    "    data_chunk = recording_f.get_traces(\n",
    "        start_frame=start_frame,\n",
    "        end_frame=end_frame\n",
    "    )  # shape: (n_channels, chunk_size)\n",
    "    \n",
    "    spike_inf_temp = spike_inf[(spike_inf['time'] >= start_frame + half_window) & (spike_inf['time'] < end_frame - half_window)]\n",
    "\n",
    "    for idx in spike_inf_temp['time']:\n",
    "        rel_idx = idx - start_frame\n",
    "        window = data_chunk.T[:, rel_idx-half_window : rel_idx+half_window+1]\n",
    "        all_windows.append(window)\n",
    "    \n",
    "    all_labels.extend(spike_inf_temp['label'])\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_windows = np.stack(all_windows) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = recording_f.get_probegroup().to_dataframe()\n",
    "channel_position = {}\n",
    "for i in range(len(probe)):\n",
    "    channel_position[i] = (probe['x'][i], probe['y'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_averages = compute_cluster_average(all_windows, all_labels)\n",
    "\n",
    "processed_averages = process_cluster_averages(cluster_averages, n_neighbors=5, channel_position=channel_position)\n",
    "\n",
    "neuron_inf = pd.DataFrame([\n",
    "    {\"cluster\": cluster, \"waveform\": waveform['waveform'], \"channel_ids\": waveform['channel_ids']}\n",
    "    for cluster, waveform in processed_averages.items()\n",
    "])\n",
    "\n",
    "neuron_inf[['position_1', 'position_2']] = neuron_inf.apply(\n",
    "    lambda row: calculate_position(row, channel_position), axis=1\n",
    ")\n",
    "\n",
    "neuron_inf['position_waveform'] = neuron_inf.apply(\n",
    "    lambda row: calculate_position_waveform(row, channel_position, power=2), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = recording_f.get_traces(\n",
    "        start_frame=1000 * 10000,\n",
    "        end_frame=1600 * 10000\n",
    "    )  \n",
    "\n",
    "threshold_result = detect_local_maxima_in_window(\n",
    "        data.T,  \n",
    "        std_multiplier=0.7\n",
    "    )\n",
    "    \n",
    "threshold_result = np.array(threshold_result) + 1000 * 10000\n",
    "\n",
    "valid_indices = threshold_result[\n",
    "    (threshold_result >= 1000 * 10000 + 15 + 1) & \n",
    "    (threshold_result < 1600 * 10000 - 15)\n",
    "]\n",
    "\n",
    "cluster_labels = cluster_label_array1_based_on_array2(valid_indices, spike_inf, threshold=2, cluster_column='label')\n",
    "\n",
    "potent_spike_inf = pd.DataFrame((valid_indices, cluster_labels), index= ['time', 'label']).T\n",
    "potent_spike_inf = potent_spike_inf[potent_spike_inf['label'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "potent_spike_inf['time'] = potent_spike_inf['time'] - 1000 * 10000\n",
    "index_to_cluster = {idx: cluster for cluster, idx in cluster_to_index.items()}\n",
    "potent_spike_inf['Neuron'] = np.array([index_to_cluster[label] for label in potent_spike_inf['label']])\n",
    "\n",
    "\n",
    "sampled_data = extract_windows(data.T, potent_spike_inf['time'], window_size=31)\n",
    "\n",
    "val_dataset = CustomDataset(sampled_data, potent_spike_inf['label'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/spike_classification/train_results/spike_classification_model_1.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Eval...\n",
      "Start KMeans...\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "predicted_labels = []\n",
    "latent_value = []\n",
    "device = 'cuda'\n",
    "print(\"Start Eval...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in val_loader:\n",
    "        batch_labels = batch_labels.float().unsqueeze(1)\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        batch_data = batch_data.reshape(-1, 31 * 32)\n",
    "        batch_data = model.fc1(batch_data)\n",
    "        batch_data = model.relu1(batch_data)\n",
    "        batch_data = model.fc2(batch_data)\n",
    "        batch_data = model.relu2(batch_data)\n",
    "        latent_value.append(model.fc3(batch_data).cpu())  \n",
    "        \n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "latent_value = torch.cat(latent_value, dim=0).numpy()\n",
    "\n",
    "print('Start KMeans...')\n",
    "latent_value_subset = latent_value[random.sample(range(len(latent_value)), 50000), :]\n",
    "final_kmeans = KMeans(n_clusters=80, n_init=10, random_state=42).fit(latent_value_subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_new(latent_value, final_kmeans)\n",
    "potent_spike_inf['cluster_predicted'] = predicted_labels\n",
    "potent_spike_inf.index = range(len(potent_spike_inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "potent_spike_inf.index = range(len(potent_spike_inf))\n",
    "cluster_averages = compute_cluster_average(sampled_data, potent_spike_inf['cluster_predicted'])\n",
    "processed_averages = process_cluster_averages(cluster_averages, channel_position=channel_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame([\n",
    "    {\"cluster\": cluster, \"waveform\": waveform['waveform'], \"channel_ids\": waveform['channel_ids']}\n",
    "    for cluster, waveform in processed_averages.items()\n",
    "])\n",
    "\n",
    "result_df[['position_1', 'position_2']] = result_df.apply(\n",
    "    lambda row: calculate_position(row, channel_position), axis=1\n",
    ")\n",
    "\n",
    "result_df['position_waveform'] = result_df.apply(\n",
    "    lambda row: calculate_position_waveform(row, channel_position, power=2), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['label'] = 1\n",
    "result_df['label'] = result_df.apply(\n",
    "    lambda row: judge_cluster_reality(row, neuron_inf), axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df[~result_df['label'].isna()]\n",
    "result_df['label'] = result_df['label'].astype(int)\n",
    "result_df['cluster'] = result_df['cluster'].astype(int)\n",
    "\n",
    "potent_spike_inf['label_predicted'] = -1\n",
    "\n",
    "cluster_label_map = result_df.drop_duplicates('cluster').set_index('cluster')['label']\n",
    "\n",
    "mask = potent_spike_inf['cluster_predicted'].isin(cluster_label_map.index)\n",
    "potent_spike_inf.loc[mask, 'label_predicted'] = potent_spike_inf.loc[mask, 'cluster_predicted'].map(cluster_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.crosstab(potent_spike_inf['label'], potent_spike_inf['label_predicted'])\n",
    "a = a.div(a.sum(axis=0), axis=1)\n",
    "a.to_csv(\"/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/spike_classification/eval_results/setting_1/heatmap_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting_jct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from probeinterface import write_prb, read_prb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_array2_in_range_of_array1(array1, array2, threshold=5):\n",
    "\n",
    "    sorted_array1 = np.sort(array1)\n",
    "    \n",
    "    lefts = array2 - threshold\n",
    "    rights = array2 + threshold\n",
    "    \n",
    "    left_indices = np.searchsorted(sorted_array1, lefts, side='left')\n",
    "    \n",
    "    right_indices = np.searchsorted(sorted_array1, rights, side='right')\n",
    "    \n",
    "    has_within_range = right_indices > left_indices\n",
    "    \n",
    "    count = np.sum(has_within_range)\n",
    "    \n",
    "    return count\n",
    "\n",
    "\n",
    "def detect_local_maxima_in_window(data, window_size=20, std_multiplier=2):\n",
    "\n",
    "    \"\"\"\n",
    "    在每个滑动窗口范围内检测局部最大值的索引，并确保最大值大于两倍的标准差。\n",
    "\n",
    "    参数:\n",
    "    data : numpy.ndarray\n",
    "        输入数据，形状为 (n_rows, n_columns)。\n",
    "    window_size : int\n",
    "        滑动窗口的大小，用于定义局部范围，默认为 20。\n",
    "    std_multiplier : float\n",
    "        标准差的倍数，用于筛选局部最大值，默认为 2。\n",
    "\n",
    "    返回:\n",
    "    local_maxima_indices : list of numpy.ndarray\n",
    "        每行局部最大值的索引列表，每个元素是对应行局部最大值的索引数组。\n",
    "    \"\"\"\n",
    "    local_maxima_indices = []\n",
    "\n",
    "    for row in data:\n",
    "        maxima_indices = []\n",
    "        row_std = np.std(row)\n",
    "        threshold = std_multiplier * row_std\n",
    "\n",
    "        for start in range(0, len(row), window_size):\n",
    "            end = min(start + window_size, len(row))\n",
    "            window = row[start:end]\n",
    "            \n",
    "            if len(window) > 0:\n",
    "                local_max_index = np.argmax(window)\n",
    "                local_max_value = window[local_max_index]\n",
    "                \n",
    "                if local_max_value > threshold:\n",
    "                    maxima_indices.append(start + local_max_index)  \n",
    "        \n",
    "        local_maxima_indices.extend(maxima_indices)\n",
    "        local_maxima_indices = list(set(local_maxima_indices))  \n",
    "\n",
    "    return local_maxima_indices\n",
    "\n",
    "\n",
    "def cluster_label_array1_based_on_array2(array1, array2, threshold=5, \n",
    "                                         cluster_column='cluster'):\n",
    "\n",
    "    \"\"\"\n",
    "    根据 array2 的 'time' 和 'cluster' 对 array1 进行标记。\n",
    "    如果 array1 中的某个值在 threshold 范围内存在于 array2 的 'time' 中，则标记为对应的 'cluster' 值，否则为 0。\n",
    "    \n",
    "    参数:\n",
    "    array1 : numpy.ndarray\n",
    "        要标记的数组。\n",
    "    array2 : numpy.ndarray\n",
    "        包含 'time' 和 'cluster' 的二维数组。\n",
    "        第一列为 'time'，第二列为 'cluster'。\n",
    "    threshold : int\n",
    "        判断范围的阈值。\n",
    "    \n",
    "    返回:\n",
    "    labels : numpy.ndarray\n",
    "        长度为 len(array1) 的标签数组，值为 array2 中的 'cluster' 或 0。\n",
    "    \"\"\"\n",
    "\n",
    "    array2 = np.array((array2['time'], array2[cluster_column])).T\n",
    "    sorted_indices = np.argsort(array2[:, 0])\n",
    "    sorted_array2 = array2[sorted_indices]\n",
    "    \n",
    "    labels = -np.ones(len(array1), dtype=int)\n",
    "    \n",
    "    # 遍历 array1 中的每个元素\n",
    "    for i, value in enumerate(array1):\n",
    "        # 计算当前值的范围\n",
    "        left = value - threshold\n",
    "        right = value + threshold\n",
    "        \n",
    "        left_index = np.searchsorted(sorted_array2[:, 0], left, side='left')\n",
    "        right_index = np.searchsorted(sorted_array2[:, 0], right, side='right')\n",
    "        \n",
    "        # 如果范围内存在值，则标记为对应的 'cluster'\n",
    "        if right_index > left_index:\n",
    "            # 获取范围内的第一个匹配值的 'cluster'\n",
    "            labels[i] = sorted_array2[left_index, 1]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def label_array1_based_on_array2(array1, array2, threshold=5):\n",
    "\n",
    "    \"\"\"\n",
    "    根据 array2 的值对 array1 进行标记。\n",
    "    如果 array1 中的某个值在 threshold 范围内存在于 array2 中，则标记为 1，否则为 0。\n",
    "    \n",
    "    参数:\n",
    "    array1 : numpy.ndarray\n",
    "        要标记的数组。\n",
    "    array2 : numpy.ndarray\n",
    "        用于判断的数组。\n",
    "    threshold : int\n",
    "        判断范围的阈值。\n",
    "    \n",
    "    返回:\n",
    "    labels : numpy.ndarray\n",
    "        长度为 len(array1) 的标签数组，值为 0 或 1。\n",
    "    \"\"\"\n",
    "    # 对 array2 进行排序以加速搜索\n",
    "    sorted_array2 = np.sort(array2)\n",
    "    \n",
    "    # 初始化标签数组，默认值为 0\n",
    "    labels = np.zeros(len(array1), dtype=int)\n",
    "    \n",
    "    # 遍历 array1 中的每个元素\n",
    "    for i, value in enumerate(array1):\n",
    "        # 计算当前值的范围\n",
    "        left = value - threshold\n",
    "        right = value + threshold\n",
    "        \n",
    "        # 使用二分搜索判断范围内是否存在值\n",
    "        left_index = np.searchsorted(sorted_array2, left, side='left')\n",
    "        right_index = np.searchsorted(sorted_array2, right, side='right')\n",
    "        \n",
    "        # 如果范围内存在值，则标记为 1\n",
    "        if right_index > left_index:\n",
    "            labels[i] = 1\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def extract_windows(data, indices, window_size=61):\n",
    "    \"\"\"\n",
    "    根据给定的时间点索引提取窗口。\n",
    "    \n",
    "    参数:\n",
    "    data : numpy.ndarray\n",
    "        输入数据，形状为 (n_channels, time)\n",
    "    indices : numpy.ndarray\n",
    "        时间点索引数组，用于指定需要提取窗口的中心点\n",
    "    window_size : int\n",
    "        窗口长度，默认为61（对应time-30到time+31）\n",
    "    \n",
    "    返回:\n",
    "    windows : numpy.ndarray\n",
    "        提取的窗口数据，形状为 (len(indices), n_channels, window_size)\n",
    "    \"\"\"\n",
    "    n_channels, time_length = data.shape\n",
    "    half_window = window_size // 2\n",
    "\n",
    "    if np.any(indices < half_window) or np.any(indices >= time_length - half_window):\n",
    "        raise ValueError(\"Some indices are out of bounds for the given window size.\")\n",
    "\n",
    "    windows = []\n",
    "    for idx in indices:\n",
    "        window = data[:, idx - half_window:idx + half_window + 1]\n",
    "        windows.append(window)\n",
    "\n",
    "    windows = np.array(windows)\n",
    "    return windows\n",
    "def calculate_position(row):\n",
    "    probe_group = str(row['probe_group'])\n",
    "    channels = channel_indices[probe_group]\n",
    "    waveform = row['waveform'] \n",
    "    \n",
    "    a_squared = [np.sum(waveform[:, j]**2) for j in range(len(channels))]\n",
    "    \n",
    "    sum_x_a = 0\n",
    "    sum_y_a = 0\n",
    "    sum_a = 0\n",
    "    \n",
    "    for j, channel in enumerate(channels):\n",
    "        x_i, y_i = channel_position.get(channel, [0, 0])  \n",
    "        a_i_sq = a_squared[j]\n",
    "        \n",
    "        sum_x_a += x_i * a_i_sq\n",
    "        sum_y_a += y_i * a_i_sq\n",
    "        sum_a += a_i_sq\n",
    "    \n",
    "    if sum_a == 0:\n",
    "        return pd.Series({'position_1': 0, 'position_2': 0})\n",
    "    \n",
    "    x_hat = sum_x_a / sum_a\n",
    "    y_hat = sum_y_a / sum_a\n",
    "    return pd.Series({'position_1': x_hat, 'position_2': y_hat})\n",
    "\n",
    "def calculate_position_waveform(row, channel_position, channel_indices, power=2):\n",
    "    x_target = row['position_1']\n",
    "    y_target = row['position_2']\n",
    "    probe_group = str(row['probe_group'])\n",
    "    channels = channel_indices[probe_group]  \n",
    "    waveforms = row['waveform']  \n",
    "    \n",
    "    distances = []\n",
    "    for channel in channels:\n",
    "        x_channel, y_channel = channel_position.get(channel, [np.nan, np.nan])\n",
    "        if np.isnan(x_channel):  \n",
    "            continue\n",
    "        distance = np.sqrt((x_target - x_channel)**2 + (y_target - y_channel)**2)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    if not distances:  \n",
    "        return np.zeros(31)\n",
    "    \n",
    "    #IDW\n",
    "    weights = 1 / (np.array(distances) ** power)\n",
    "    if np.any(distances == 0):\n",
    "        zero_idx = np.argwhere(distances == 0).flatten()\n",
    "        return waveforms[:, zero_idx[0]]\n",
    "    \n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    synthesized_waveform = np.zeros(31)\n",
    "    for t in range(31): \n",
    "        weighted_sum = np.dot(waveforms[t, :], weights)\n",
    "        synthesized_waveform[t] = weighted_sum\n",
    "    \n",
    "    return synthesized_waveform\n",
    "\n",
    "\n",
    "def compute_cluster_average(sample_data, potent_spike_inf, cluster_column='cluster_predicted'):\n",
    "    \"\"\"\n",
    "    计算 potent_spike_inf 中每个 cluster_predicted 对应的 sample_data 的平均值。\n",
    "    \n",
    "    参数:\n",
    "    - sample_data: np.ndarray, 输入的 (n, 30, 61) 矩阵。\n",
    "    - potent_spike_inf: pd.DataFrame, 包含 cluster_predicted 信息的 DataFrame。\n",
    "    - cluster_column: str, cluster 信息所在的列名。\n",
    "    \n",
    "    返回:\n",
    "    - cluster_averages: dict, 每个 cluster 对应的平均值矩阵 (30, 61)。\n",
    "    \"\"\"\n",
    "    cluster_averages = {}\n",
    "    unique_clusters = potent_spike_inf[cluster_column].unique()\n",
    "    \n",
    "    for cluster in unique_clusters:\n",
    "        cluster_indices = potent_spike_inf[potent_spike_inf[cluster_column] == cluster].index\n",
    "        cluster_average = sample_data[cluster_indices].mean(axis=0) \n",
    "        cluster_averages[cluster] = cluster_average\n",
    "    \n",
    "    return cluster_averages\n",
    "\n",
    "def judge_cluster_reality(row, neuron_inf):\n",
    "    from scipy.stats import pearsonr\n",
    "\n",
    "    position_threshold =10\n",
    "    position_condition = (\n",
    "        (abs(neuron_inf['position_1'] - row['position_1']) <= position_threshold) &\n",
    "        (abs(neuron_inf['position_2'] - row['position_2']) <= position_threshold)\n",
    "    )\n",
    "\n",
    "    candidate_neurons = neuron_inf[position_condition]\n",
    "\n",
    "    if candidate_neurons.empty:\n",
    "        return None\n",
    "\n",
    "    waveform_threshold = 0.95\n",
    "    row_waveform = row['position_waveform']\n",
    "    best_match = None\n",
    "    best_corr = -1 \n",
    "\n",
    "    for _, candidate in candidate_neurons.iterrows():\n",
    "        neuron_inf_waveform = candidate['position_waveform'][15:-15]\n",
    "        corr, _ = pearsonr(row_waveform, neuron_inf_waveform)\n",
    "\n",
    "        if corr > waveform_threshold and corr > best_corr:\n",
    "            best_corr = corr\n",
    "            best_match = candidate['cluster']\n",
    "\n",
    "    return best_match if best_match is not None else None\n",
    "\n",
    "\n",
    "def process_cluster_averages(cluster_averages, channel_indices):\n",
    "    \"\"\"\n",
    "    对 cluster_averages 中的每个 item，找到最大值所在的通道，\n",
    "    并根据 channel_indices 保留对应的 6 个通道。\n",
    "    \n",
    "    参数:\n",
    "    - cluster_averages: dict, 每个 cluster 的平均值 (30, 61)。\n",
    "    - channel_indices: dict, 通道索引字典。\n",
    "    \n",
    "    返回:\n",
    "    - processed_averages: dict, 处理后的字典，键为 cluster_channelindices，值为 (6, 61) 的数组。\n",
    "    \"\"\"\n",
    "    processed_averages = {}\n",
    "    \n",
    "    for cluster, avg_matrix in cluster_averages.items():\n",
    "        max_channel = np.argmax(avg_matrix.max(axis=1))  \n",
    "        \n",
    "        for key, indices in channel_indices.items():\n",
    "            if max_channel in indices:\n",
    "                selected_channels = avg_matrix[indices, :]\n",
    "                new_key = f\"{cluster}_{key}\"\n",
    "                processed_averages[new_key] = selected_channels\n",
    "                break\n",
    "    \n",
    "    return processed_averages\n",
    "\n",
    "def predict_new(feature, kmeans):\n",
    "    dists = pairwise_distances(feature, kmeans.cluster_centers_ )\n",
    "    return np.argmin(dists, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "        return self.data[idx].astype(np.float32), self.labels[idx]\n",
    "\n",
    "class Spike_Classification_MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Spike_Classification_MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 31 * 32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m end_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_frame \u001b[38;5;241m+\u001b[39m chunk_size, total_frames)\n\u001b[1;32m     20\u001b[0m data_chunk \u001b[38;5;241m=\u001b[39m recording_f\u001b[38;5;241m.\u001b[39mget_traces(\n\u001b[1;32m     21\u001b[0m     start_frame\u001b[38;5;241m=\u001b[39mstart_frame,\n\u001b[1;32m     22\u001b[0m     end_frame\u001b[38;5;241m=\u001b[39mend_frame\n\u001b[1;32m     23\u001b[0m )  \u001b[38;5;66;03m# shape: (n_channels, chunk_size)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m threshold_result \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_local_maxima_in_window\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstd_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m threshold_result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(threshold_result) \u001b[38;5;241m+\u001b[39m start_frame\n\u001b[1;32m     31\u001b[0m valid_indices \u001b[38;5;241m=\u001b[39m threshold_result[\n\u001b[1;32m     32\u001b[0m     (threshold_result \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_frame \u001b[38;5;241m+\u001b[39m half_window \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \n\u001b[1;32m     33\u001b[0m     (threshold_result \u001b[38;5;241m<\u001b[39m end_frame \u001b[38;5;241m-\u001b[39m half_window)\n\u001b[1;32m     34\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m, in \u001b[0;36mdetect_local_maxima_in_window\u001b[0;34m(data, window_size, std_multiplier)\u001b[0m\n\u001b[1;32m     45\u001b[0m window \u001b[38;5;241m=\u001b[39m row[start:end]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(window) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     local_max_index \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     local_max_value \u001b[38;5;241m=\u001b[39m window[local_max_index]\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local_max_value \u001b[38;5;241m>\u001b[39m threshold:\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/data_generation/setting_2_neuron_num/recordings/\"):\n",
    "    file = file.split(\".\")[0]\n",
    "    recording_raw = se.MEArecRecordingExtractor(file_path=f'/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/data_generation/setting_2_neuron_num/recordings/{file}.h5')\n",
    "    recording_f = spre.bandpass_filter(recording_raw, freq_min=300, freq_max=3000)\n",
    "    recording_f = spre.common_reference(recording_f, reference=\"global\", operator=\"median\")\n",
    "    spike_inf = pd.read_csv(f\"/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/data_generation/setting_2_neuron_num/spike_inf/{file}_spike_inf.csv\")\n",
    "    os.makedirs(f'/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/spike_classification/train_results/setting_2/{file}', exist_ok=True)\n",
    "\n",
    "    total_frames = int(1000 * 10000)\n",
    "    chunk_size = 100000  \n",
    "    window_size = 31\n",
    "    half_window = window_size // 2\n",
    "\n",
    "    all_valid_indices = []\n",
    "    all_windows = []\n",
    "\n",
    "    for start_frame in range(0, total_frames, chunk_size):\n",
    "        end_frame = min(start_frame + chunk_size, total_frames)\n",
    "        \n",
    "        data_chunk = recording_f.get_traces(\n",
    "            start_frame=start_frame,\n",
    "            end_frame=end_frame\n",
    "        )  # shape: (n_channels, chunk_size)\n",
    "        \n",
    "        threshold_result = detect_local_maxima_in_window(\n",
    "            data_chunk.T,  \n",
    "            std_multiplier=0.7\n",
    "        )\n",
    "        \n",
    "        threshold_result = np.array(threshold_result) + start_frame\n",
    "        valid_indices = threshold_result[\n",
    "            (threshold_result >= start_frame + half_window + 1) & \n",
    "            (threshold_result < end_frame - half_window)\n",
    "        ]\n",
    "        \n",
    "        for idx in valid_indices:\n",
    "            rel_idx = idx - start_frame\n",
    "            window = data_chunk.T[:, rel_idx-half_window : rel_idx+half_window+1]\n",
    "            all_windows.append(window)\n",
    "        \n",
    "        all_valid_indices.extend(valid_indices)\n",
    "\n",
    "    all_valid_indices = np.array(all_valid_indices)\n",
    "    all_windows = np.stack(all_windows)  \n",
    "\n",
    "    unique_clusters = np.unique(spike_inf['Neuron'])\n",
    "    cluster_to_index = {cluster: idx for idx, cluster in enumerate(unique_clusters)}\n",
    "    spike_inf['Neuron'] = np.array([cluster_to_index[cluster] for cluster in spike_inf['Neuron']])\n",
    "    \n",
    "\n",
    "    cluster_labels = cluster_label_array1_based_on_array2(all_valid_indices, spike_inf, threshold=1, cluster_column='Neuron')\n",
    "\n",
    "    valid_indices = cluster_labels != -1\n",
    "    all_windows = all_windows[valid_indices]\n",
    "    cluster_labels = cluster_labels[valid_indices]\n",
    "\n",
    "    potent_spike_inf = pd.DataFrame((all_valid_indices[valid_indices], cluster_labels), index= ['time', 'cluster']).T\n",
    "\n",
    "    indices = potent_spike_inf['time'].values\n",
    "\n",
    "    labels = potent_spike_inf['cluster'].values\n",
    "\n",
    "    balanced_indices = []\n",
    "    for cluster in potent_spike_inf['cluster'].unique():\n",
    "        cluster_indices = np.where(labels == cluster)[0]\n",
    "        if len(cluster_indices) > 8000:\n",
    "            sampled_indices = np.random.choice(cluster_indices, 8000, replace=False)\n",
    "        else:\n",
    "            sampled_indices = cluster_indices\n",
    "        balanced_indices.extend(sampled_indices)\n",
    "\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    balanced_data = all_windows[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    dataset = CustomDataset(balanced_data, balanced_labels)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    batch_size = 1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for trail in range(1, 2):\n",
    "        device = 'cuda'\n",
    "        input_size = balanced_data.shape[1] * balanced_data.shape[2]\n",
    "        hidden_size1 = 64\n",
    "        hidden_size2 = 50\n",
    "        num_classes = potent_spike_inf['cluster'].nunique()\n",
    "        model = Spike_Classification_MLP(input_size, hidden_size1, hidden_size2, num_classes)\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()  \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "        num_epochs = 210\n",
    "        accuracy_best = 0\n",
    "        i = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            all_labels = []\n",
    "            all_predictions = []\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for batch_data, batch_labels in train_loader:\n",
    "                batch_data = batch_data.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "\n",
    "                outputs = model(batch_data)\n",
    "                predicted = torch.argmax(outputs, dim=1)  \n",
    "\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            accuracy = accuracy_score(all_labels, all_predictions)\n",
    "            # print(f\"Train Accuracy: {accuracy * 100:.2f}%\")\n",
    "            # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "            model.eval()\n",
    "            all_labels = []\n",
    "            all_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch_data, batch_labels in test_loader:\n",
    "                    batch_data = batch_data.to(device)\n",
    "                    batch_labels = batch_labels.to(device)\n",
    "\n",
    "                    outputs = model(batch_data)\n",
    "                    predicted = torch.argmax(outputs, dim=1)  \n",
    "\n",
    "                    all_labels.extend(batch_labels.cpu().numpy())\n",
    "                    all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            all_labels = np.array(all_labels)\n",
    "            all_predictions = np.array(all_predictions)\n",
    "\n",
    "            accuracy = accuracy_score(all_labels, all_predictions)\n",
    "            if accuracy > accuracy_best:\n",
    "                accuracy_best = accuracy\n",
    "                i = 0\n",
    "                torch.save(model, f'/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/spike_classification/train_results/setting_2/{file}/spike_classification_model_{trail}.pth')\n",
    "                print(f\"Best model saved with TPR: {accuracy_best:.4f}\")\n",
    "                print(\"_\" * 60)\n",
    "                      \n",
    "            else:\n",
    "                i += 1\n",
    "                if i == 3:\n",
    "                    print(f\"Training stopped after {epoch+1} epochs with best TPR: {accuracy_best:.4f}\")\n",
    "                    print(\"_\" * 60)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_raw = se.MEArecRecordingExtractor(file_path=f'/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/data_generation/setting_2_neuron_num/recordings/Neuronexus_32_20_recording.h5')\n",
    "recording_f = spre.bandpass_filter(recording_raw, freq_min=300, freq_max=3000)\n",
    "recording_f = spre.common_reference(recording_f, reference=\"global\", operator=\"median\")\n",
    "spike_inf = pd.read_csv(f\"/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/data_generation/setting_2_neuron_num/spike_inf/Neuronexus_32_20_recording_spike_inf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = int(1000 * 10000)\n",
    "chunk_size = 100000  \n",
    "window_size = 31\n",
    "half_window = window_size // 2\n",
    "\n",
    "all_valid_indices = []\n",
    "all_windows = []\n",
    "\n",
    "for start_frame in range(0, total_frames, chunk_size):\n",
    "    end_frame = min(start_frame + chunk_size, total_frames)\n",
    "    \n",
    "    data_chunk = recording_f.get_traces(\n",
    "        start_frame=start_frame,\n",
    "        end_frame=end_frame\n",
    "    )  # shape: (n_channels, chunk_size)\n",
    "    \n",
    "    threshold_result = detect_local_maxima_in_window(\n",
    "        data_chunk.T,  \n",
    "        std_multiplier=0\n",
    "    )\n",
    "    \n",
    "    threshold_result = np.array(threshold_result) + start_frame\n",
    "    valid_indices = threshold_result[\n",
    "        (threshold_result >= start_frame + half_window + 1) & \n",
    "        (threshold_result < end_frame - half_window)\n",
    "    ]\n",
    "    \n",
    "    for idx in valid_indices:\n",
    "        rel_idx = idx - start_frame\n",
    "        window = data_chunk.T[:, rel_idx-half_window : rel_idx+half_window+1]\n",
    "        all_windows.append(window)\n",
    "    \n",
    "    all_valid_indices.extend(valid_indices)\n",
    "\n",
    "all_valid_indices = np.array(all_valid_indices)\n",
    "all_windows = np.stack(all_windows)  \n",
    "\n",
    "unique_clusters = np.unique(spike_inf['Neuron'])\n",
    "cluster_to_index = {cluster: idx for idx, cluster in enumerate(unique_clusters)}\n",
    "spike_inf['Neuron'] = np.array([cluster_to_index[cluster] for cluster in spike_inf['Neuron']])\n",
    "\n",
    "# cluster_labels = cluster_label_array1_based_on_array2(all_valid_indices, spike_inf, threshold=1, cluster_column='Neuron')\n",
    "\n",
    "# valid_indices = cluster_labels != -1\n",
    "# all_windows = all_windows[valid_indices]\n",
    "# cluster_labels = cluster_labels[valid_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "potent_spike_inf = pd.DataFrame((all_valid_indices[valid_indices], cluster_labels), index= ['time', 'cluster']).T\n",
    "# potent_spike_inf = potent_spike_inf[potent_spike_inf['cluster'] != -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = potent_spike_inf['time'].values\n",
    "\n",
    "labels = potent_spike_inf['cluster'].values\n",
    "\n",
    "balanced_indices = []\n",
    "for cluster in potent_spike_inf['cluster'].unique():\n",
    "    cluster_indices = np.where(labels == cluster)[0]\n",
    "    if len(cluster_indices) > 8000:\n",
    "        sampled_indices = np.random.choice(cluster_indices, 8000, replace=False)\n",
    "    else:\n",
    "        sampled_indices = cluster_indices\n",
    "    balanced_indices.extend(sampled_indices)\n",
    "\n",
    "np.random.shuffle(balanced_indices)\n",
    "\n",
    "balanced_data = all_windows[balanced_indices]\n",
    "balanced_labels = labels[balanced_indices]\n",
    "\n",
    "dataset = CustomDataset(balanced_data, balanced_labels)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with TPR: 0.2984\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.4647\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.6219\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.7371\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.7923\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.8325\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.8577\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.8727\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.8846\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.8945\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9040\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9114\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9180\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9246\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9295\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9336\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9374\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9409\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9442\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9469\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9490\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9511\n",
      "____________________________________________________________\n",
      "Best model saved with TPR: 0.9534\n",
      "____________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/spike_sorting_jct/lib/python3.11/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for trail in range(1, 2):\n",
    "    device = 'cuda'\n",
    "    input_size = balanced_data.shape[1] * balanced_data.shape[2]\n",
    "    hidden_size1 = 64\n",
    "    hidden_size2 = 50\n",
    "    num_classes = potent_spike_inf['cluster'].nunique()\n",
    "    model = Spike_Classification_MLP(input_size, hidden_size1, hidden_size2, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "    num_epochs = 210\n",
    "    accuracy_best = 0\n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "\n",
    "            outputs = model(batch_data)\n",
    "            predicted = torch.argmax(outputs, dim=1)  \n",
    "\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        # print(f\"Train Accuracy: {accuracy * 100:.2f}%\")\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in test_loader:\n",
    "                batch_data = batch_data.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                outputs = model(batch_data)\n",
    "                predicted = torch.argmax(outputs, dim=1)  \n",
    "\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        if accuracy > accuracy_best:\n",
    "            accuracy_best = accuracy\n",
    "            i = 0\n",
    "            torch.save(model, f'/media/ubuntu/sda/Spike_Sorting/paper_architecture/02_simulation_data/01_Neuroxenus_32_channels/spike_classification/eval_results/setting_2/Neuronexus_32_20_recording/spike_classification_model_{trail}.pth')\n",
    "            print(f\"Best model saved with TPR: {accuracy_best:.4f}\")\n",
    "            print(\"_\" * 60)\n",
    "\n",
    "        else:\n",
    "            i += 1\n",
    "            if i == 3:\n",
    "                print(f\"Training stopped after {epoch+1} epochs with best TPR: {accuracy_best:.4f}\")\n",
    "                print(\"_\" * 60)\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recording_f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecording_f\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recording_f' is not defined"
     ]
    }
   ],
   "source": [
    "recording_f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting_jct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

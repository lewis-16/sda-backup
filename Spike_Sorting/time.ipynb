{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_sorting_utils.basic_util import *\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from probeinterface import write_prb, read_prb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "class Spike_Detection_MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(Spike_Detection_MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 61 * 30)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class Spike_Classification_MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Spike_Classification_MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 61 * 30)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_detection_model = torch.load(\"/home/ubuntu/Documents/jct/project/code/Spike_Sorting/spike_detection/new_model/spike_detection_model_1.pth\")\n",
    "spike_classification_model = torch.load(\"/home/ubuntu/Documents/jct/project/code/Spike_Sorting/spike_classification/new_model/spike_classification_model_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_raw = se.read_blackrock(file_path='/media/ubuntu/sda/data/mouse6/ns4/natural_image/mouse6_022522_natural_image_001.ns4')\n",
    "recording_recorded = recording_raw.remove_channels(['98', '31', '32'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/Documents/jct/project/code/Spike_Sorting/sorting_results/021322/neuron_inf.pkl', 'rb') as f:\n",
    "    neuron_inf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_indices = {\n",
    "        \"1\": [1, 3, 5, 7, 9, 11],\n",
    "        \"2\": [13, 15, 17, 19, 21, 23],\n",
    "        \"3\": [24, 25, 26, 27, 28, 29],\n",
    "        \"4\": [12, 14, 16, 18, 20, 22],\n",
    "        \"5\": [0, 2, 4, 6, 8, 10]\n",
    "        }\n",
    "channel_position = {\n",
    "    0: [650, 0],\n",
    "    2: [650, 50],\n",
    "    4: [650, 100],\n",
    "    6: [600, 100],\n",
    "    8: [600, 50],\n",
    "    10: [600, 0],\n",
    "    1: [0, 0],\n",
    "    3: [0, 50],\n",
    "    5: [0, 100],\n",
    "    7: [50, 100],\n",
    "    9: [50, 50],\n",
    "    11: [50, 0],\n",
    "    13: [150, 200], \n",
    "    15: [150, 250],\n",
    "    17: [150, 300],\n",
    "    19: [200, 300],\n",
    "    21: [200, 250],\n",
    "    23: [200, 200],\n",
    "    12: [500, 200],\n",
    "    14: [500, 250],\n",
    "    16: [500, 300],\n",
    "    18: [450, 300],\n",
    "    20: [450, 250],\n",
    "    22: [450, 200],\n",
    "    24: [350, 400],\n",
    "    26: [350, 450],\n",
    "    28: [350, 500],\n",
    "    25: [300, 400],\n",
    "    27: [300, 450],\n",
    "    29: [300, 500]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set-up Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_slice = recording_recorded.time_slice(0, 10)\n",
    "recording_f_22522 = spre.bandpass_filter(recording_slice, freq_min=300, freq_max=3000)\n",
    "recording_f_22522 = spre.common_reference(recording_f_22522, reference=\"global\", operator=\"median\")\n",
    "data_22522 = recording_f_22522.get_traces().astype(\"float32\").T\n",
    "\n",
    "threshold_result_22522 = detect_local_maxima_in_window(data_22522)\n",
    "threshold_result_22522 = np.array(threshold_result_22522)\n",
    "valid_indices_22522 = threshold_result_22522[(threshold_result_22522 > 30)]\n",
    "valid_indices_22522 = valid_indices_22522[valid_indices_22522 < data_22522.shape[1] - 31]\n",
    "\n",
    "potent_spike_inf = pd.DataFrame(valid_indices_22522, columns= ['time'])\n",
    "data_input = extract_windows(data_22522, valid_indices_22522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpikeDataset(data_input)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "device = 'cuda'\n",
    "spike_detection_model = spike_detection_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in val_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        outputs = spike_detection_model(batch_data)\n",
    "        predicted = (outputs > 0.5).float()  \n",
    "\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        \n",
    "predicted_labels = np.array(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potent_spike_inf['spike_detection_label'] = predicted_labels\n",
    "potent_spike_inf = potent_spike_inf[potent_spike_inf['spike_detection_label'] == 1]\n",
    "\n",
    "potent_spikes = np.where(predicted_labels == 1)[0]\n",
    "data_input = data_input[potent_spikes, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpikeDataset(data_input)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_value = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in val_loader:\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_data = batch_data.reshape(-1, 61 * 30)\n",
    "        batch_data = spike_classification_model.fc1(batch_data)\n",
    "        batch_data = spike_classification_model.relu1(batch_data)\n",
    "        batch_data = spike_classification_model.fc2(batch_data)\n",
    "        batch_data = spike_classification_model.relu2(batch_data)\n",
    "        latent_value.append(batch_data.cpu())  \n",
    "        \n",
    "latent_value = torch.cat(latent_value, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kmeans = KMeans(n_clusters=50, n_init=10, random_state=42).fit(latent_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_new(latent_value, final_kmeans)\n",
    "\n",
    "potent_spike_inf['cluster_predicted'] = predicted_labels\n",
    "potent_spike_inf.index = range(len(potent_spike_inf))\n",
    "\n",
    "cluster_averages = compute_cluster_average(data_input, potent_spike_inf)\n",
    "processed_averages = process_cluster_averages(cluster_averages, channel_indices)\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"cluster\": key.split(\"_\")[0], \"probe_group\": key.split(\"_\")[1], \"waveform\": value.T}\n",
    "    for key, value in processed_averages.items()\n",
    "])\n",
    "\n",
    "df[['position_1', 'position_2']] = df.apply(\n",
    "    lambda row: calculate_position(row, channel_indices, channel_position), axis=1\n",
    ")\n",
    "df['position_waveform'] = df.apply(\n",
    "    lambda row: calculate_position_waveform(row, channel_position, channel_indices), axis=1\n",
    ")\n",
    "\n",
    "df['label'] = 1\n",
    "df['label'] = df.apply(\n",
    "    lambda row: judge_cluster_reality(row, neuron_inf), axis=1\n",
    ")\n",
    "\n",
    "df = df[~df['label'].isna()]\n",
    "df['cluster'] = df['cluster'].astype(int)\n",
    "\n",
    "\n",
    "potent_spike_inf['label'] = '-1'\n",
    "\n",
    "for i, row in potent_spike_inf.iterrows():\n",
    "    df_temp = df[df['cluster'] == row['cluster_predicted']]\n",
    "    \n",
    "    if not df_temp.empty:\n",
    "        potent_spike_inf.loc[i, 'label'] = df_temp['label'].values[0] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real-time implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_1s = []\n",
    "device = 'cuda'\n",
    "batch_size = 1024\n",
    "for i in range(10, 150):\n",
    "    t1 = time.time()\n",
    "    recording_slice = recording_recorded.time_slice(i, i+1)\n",
    "    recording_f_22522 = spre.bandpass_filter(recording_slice, freq_min=300, freq_max=3000)\n",
    "    recording_f_22522 = spre.common_reference(recording_f_22522, reference=\"global\", operator=\"median\")\n",
    "    data_22522 = recording_f_22522.get_traces().astype(\"float32\").T\n",
    "\n",
    "    threshold_result_22522 = detect_local_maxima_in_window(data_22522)\n",
    "    threshold_result_22522 = np.array(threshold_result_22522)\n",
    "    valid_indices_22522 = threshold_result_22522[(threshold_result_22522 > 30) & \n",
    "                                            (threshold_result_22522 < data_22522.shape[1] - 31)]\n",
    "\n",
    "    potent_spike_inf_temp = pd.DataFrame(valid_indices_22522, columns= ['time'])\n",
    "    data_input = extract_windows(data_22522, valid_indices_22522)\n",
    "\n",
    "    val_dataset = SpikeDataset(data_input)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predicted_labels = np.empty(len(val_dataset), dtype=np.float32)  \n",
    "    spike_detection_model = spike_detection_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_data in enumerate(val_loader):            \n",
    "            batch_data = batch_data.to(device)\n",
    "\n",
    "            outputs = spike_detection_model(batch_data)\n",
    "            predicted = (outputs > 0.5).float()  \n",
    "\n",
    "            start = idx * batch_size\n",
    "            end = start + len(batch_data)\n",
    "            predicted_labels[start:end] = predicted.squeeze(1).cpu().numpy()\n",
    "            \n",
    "    potent_spike_inf_temp['spike_detection_label'] = predicted_labels\n",
    "    potent_spike_inf_temp = potent_spike_inf_temp[potent_spike_inf_temp['spike_detection_label'] == 1]\n",
    "\n",
    "    potent_spikes = np.where(predicted_labels == 1)[0]\n",
    "    data_input = data_input[potent_spikes, :, :]\n",
    "\n",
    "    val_dataset = SpikeDataset(data_input)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    latent_value = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in val_loader:\n",
    "\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_data = batch_data.reshape(-1, 61 * 30)\n",
    "            batch_data = spike_classification_model.relu2(spike_classification_model.fc2(spike_classification_model.relu1(spike_classification_model.fc1(batch_data))))\n",
    "            latent_value.append(batch_data.cpu())  \n",
    "            \n",
    "    latent_value = torch.cat(latent_value, dim=0).numpy()\n",
    "\n",
    "\n",
    "    predicted_labels = predict_new(latent_value, final_kmeans)\n",
    "    potent_spike_inf_temp['cluster_predicted'] = predicted_labels\n",
    "    potent_spike_inf_temp.index = range(len(potent_spike_inf_temp))\n",
    "\n",
    "    cluster_averages = compute_cluster_average(data_input, potent_spike_inf_temp)\n",
    "    processed_averages = process_cluster_averages(cluster_averages, channel_indices)\n",
    "\n",
    "    df_temp = pd.DataFrame([\n",
    "        {\"cluster\": key.split(\"_\")[0], \"probe_group\": key.split(\"_\")[1], \"waveform\": value.T}\n",
    "        for key, value in processed_averages.items()\n",
    "    ])\n",
    "\n",
    "    df_temp[['position_1', 'position_2']] = df_temp.apply(\n",
    "        lambda row: calculate_position(row, channel_indices, channel_position), axis=1\n",
    "    )\n",
    "    df_temp['position_waveform'] = df_temp.apply(\n",
    "        lambda row: calculate_position_waveform(row, channel_position, channel_indices), axis=1\n",
    "    )\n",
    "\n",
    "    df_temp['label'] = 1\n",
    "    df_temp['label'] = df_temp.apply(\n",
    "        lambda row: judge_cluster_reality(row, neuron_inf), axis=1\n",
    "    )\n",
    "\n",
    "    df_temp = df_temp[~df_temp['label'].isna()]\n",
    "    df_temp['cluster'] = df_temp['cluster'].astype(int)\n",
    "\n",
    "\n",
    "    potent_spike_inf['label'] = '-1'\n",
    "\n",
    "    label_map = df_temp.set_index('cluster')['label'].to_dict()\n",
    "    potent_spike_inf_temp['label'] = potent_spike_inf_temp['cluster_predicted'].map(label_map).fillna('-1')\n",
    "    t2 = time.time()\n",
    "    #potent_spike_inf = pd.concat((potent_spike_inf, potent_spike_inf_temp), axis=0)\n",
    "    time_1s.extend([t2-t1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(time_1s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting_jct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

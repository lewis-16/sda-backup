{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spike_sorting_utils.basic_util import *\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from probeinterface import write_prb, read_prb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "class Spike_Detection_MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(Spike_Detection_MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 61 * 30)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class Spike_Classification_MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Spike_Classification_MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 61 * 30)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_detection_model = torch.load(\"/home/ubuntu/Documents/jct/project/code/Spike_Sorting/spike_detection/new_model/spike_detection_model_1.pth\")\n",
    "spike_classification_model = torch.load(\"/home/ubuntu/Documents/jct/project/code/Spike_Sorting/spike_classification/new_model/spike_classification_model_1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_raw = se.read_blackrock(file_path='/media/ubuntu/sda/data/mouse6/ns4/natural_image/mouse6_022522_natural_image_001.ns4')\n",
    "recording_recorded = recording_raw.remove_channels(['98', '31', '32'])\n",
    "recording_stimulated = recording_raw.channel_slice(['98'])\n",
    "\n",
    "recording_f_22522 = spre.bandpass_filter(recording_recorded, freq_min=300, freq_max=3000)\n",
    "recording_f_22522 = spre.common_reference(recording_f_22522, reference=\"global\", operator=\"median\")\n",
    "\n",
    "data_22522 = recording_f_22522.get_traces().astype(\"float32\").T\n",
    "\n",
    "threshold_result_22522 = detect_local_maxima_in_window(data_22522)\n",
    "threshold_result_22522 = np.array(threshold_result_22522)\n",
    "valid_indices_22522 = threshold_result_22522[(threshold_result_22522 > 30)]\n",
    "valid_indices_22522 = valid_indices_22522[valid_indices_22522 < data_22522.shape[1] - 31]\n",
    "\n",
    "potent_spike_inf = pd.DataFrame(valid_indices_22522, columns= ['time'])\n",
    "data_input = extract_windows(data_22522, valid_indices_22522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpikeDataset(data_input)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spike_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "device = 'cuda'\n",
    "spike_detection_model = spike_detection_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in val_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        outputs = spike_detection_model(batch_data)\n",
    "        predicted = (outputs > 0.5).float()  \n",
    "\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        \n",
    "predicted_labels = np.array(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "potent_spike_inf['spike_detection_label'] = predicted_labels\n",
    "potent_spike_inf = potent_spike_inf[potent_spike_inf['spike_detection_label'] == 1]\n",
    "\n",
    "potent_spikes = np.where(predicted_labels == 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = data_input[potent_spikes, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpikeDataset(data_input)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_value = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in val_loader:\n",
    "\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_data = batch_data.reshape(-1, 61 * 30)\n",
    "        batch_data = spike_classification_model.fc1(batch_data)\n",
    "        batch_data = spike_classification_model.relu1(batch_data)\n",
    "        batch_data = spike_classification_model.fc2(batch_data)\n",
    "        batch_data = spike_classification_model.relu2(batch_data)\n",
    "        latent_value.append(batch_data.cpu())  \n",
    "        \n",
    "latent_value = torch.cat(latent_value, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_value_subset = latent_value[random.sample(range(len(latent_value)), 10000), :]\n",
    "final_kmeans = KMeans(n_clusters=50, n_init=10, random_state=42).fit(latent_value_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/Documents/jct/project/code/Spike_Sorting/sorting_results/021322/neuron_inf.pkl', 'rb') as f:\n",
    "    neuron_inf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_indices = {\n",
    "        \"1\": [1, 3, 5, 7, 9, 11],\n",
    "        \"2\": [13, 15, 17, 19, 21, 23],\n",
    "        \"3\": [24, 25, 26, 27, 28, 29],\n",
    "        \"4\": [12, 14, 16, 18, 20, 22],\n",
    "        \"5\": [0, 2, 4, 6, 8, 10]\n",
    "        }\n",
    "channel_position = {\n",
    "    0: [650, 0],\n",
    "    2: [650, 50],\n",
    "    4: [650, 100],\n",
    "    6: [600, 100],\n",
    "    8: [600, 50],\n",
    "    10: [600, 0],\n",
    "    1: [0, 0],\n",
    "    3: [0, 50],\n",
    "    5: [0, 100],\n",
    "    7: [50, 100],\n",
    "    9: [50, 50],\n",
    "    11: [50, 0],\n",
    "    13: [150, 200], \n",
    "    15: [150, 250],\n",
    "    17: [150, 300],\n",
    "    19: [200, 300],\n",
    "    21: [200, 250],\n",
    "    23: [200, 200],\n",
    "    12: [500, 200],\n",
    "    14: [500, 250],\n",
    "    16: [500, 300],\n",
    "    18: [450, 300],\n",
    "    20: [450, 250],\n",
    "    22: [450, 200],\n",
    "    24: [350, 400],\n",
    "    26: [350, 450],\n",
    "    28: [350, 500],\n",
    "    25: [300, 400],\n",
    "    27: [300, 450],\n",
    "    29: [300, 500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_new(latent_value, final_kmeans)\n",
    "\n",
    "potent_spike_inf['cluster_predicted'] = predicted_labels\n",
    "potent_spike_inf.index = range(len(potent_spike_inf))\n",
    "\n",
    "cluster_averages = compute_cluster_average(data_input, potent_spike_inf)\n",
    "processed_averages = process_cluster_averages(cluster_averages, channel_indices)\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"cluster\": key.split(\"_\")[0], \"probe_group\": key.split(\"_\")[1], \"waveform\": value.T}\n",
    "    for key, value in processed_averages.items()\n",
    "])\n",
    "\n",
    "df[['position_1', 'position_2']] = df.apply(\n",
    "    lambda row: calculate_position(row, channel_indices, channel_position), axis=1\n",
    ")\n",
    "df['position_waveform'] = df.apply(\n",
    "    lambda row: calculate_position_waveform(row, channel_position, channel_indices), axis=1\n",
    ")\n",
    "\n",
    "df['label'] = 1\n",
    "df['label'] = df.apply(\n",
    "    lambda row: judge_cluster_reality(row, neuron_inf), axis=1\n",
    ")\n",
    "\n",
    "df = df[~df['label'].isna()]\n",
    "df['cluster'] = df['cluster'].astype(int)\n",
    "\n",
    "\n",
    "potent_spike_inf['label'] = '-1'\n",
    "\n",
    "for i, row in potent_spike_inf.iterrows():\n",
    "    df_temp = df[df['cluster'] == row['cluster_predicted']]\n",
    "    \n",
    "    if not df_temp.empty:\n",
    "        potent_spike_inf.loc[i, 'label'] = df_temp['label'].values[0] \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting_jct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Process Sorted Data and Generate Raster Plots\n",
        "\n",
        "This notebook processes sorted spike sorting data from the `sorted` folder and generates raster plots for each neuron.\n",
        "\n",
        "## Features:\n",
        "- Load cluster information and spike data from phy_folder_for_kilosort\n",
        "- Generate cluster_inf and spike_inf DataFrames\n",
        "- Create raster plots for each neuron (20s per PDF page)\n",
        "- Export results as CSV files and PDF plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import glob\n",
        "from typing import List, Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set matplotlib parameters for better plots\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.linewidth'] = 1.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_cluster_info(phy_dir: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load cluster information from cluster_info.tsv file.\n",
        "    \n",
        "    Args:\n",
        "        phy_dir: Path to phy_folder_for_kilosort directory\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with cluster information\n",
        "    \"\"\"\n",
        "    cluster_info_path = os.path.join(phy_dir, 'cluster_info.tsv')\n",
        "    if not os.path.exists(cluster_info_path):\n",
        "        raise FileNotFoundError(f\"Cluster info file not found: {cluster_info_path}\")\n",
        "    \n",
        "    df = pd.read_csv(cluster_info_path, sep='\\t')\n",
        "    \n",
        "    # Ensure cluster_id column exists\n",
        "    if 'cluster_id' not in df.columns:\n",
        "        raise ValueError(f\"cluster_id column not found in {cluster_info_path}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def load_spike_data(phy_dir: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load spike-level data from spike_clusters.npy and spike_times.npy files.\n",
        "    \n",
        "    Args:\n",
        "        phy_dir: Path to phy_folder_for_kilosort directory\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with spike data (cluster_id, time)\n",
        "    \"\"\"\n",
        "    spike_clusters_path = os.path.join(phy_dir, 'spike_clusters.npy')\n",
        "    spike_times_path = os.path.join(phy_dir, 'spike_times.npy')\n",
        "    \n",
        "    if not os.path.exists(spike_clusters_path):\n",
        "        raise FileNotFoundError(f\"Spike clusters file not found: {spike_clusters_path}\")\n",
        "    if not os.path.exists(spike_times_path):\n",
        "        raise FileNotFoundError(f\"Spike times file not found: {spike_times_path}\")\n",
        "    \n",
        "    # Load numpy arrays\n",
        "    spike_clusters = np.load(spike_clusters_path)\n",
        "    spike_times = np.load(spike_times_path)\n",
        "    \n",
        "    # Flatten arrays\n",
        "    spike_clusters = spike_clusters.flatten()\n",
        "    spike_times = spike_times.flatten()\n",
        "    \n",
        "    if len(spike_clusters) != len(spike_times):\n",
        "        raise ValueError(f\"Mismatch in spike_clusters and spike_times lengths: {len(spike_clusters)} vs {len(spike_times)}\")\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'cluster_id': spike_clusters.astype(int),\n",
        "        'time': spike_times.astype(int)\n",
        "    })\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def get_sample_rate(phy_dir: str) -> float:\n",
        "    \"\"\"\n",
        "    Get sample rate from params.py file.\n",
        "    \n",
        "    Args:\n",
        "        phy_dir: Path to phy_folder_for_kilosort directory\n",
        "        \n",
        "    Returns:\n",
        "        Sample rate in Hz\n",
        "    \"\"\"\n",
        "    params_path = os.path.join(phy_dir, 'params.py')\n",
        "    if not os.path.exists(params_path):\n",
        "        print(f\"Warning: params.py not found in {phy_dir}, using default sample rate 20000 Hz\")\n",
        "        return 20000.0\n",
        "    \n",
        "    with open(params_path, 'r') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    # Extract sample_rate from params.py\n",
        "    import re\n",
        "    match = re.search(r'sample_rate\\s*=\\s*([0-9.]+)', content)\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    else:\n",
        "        print(f\"Warning: Could not extract sample_rate from {params_path}, using default 20000 Hz\")\n",
        "        return 20000.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Processing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_single_session(session_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
        "    \"\"\"\n",
        "    Process a single session directory and return cluster_inf, spike_inf, and metadata.\n",
        "    \n",
        "    Args:\n",
        "        session_dir: Path to session directory (e.g., 20250909_Janus_1_250909_144332)\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (cluster_inf, spike_inf, metadata)\n",
        "    \"\"\"\n",
        "    phy_dir = os.path.join(session_dir, 'phy_folder_for_kilosort')\n",
        "    \n",
        "    if not os.path.exists(phy_dir):\n",
        "        raise FileNotFoundError(f\"phy_folder_for_kilosort not found in {session_dir}\")\n",
        "    \n",
        "    # Load data\n",
        "    cluster_inf = load_cluster_info(phy_dir)\n",
        "    spike_inf = load_spike_data(phy_dir)\n",
        "    sample_rate = get_sample_rate(phy_dir)\n",
        "    \n",
        "    # Extract session metadata from directory name\n",
        "    session_name = os.path.basename(session_dir)\n",
        "    \n",
        "    # Add metadata to cluster_inf\n",
        "    cluster_inf['session'] = session_name\n",
        "    cluster_inf['sample_rate'] = sample_rate\n",
        "    cluster_inf = cluster_inf[cluster_inf['group'] == 'good']\n",
        "    \n",
        "    # Add metadata to spike_inf\n",
        "    spike_inf['session'] = session_name\n",
        "    spike_inf['sample_rate'] = sample_rate\n",
        "    \n",
        "    # Convert spike times to seconds\n",
        "    spike_inf['time_seconds'] = spike_inf['time'] / sample_rate\n",
        "    \n",
        "    metadata = {\n",
        "        'session': session_name,\n",
        "        'sample_rate': sample_rate,\n",
        "        'total_spikes': len(spike_inf),\n",
        "        'total_clusters': len(cluster_inf),\n",
        "        'recording_duration': spike_inf['time_seconds'].max() if len(spike_inf) > 0 else 0\n",
        "    }\n",
        "    \n",
        "    return cluster_inf, spike_inf, metadata\n",
        "\n",
        "\n",
        "def process_all_sessions(sorted_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Process all sessions in the sorted directory.\n",
        "    \n",
        "    Args:\n",
        "        sorted_dir: Path to sorted directory\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (all_cluster_inf, all_spike_inf, all_metadata)\n",
        "    \"\"\"\n",
        "    all_cluster_inf = []\n",
        "    all_spike_inf = []\n",
        "    all_metadata = []\n",
        "    \n",
        "    # Get all session directories\n",
        "    session_dirs = [d for d in os.listdir(sorted_dir) \n",
        "                   if os.path.isdir(os.path.join(sorted_dir, d))]\n",
        "    session_dirs.sort()\n",
        "    \n",
        "    print(f\"Found {len(session_dirs)} sessions to process:\")\n",
        "    for session_dir in session_dirs:\n",
        "        print(f\"  - {session_dir}\")\n",
        "    \n",
        "    for session_dir in session_dirs:\n",
        "        session_path = os.path.join(sorted_dir, session_dir)\n",
        "        \n",
        "        try:\n",
        "            print(f\"\\nProcessing {session_dir}...\")\n",
        "            cluster_inf, spike_inf, metadata = process_single_session(session_path)\n",
        "            \n",
        "            all_cluster_inf.append(cluster_inf)\n",
        "            all_spike_inf.append(spike_inf)\n",
        "            all_metadata.append(metadata)\n",
        "            \n",
        "            print(f\"  ✓ Loaded {metadata['total_clusters']} clusters, {metadata['total_spikes']} spikes\")\n",
        "            print(f\"  ✓ Recording duration: {metadata['recording_duration']:.2f} seconds\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error processing {session_dir}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if len(all_cluster_inf) == 0:\n",
        "        raise RuntimeError(\"No sessions were successfully processed\")\n",
        "    \n",
        "    # Concatenate all data\n",
        "    final_cluster_inf = pd.concat(all_cluster_inf, ignore_index=True)\n",
        "    final_spike_inf = pd.concat(all_spike_inf, ignore_index=True)\n",
        "    \n",
        "    print(f\"\\n=== Summary ===\")\n",
        "    print(f\"Total sessions processed: {len(all_metadata)}\")\n",
        "    print(f\"Total clusters: {len(final_cluster_inf)}\")\n",
        "    print(f\"Total spikes: {len(final_spike_inf)}\")\n",
        "    \n",
        "    return final_cluster_inf, final_spike_inf, all_metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Raster Plot Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_raster_plot(spike_inf: pd.DataFrame, cluster_inf: pd.DataFrame, \n",
        "                      session_name: str, output_path: str, \n",
        "                      time_window: float = 50.0, figsize: Tuple[float, float] = (15, 10)):\n",
        "    \"\"\"\n",
        "    Create a multi-page raster plot for all clusters in a session.\n",
        "    Each page shows a time window (default 50s), each cluster occupies one row.\n",
        "    \n",
        "    Args:\n",
        "        spike_inf: DataFrame with spike data\n",
        "        cluster_inf: DataFrame with cluster information\n",
        "        session_name: Name of the session\n",
        "        output_path: Path for output PDF file\n",
        "        time_window: Time window in seconds for each page\n",
        "        figsize: Figure size\n",
        "    \"\"\"\n",
        "    # Filter data for this session\n",
        "    session_spikes = spike_inf[spike_inf['session'] == session_name]\n",
        "    session_clusters = cluster_inf[cluster_inf['session'] == session_name]\n",
        "    \n",
        "    if len(session_spikes) == 0:\n",
        "        print(f\"No spike data found for session {session_name}\")\n",
        "        return\n",
        "    \n",
        "    # Get unique cluster IDs and sort them\n",
        "    cluster_ids = sorted(session_clusters['cluster_id'].unique())\n",
        "    \n",
        "    # Get time range for the entire session\n",
        "    min_time = session_spikes['time_seconds'].min()\n",
        "    max_time = session_spikes['time_seconds'].max()\n",
        "    total_duration = max_time - min_time\n",
        "    \n",
        "    # Calculate number of pages needed\n",
        "    num_pages = int(np.ceil(total_duration / time_window))\n",
        "    \n",
        "    print(f\"Creating raster plot for {len(cluster_ids)} clusters in session {session_name}...\")\n",
        "    print(f\"Total duration: {total_duration:.1f}s, {num_pages} pages (50s per page)\")\n",
        "    \n",
        "    with PdfPages(output_path) as pdf:\n",
        "        for page in range(num_pages):\n",
        "            # Calculate time window for this page\n",
        "            page_start = min_time + page * time_window\n",
        "            page_end = min(page_start + time_window, max_time)\n",
        "            \n",
        "            # Filter spikes for this time window\n",
        "            page_spikes = session_spikes[\n",
        "                (session_spikes['time_seconds'] >= page_start) & \n",
        "                (session_spikes['time_seconds'] < page_end)\n",
        "            ]\n",
        "            \n",
        "            # Create figure with white background\n",
        "            fig, ax = plt.subplots(figsize=figsize)\n",
        "            ax.set_facecolor('white')\n",
        "            fig.patch.set_facecolor('white')\n",
        "            \n",
        "            # Set up the plot\n",
        "            ax.set_xlim(page_start, page_end)\n",
        "            ax.set_ylim(-0.5, len(cluster_ids) - 0.5)\n",
        "            \n",
        "            # Plot spikes for each cluster\n",
        "            for i, cluster_id in enumerate(cluster_ids):\n",
        "                cluster_spikes = page_spikes[page_spikes['cluster_id'] == cluster_id]\n",
        "                \n",
        "                if len(cluster_spikes) == 0:\n",
        "                    continue\n",
        "                \n",
        "                # Plot spikes as vertical lines\n",
        "                spike_times = cluster_spikes['time_seconds'].values\n",
        "                for spike_time in spike_times:\n",
        "                    ax.axvline(x=spike_time, ymin=i-0.4, ymax=i+0.4, color='black', linewidth=0.3, alpha=0.8)\n",
        "                \n",
        "                # Add cluster label on the left\n",
        "                firing_rate = len(spike_times) / time_window if time_window > 0 else 0\n",
        "                label_text = f\"Cluster {cluster_id}\\n({len(spike_times)} spikes)\"\n",
        "                \n",
        "                ax.text(page_start - (page_end - page_start) * 0.05, i, label_text, \n",
        "                        verticalalignment='center', horizontalalignment='right',\n",
        "                        fontsize=8, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgray', alpha=0.7))\n",
        "            \n",
        "            # Set labels and title\n",
        "            ax.set_xlabel('Time (seconds)', fontsize=12)\n",
        "            ax.set_ylabel('Clusters', fontsize=12)\n",
        "            ax.set_title(f'Session: {session_name} | Page {page+1}/{num_pages} | {page_start:.1f}-{page_end:.1f}s', \n",
        "                        fontsize=14, fontweight='bold')\n",
        "            \n",
        "            # Set y-axis ticks\n",
        "            ax.set_yticks(range(len(cluster_ids)))\n",
        "            ax.set_yticklabels([f'C{cid}' for cid in cluster_ids])\n",
        "            \n",
        "            # Add grid\n",
        "            ax.grid(True, alpha=0.3, axis='x')\n",
        "            \n",
        "            # Add page info\n",
        "            page_spikes_count = len(page_spikes)\n",
        "            info_text = f\"Page {page+1}/{num_pages} | Time: {page_start:.1f}-{page_end:.1f}s | Spikes: {page_spikes_count}\"\n",
        "            ax.text(0.5, 1.02, info_text, transform=ax.transAxes, \n",
        "                    horizontalalignment='center', fontsize=10,\n",
        "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            \n",
        "            # Save page to PDF\n",
        "            pdf.savefig(fig, bbox_inches='tight', facecolor='white')\n",
        "            plt.close(fig)\n",
        "            \n",
        "            print(f\"  ✓ Added page {page+1}/{num_pages} ({page_spikes_count} spikes)\")\n",
        "    \n",
        "    print(f\"Raster plot saved to: {output_path}\")\n",
        "\n",
        "\n",
        "def create_all_raster_pdfs(cluster_inf: pd.DataFrame, spike_inf: pd.DataFrame, \n",
        "                          output_dir: str, time_window: float = 50.0):\n",
        "    \"\"\"\n",
        "    Create raster plot PDFs for all sessions.\n",
        "    Each PDF contains multiple pages, each page shows a time window.\n",
        "    \n",
        "    Args:\n",
        "        cluster_inf: DataFrame with cluster information\n",
        "        spike_inf: DataFrame with spike data\n",
        "        output_dir: Directory to save PDF files\n",
        "        time_window: Time window in seconds for each page\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Get unique sessions\n",
        "    sessions = sorted(cluster_inf['session'].unique())\n",
        "    \n",
        "    print(f\"Creating raster plots for {len(sessions)} sessions...\")\n",
        "    \n",
        "    for session in sessions:\n",
        "        output_path = os.path.join(output_dir, f\"raster_plot_{session}.pdf\")\n",
        "        create_raster_plot(spike_inf, cluster_inf, session, output_path, time_window)\n",
        "    \n",
        "    print(f\"\\nAll raster plots saved to: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Processing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Processing Sorted Data ===\n",
            "Sorted directory: /media/ubuntu/sda/mouse_test/sorted\n",
            "Output directory: /media/ubuntu/sda/mouse_test/processed_results\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set paths\n",
        "sorted_dir = \"/media/ubuntu/sda/mouse_test/sorted\"\n",
        "output_dir = \"/media/ubuntu/sda/mouse_test/processed_results\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"=== Processing Sorted Data ===\")\n",
        "print(f\"Sorted directory: {sorted_dir}\")\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5 sessions to process:\n",
            "  - 20250909_Janus_1_250909_144332\n",
            "  - 20250909_Janus_2_250909_145527\n",
            "  - 20250909_Janus_3_250909_151329\n",
            "  - 20250910_Janus2_1_250910_110238\n",
            "  - 20250910_Janus2_2_250910_112659\n",
            "\n",
            "Processing 20250909_Janus_1_250909_144332...\n",
            "  ✓ Loaded 15 clusters, 274041 spikes\n",
            "  ✓ Recording duration: 599.38 seconds\n",
            "\n",
            "Processing 20250909_Janus_2_250909_145527...\n",
            "  ✓ Loaded 17 clusters, 499201 spikes\n",
            "  ✓ Recording duration: 944.69 seconds\n",
            "\n",
            "Processing 20250909_Janus_3_250909_151329...\n",
            "  ✓ Loaded 5 clusters, 112999 spikes\n",
            "  ✓ Recording duration: 298.93 seconds\n",
            "\n",
            "Processing 20250910_Janus2_1_250910_110238...\n",
            "  ✓ Loaded 13 clusters, 713816 spikes\n",
            "  ✓ Recording duration: 1370.29 seconds\n",
            "\n",
            "Processing 20250910_Janus2_2_250910_112659...\n",
            "  ✓ Loaded 5 clusters, 143147 spikes\n",
            "  ✓ Recording duration: 824.95 seconds\n",
            "\n",
            "=== Summary ===\n",
            "Total sessions processed: 5\n",
            "Total clusters: 55\n",
            "Total spikes: 1743204\n"
          ]
        }
      ],
      "source": [
        "# Process all sessions\n",
        "cluster_inf, spike_inf, metadata = process_all_sessions(sorted_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Session Summary ===\n",
            "Session: 20250909_Janus_1_250909_144332\n",
            "  Clusters: 15\n",
            "  Spikes: 274,041\n",
            "  Duration: 599.38 seconds\n",
            "  Sample Rate: 20000.0 Hz\n",
            "\n",
            "Session: 20250909_Janus_2_250909_145527\n",
            "  Clusters: 17\n",
            "  Spikes: 499,201\n",
            "  Duration: 944.69 seconds\n",
            "  Sample Rate: 20000.0 Hz\n",
            "\n",
            "Session: 20250909_Janus_3_250909_151329\n",
            "  Clusters: 5\n",
            "  Spikes: 112,999\n",
            "  Duration: 298.93 seconds\n",
            "  Sample Rate: 20000.0 Hz\n",
            "\n",
            "Session: 20250910_Janus2_1_250910_110238\n",
            "  Clusters: 13\n",
            "  Spikes: 713,816\n",
            "  Duration: 1370.29 seconds\n",
            "  Sample Rate: 20000.0 Hz\n",
            "\n",
            "Session: 20250910_Janus2_2_250910_112659\n",
            "  Clusters: 5\n",
            "  Spikes: 143,147\n",
            "  Duration: 824.95 seconds\n",
            "  Sample Rate: 20000.0 Hz\n",
            "\n",
            "\n",
            "=== Cluster Information Summary ===\n",
            "Total clusters across all sessions: 55\n",
            "Clusters by group:\n",
            "  good: 55\n",
            "\n",
            "Clusters by session:\n",
            "  20250909_Janus_2_250909_145527: 17\n",
            "  20250909_Janus_1_250909_144332: 15\n",
            "  20250910_Janus2_1_250910_110238: 13\n",
            "  20250909_Janus_3_250909_151329: 5\n",
            "  20250910_Janus2_2_250910_112659: 5\n"
          ]
        }
      ],
      "source": [
        "# Display summary information\n",
        "print(\"\\n=== Session Summary ===\")\n",
        "for meta in metadata:\n",
        "    print(f\"Session: {meta['session']}\")\n",
        "    print(f\"  Clusters: {meta['total_clusters']}\")\n",
        "    print(f\"  Spikes: {meta['total_spikes']:,}\")\n",
        "    print(f\"  Duration: {meta['recording_duration']:.2f} seconds\")\n",
        "    print(f\"  Sample Rate: {meta['sample_rate']} Hz\")\n",
        "    print()\n",
        "\n",
        "# Display cluster information summary\n",
        "print(\"\\n=== Cluster Information Summary ===\")\n",
        "print(f\"Total clusters across all sessions: {len(cluster_inf)}\")\n",
        "print(f\"Clusters by group:\")\n",
        "if 'group' in cluster_inf.columns:\n",
        "    group_counts = cluster_inf['group'].value_counts()\n",
        "    for group, count in group_counts.items():\n",
        "        print(f\"  {group}: {count}\")\n",
        "\n",
        "print(f\"\\nClusters by session:\")\n",
        "session_counts = cluster_inf['session'].value_counts()\n",
        "for session, count in session_counts.items():\n",
        "    print(f\"  {session}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Saved Data Files ===\n",
            "Cluster information: /media/ubuntu/sda/mouse_test/processed_results/cluster_inf_all_sessions.csv\n",
            "Spike information: /media/ubuntu/sda/mouse_test/processed_results/spike_inf_all_sessions.csv\n",
            "\n",
            "Cluster info shape: (55, 31)\n",
            "Spike info shape: (1743204, 5)\n"
          ]
        }
      ],
      "source": [
        "# Save cluster_inf and spike_inf as CSV files\n",
        "cluster_csv_path = os.path.join(output_dir, \"cluster_inf_all_sessions.csv\")\n",
        "spike_csv_path = os.path.join(output_dir, \"spike_inf_all_sessions.csv\")\n",
        "\n",
        "cluster_inf.to_csv(cluster_csv_path, index=False)\n",
        "spike_inf.to_csv(spike_csv_path, index=False)\n",
        "\n",
        "print(f\"\\n=== Saved Data Files ===\")\n",
        "print(f\"Cluster information: {cluster_csv_path}\")\n",
        "print(f\"Spike information: {spike_csv_path}\")\n",
        "print(f\"\\nCluster info shape: {cluster_inf.shape}\")\n",
        "print(f\"Spike info shape: {spike_inf.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "spike_inf_temp = spike_inf[spike_inf['session'] == '20250909_Janus_1_250909_144332']\n",
        "trigger_time = np.linspace(0, spike_inf_temp['time_seconds'].max(), 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_inf_temp = cluster_inf[cluster_inf['session'] == '20250909_Janus_1_250909_144332']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cluster_id</th>\n",
              "      <th>time</th>\n",
              "      <th>session</th>\n",
              "      <th>sample_rate</th>\n",
              "      <th>time_seconds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>49</td>\n",
              "      <td>35</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.00175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49</td>\n",
              "      <td>202</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.01010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17</td>\n",
              "      <td>270</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.01350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>22</td>\n",
              "      <td>290</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.01450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>49</td>\n",
              "      <td>303</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>0.01515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274036</th>\n",
              "      <td>17</td>\n",
              "      <td>11987149</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>599.35745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274037</th>\n",
              "      <td>8</td>\n",
              "      <td>11987196</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>599.35980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274038</th>\n",
              "      <td>7</td>\n",
              "      <td>11987301</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>599.36505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274039</th>\n",
              "      <td>49</td>\n",
              "      <td>11987431</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>599.37155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274040</th>\n",
              "      <td>7</td>\n",
              "      <td>11987568</td>\n",
              "      <td>20250909_Janus_1_250909_144332</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>599.37840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191365 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        cluster_id      time                         session  sample_rate  \\\n",
              "0               49        35  20250909_Janus_1_250909_144332      20000.0   \n",
              "2               49       202  20250909_Janus_1_250909_144332      20000.0   \n",
              "4               17       270  20250909_Janus_1_250909_144332      20000.0   \n",
              "5               22       290  20250909_Janus_1_250909_144332      20000.0   \n",
              "7               49       303  20250909_Janus_1_250909_144332      20000.0   \n",
              "...            ...       ...                             ...          ...   \n",
              "274036          17  11987149  20250909_Janus_1_250909_144332      20000.0   \n",
              "274037           8  11987196  20250909_Janus_1_250909_144332      20000.0   \n",
              "274038           7  11987301  20250909_Janus_1_250909_144332      20000.0   \n",
              "274039          49  11987431  20250909_Janus_1_250909_144332      20000.0   \n",
              "274040           7  11987568  20250909_Janus_1_250909_144332      20000.0   \n",
              "\n",
              "        time_seconds  \n",
              "0            0.00175  \n",
              "2            0.01010  \n",
              "4            0.01350  \n",
              "5            0.01450  \n",
              "7            0.01515  \n",
              "...              ...  \n",
              "274036     599.35745  \n",
              "274037     599.35980  \n",
              "274038     599.36505  \n",
              "274039     599.37155  \n",
              "274040     599.37840  \n",
              "\n",
              "[191365 rows x 5 columns]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spike_inf_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raster plot saved successfully!\n",
            "Raster plot saved successfully!\n",
            "Raster plot saved successfully!\n",
            "Raster plot saved successfully!\n",
            "Raster plot saved successfully!\n"
          ]
        }
      ],
      "source": [
        "for session in cluster_inf['session'].unique():\n",
        "    time_bin_duration = 50.0  # seconds\n",
        "    cluster_inf_temp = cluster_inf[cluster_inf['session'] == session]\n",
        "\n",
        "    spike_inf_temp = spike_inf[spike_inf['session'] == session]\n",
        "    spike_inf_temp = spike_inf_temp[spike_inf_temp['cluster_id'].isin(cluster_inf_temp['cluster_id'].unique())]\n",
        "    # Get all unique clusters\n",
        "    clusters = sorted(spike_inf_temp['cluster_id'].unique())\n",
        "\n",
        "    with PdfPages(f\"/media/ubuntu/sda/mouse_test/processed_results/raster_overall_{session}.pdf\") as pdf:\n",
        "        for bin_idx in range(int(spike_inf_temp['time_seconds'].max() // time_bin_duration) + 1):\n",
        "            # Calculate time window for this bin\n",
        "            bin_start_time = bin_idx * time_bin_duration * 20000  # Convert to samples\n",
        "            bin_end_time = min(bin_start_time + time_bin_duration * 20000, spike_inf_temp['time'].max())\n",
        "            \n",
        "            bin_start_sec = bin_start_time / 20000.0\n",
        "            bin_end_sec = bin_end_time / 20000.0\n",
        "            \n",
        "            fig, ax = plt.subplots(figsize=(12, 7.5))\n",
        "            ax.set_facecolor('white')\n",
        "            fig.patch.set_facecolor('white')\n",
        "            \n",
        "            ax.set_xlim(0, time_bin_duration)\n",
        "            \n",
        "            # 设置y轴范围，确保每个cluster有足够的空间\n",
        "            ax.set_ylim(-1, len(clusters))\n",
        "            \n",
        "            for i, cluster_id in enumerate(clusters):\n",
        "                cluster_spikes = spike_inf_temp[\n",
        "                    (spike_inf_temp['cluster_id'] == cluster_id) & \n",
        "                    (spike_inf_temp['time'] >= bin_start_time) & \n",
        "                    (spike_inf_temp['time'] < bin_end_time)\n",
        "                ]\n",
        "                \n",
        "                if not cluster_spikes.empty:\n",
        "                    relative_times = (cluster_spikes['time'] - bin_start_time) / 20000.0\n",
        "                    \n",
        "                    # 为每个cluster的spike绘制在不同高度的水平线上\n",
        "                    y_position = i  # 每个cluster占据一个唯一的y位置\n",
        "                    \n",
        "                    # 使用eventplot而不是多个axvline，提高效率\n",
        "                    ax.eventplot(relative_times, \n",
        "                            lineoffsets=y_position, \n",
        "                            linelengths=0.8,  # 控制线条高度\n",
        "                            linewidths=0.5, \n",
        "                            colors='black', \n",
        "                            alpha=0.8)\n",
        "            \n",
        "            # 设置y轴标签\n",
        "            ax.set_yticks(range(len(clusters)))\n",
        "            ax.set_yticklabels([f'C{cid}' for cid in clusters])\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            pdf.savefig(fig, bbox_inches='tight', facecolor='white')\n",
        "            plt.close(fig)\n",
        "            \n",
        "    print(\"Raster plot saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_mean_spike_rate_data = {}\n",
        "gk = GaussianKernel(25 * ms)\n",
        "\n",
        "total_duration = 20000  \n",
        "\n",
        "for image in range(1, 118):\n",
        "    image_dict = {}\n",
        "    \n",
        "    for date in date_order:\n",
        "        neuron_data = []\n",
        "        \n",
        "        for neuron in spike_inf['Neuron'].unique():\n",
        "            neuron_df = spike_inf[spike_inf['Neuron'] == neuron]\n",
        "            trigger_time_temp = trigger_time[(trigger_time['image'] == image) \n",
        "                                            & (trigger_time['date'] == int(date))]\n",
        "            \n",
        "            trial_rates = [] \n",
        "            \n",
        "            for _, row in trigger_time_temp.iterrows():\n",
        "                start = row['start'] - 5000\n",
        "                end = row['end'] + 10000\n",
        "                \n",
        "                filtered_spikes = neuron_df[(neuron_df['date'] == date) \n",
        "                                          & (neuron_df['time'] >= start)\n",
        "                                          & (neuron_df['time'] <= end)]\n",
        "                \n",
        "                relative_spikes = filtered_spikes['time'] - start\n",
        "                relative_spikes = relative_spikes.values / 10\n",
        "                temp_spiketrain = neo.SpikeTrain(relative_spikes.astype(int) * ms, t_stop=2000, t_start=0)\n",
        "                inst_rate = instantaneous_rate(temp_spiketrain, kernel=gk, sampling_period=10*ms).magnitude\n",
        "                trial_rates.append(inst_rate)\n",
        "            \n",
        "            if trial_rates:\n",
        "                mean_rate = np.mean(trial_rates, axis=0)\n",
        "            \n",
        "            neuron_data.append(mean_rate)\n",
        "        \n",
        "        neuron_data = np.stack(neuron_data)\n",
        "        image_dict[date] = neuron_data\n",
        "    \n",
        "    image_mean_spike_rate_data[image] = image_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.565000e+01, 3.650000e+01, 1.240500e+02, ..., 4.988320e+04,\n",
              "       4.998650e+04, 4.999165e+04], shape=(1294,))"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relative_times_sec.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Firing rate plots saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import neo\n",
        "from quantities import ms\n",
        "from elephant.statistics import instantaneous_rate\n",
        "from elephant.kernels import GaussianKernel\n",
        "import numpy as np\n",
        "from quantities import sec  # 添加sec单位\n",
        "\n",
        "for session in cluster_inf['session'].unique():\n",
        "    gk = GaussianKernel(2000 * ms)\n",
        "\n",
        "    time_bin_duration = 50.0  # seconds\n",
        "    cluster_inf_temp = cluster_inf[cluster_inf['session'] == session]\n",
        "\n",
        "    spike_inf_temp = spike_inf[spike_inf['session'] == session]\n",
        "    spike_inf_temp = spike_inf_temp[spike_inf_temp['cluster_id'].isin(cluster_inf_temp['cluster_id'].unique())]\n",
        "    # Get all unique clusters\n",
        "    clusters = sorted(spike_inf_temp['cluster_id'].unique())\n",
        "\n",
        "    with PdfPages(f\"/media/ubuntu/sda/mouse_test/processed_results/firing_rate_{session}.pdf\") as pdf:\n",
        "        for bin_idx in range(int(spike_inf_temp['time_seconds'].max() // time_bin_duration) + 1):\n",
        "            # Calculate time window for this bin\n",
        "            bin_start_time = bin_idx * time_bin_duration * 20000  # Convert to samples\n",
        "            bin_end_time = min(bin_start_time + time_bin_duration * 20000, spike_inf_temp['time'].max())\n",
        "            \n",
        "            bin_start_sec = bin_start_time / 20000.0\n",
        "            bin_end_sec = bin_end_time / 20000.0\n",
        "            \n",
        "            fig, ax = plt.subplots(figsize=(12, 7.5))\n",
        "            ax.set_facecolor('white')\n",
        "            fig.patch.set_facecolor('white')\n",
        "            \n",
        "            ax.set_xlim(0, time_bin_duration)\n",
        "            \n",
        "            \n",
        "            for i, cluster_id in enumerate(clusters):\n",
        "                cluster_spikes = spike_inf_temp[\n",
        "                    (spike_inf_temp['cluster_id'] == cluster_id) & \n",
        "                    (spike_inf_temp['time'] >= bin_start_time) & \n",
        "                    (spike_inf_temp['time'] < bin_end_time)\n",
        "                ]\n",
        "                \n",
        "                if not cluster_spikes.empty:\n",
        "                    relative_times_sec = (cluster_spikes['time'] - bin_start_time) / 20000\n",
        "                    temp_spiketrain = neo.SpikeTrain(relative_times_sec.values * sec, t_stop=time_bin_duration * sec, t_start=0 * sec)\n",
        "                    inst_rate = instantaneous_rate(temp_spiketrain, kernel=gk, sampling_period=1000*ms)\n",
        "                    \n",
        "                    # 获取时间点和firing rate值\n",
        "                    times = np.linspace(0, time_bin_duration, len(inst_rate))\n",
        "                    rates = inst_rate.magnitude.flatten()\n",
        "                \n",
        "                    ax.plot(times, rates, color='black', linewidth=0.8, alpha=0.8)\n",
        "                    \n",
        "            \n",
        "            # 设置y轴标签\n",
        "            ax.set_ylabel('Firing Rate (Hz)')\n",
        "            ax.set_xlabel('Time (s)')\n",
        "            \n",
        "            # 隐藏y轴刻度，因为实际值已被偏移\n",
        "            ax.set_yticks([])\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            pdf.savefig(fig, bbox_inches='tight', facecolor='white')\n",
        "            plt.close(fig)\n",
        "            \n",
        "print(\"Firing rate plots saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([63.45622917, 63.63365882, 63.57373194, 63.50425547, 63.54025903,\n",
              "       63.64183593, 63.75008083, 63.90017355, 64.19781196, 64.71384416,\n",
              "       65.39737031, 66.04954479, 66.39189852, 66.24291922, 65.66487686,\n",
              "       64.90140023, 64.1891685 , 63.69458293, 63.56510784, 63.85604459,\n",
              "       64.35942027, 64.6639831 , 64.49267384, 63.90533768, 63.14042252,\n",
              "       62.38867951, 61.75685142, 61.30741107, 61.04893821, 60.93253249,\n",
              "       60.89204079, 60.88120215, 60.87896794, 60.87861366, 60.87857016,\n",
              "       60.8785657 , 60.8785657 , 60.8785657 , 60.8785657 , 60.8785657 ,\n",
              "       60.8785657 , 60.8785657 , 60.8785657 , 60.8785657 , 60.8785657 ,\n",
              "       60.8785657 , 60.8785657 , 60.8785657 , 60.8785657 , 60.8785657 ])"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adjusted_rates"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spike_sorting_jct",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

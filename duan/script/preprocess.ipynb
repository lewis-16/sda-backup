{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca6a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spikeinterface as si\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy.stats import  pearsonr\n",
    "import neo\n",
    "from quantities import ms\n",
    "from elephant.statistics import instantaneous_rate\n",
    "from elephant.kernels import GaussianKernel\n",
    "\n",
    "import glob\n",
    "import neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f19b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tsv_files(directory_path):\n",
    "    tsv_files = glob.glob(os.path.join(directory_path, \"*.tsv\"))\n",
    "    \n",
    "    for file in tsv_files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "    # 读取所有tsv文件\n",
    "    dataframes = []\n",
    "    \n",
    "    for file_path in tsv_files:\n",
    "        try:\n",
    "            # 读取tsv文件\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            if len(df) == 555:       \n",
    "                dataframes.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "\n",
    "    \n",
    "    # 检查是否有共同的列（cluster_id）\n",
    "    common_columns = set(dataframes[0].columns)\n",
    "    for df in dataframes[1:]:\n",
    "        common_columns = common_columns.intersection(set(df.columns))\n",
    "    \n",
    "    \n",
    "    # 如果有cluster_id列，使用它作为合并键\n",
    "    if 'cluster_id' in common_columns:\n",
    "        \n",
    "        # 从第一个文件开始\n",
    "        merged_df = dataframes[0].copy()\n",
    "        \n",
    "        # 逐个合并其他文件\n",
    "        for i, df in enumerate(dataframes[1:], 1):\n",
    "            # 使用outer join保留所有cluster_id\n",
    "            merged_df = pd.merge(merged_df, df, on='cluster_id', how='outer', suffixes=('', f'_file{i+1}'))\n",
    "    \n",
    "        \n",
    "        return merged_df\n",
    "    \n",
    "    else:\n",
    "        # 如果没有共同列，直接拼接\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "        \n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97a9fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - cluster_KSLabel.tsv\n",
      "  - cluster_info.tsv\n",
      "  - cluster_ContamPct.tsv\n",
      "  - cluster_group.tsv\n",
      "  - cluster_Amplitude.tsv\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"/media/ubuntu/sda/duan\"\n",
    "\n",
    "cluster_inf = merge_tsv_files(directory_path)\n",
    "cluster_inf = cluster_inf[cluster_inf['KSLabel'] == 'good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33958703",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_clusters = np.load(\"/media/ubuntu/sda/duan/spike_clusters.npy\")\n",
    "spike_times = np.load(\"/media/ubuntu/sda/duan/spike_times.npy\")\n",
    "\n",
    "spike_inf = pd.DataFrame([spike_clusters, spike_times])\n",
    "spike_inf = spike_inf.T\n",
    "spike_inf.columns = ['cluster', 'time']\n",
    "spike_inf = spike_inf[spike_inf['cluster'].isin(cluster_inf['cluster_id'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec176717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import class_types\n",
    "\n",
    "\n",
    "valid_cluster = spike_inf['cluster'].value_counts()\n",
    "valid_cluster = valid_cluster[valid_cluster > 15000].index\n",
    "\n",
    "spike_inf = spike_inf[spike_inf['cluster'].isin(valid_cluster)]\n",
    "cluster_inf = cluster_inf[cluster_inf['cluster_id'].isin(valid_cluster)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb09bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_time = pd.read_csv(\"/media/ubuntu/sda/duan/rec_params.csv\")\n",
    "trigger_time = trigger_time[trigger_time['bhv_codes'] == 10]\n",
    "trigger_time['trial_condition'] = trigger_time['trial_condition'].astype(int)\n",
    "trigger_time['trial_target'] = trigger_time['trial_target'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b42907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spike_train_dict(spike_inf, trigger_time, t_start=0, t_stop=500):\n",
    "    gk = GaussianKernel(50 * ms)\n",
    "\n",
    "    spike_train_dict = {}\n",
    "\n",
    "    # 按trial_condition和trial_target分组\n",
    "    for (trial_condition, trial_target), group in trigger_time.groupby(['trial_condition', 'trial_target']):\n",
    "        key = f\"{trial_condition}_{trial_target}\"\n",
    "        spike_train_dict[key] = []\n",
    "        \n",
    "        print(f\"Processing {key}: {len(group)} trials\")\n",
    "        \n",
    "        # 对每个trial\n",
    "        for _, trial_row in group.iterrows():\n",
    "            rec_codes_point = trial_row['rec_codes_points']\n",
    "            \n",
    "            # 定义时间窗口\n",
    "            start_time = rec_codes_point\n",
    "            end_time = rec_codes_point + t_stop * 30  \n",
    "            \n",
    "            trial_spikes = spike_inf[\n",
    "                (spike_inf['time'] >= start_time) & \n",
    "                (spike_inf['time'] < end_time)\n",
    "            ].copy()\n",
    "                        \n",
    "            trial_spikes['relative_time'] = trial_spikes['time'] - rec_codes_point\n",
    "            \n",
    "            # 为每个cluster创建spike train\n",
    "            trial_spike_trains = []\n",
    "            \n",
    "            for cluster_id in spike_inf['cluster'].unique():\n",
    "                # 获取该cluster的spikes\n",
    "                cluster_spikes = trial_spikes[trial_spikes['cluster'] == cluster_id]['relative_time'].values\n",
    "                \n",
    "                cluster_spikes_ms = cluster_spikes / 30.0  \n",
    "                \n",
    "                valid_spikes = cluster_spikes_ms[(cluster_spikes_ms >= t_start) & (cluster_spikes_ms <= t_stop)]\n",
    "                \n",
    "                spike_train = neo.SpikeTrain(\n",
    "                    valid_spikes.astype(int) * ms, \n",
    "                    t_stop=t_stop * ms, \n",
    "                    t_start=t_start * ms\n",
    "                )\n",
    "\n",
    "                inst_rate = instantaneous_rate(spike_train, kernel=gk, sampling_period=25*ms).magnitude\n",
    "\n",
    "                trial_spike_trains.append(inst_rate)\n",
    "            \n",
    "            spike_train_dict[key].append(trial_spike_trains)\n",
    "    \n",
    "    return spike_train_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54425797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating spike train dictionary...\n",
      "Processing 1_1: 13 trials\n",
      "Processing 1_2: 23 trials\n",
      "Processing 1_3: 20 trials\n",
      "Processing 2_1: 11 trials\n",
      "Processing 2_2: 17 trials\n",
      "Processing 2_3: 19 trials\n",
      "Processing 3_1: 14 trials\n",
      "Processing 3_2: 13 trials\n",
      "Processing 3_3: 19 trials\n",
      "Processing 4_1: 10 trials\n",
      "Processing 4_2: 19 trials\n",
      "Processing 4_3: 15 trials\n",
      "Processing 5_1: 20 trials\n",
      "Processing 5_2: 13 trials\n",
      "Processing 5_3: 21 trials\n",
      "Processing 6_1: 18 trials\n",
      "Processing 6_2: 20 trials\n",
      "Processing 6_3: 25 trials\n",
      "Processing 7_1: 10 trials\n",
      "Processing 7_2: 10 trials\n",
      "Processing 7_3: 32 trials\n",
      "Processing 8_1: 19 trials\n",
      "Processing 8_2: 15 trials\n",
      "Processing 8_3: 21 trials\n",
      "Processing 9_1: 15 trials\n",
      "Processing 9_2: 17 trials\n",
      "Processing 9_3: 16 trials\n",
      "Processing 10_1: 16 trials\n",
      "Processing 10_2: 19 trials\n",
      "Processing 10_3: 21 trials\n",
      "Processing 11_1: 17 trials\n",
      "Processing 11_2: 18 trials\n",
      "Processing 11_3: 16 trials\n",
      "Processing 12_1: 19 trials\n",
      "Processing 12_2: 23 trials\n",
      "Processing 12_3: 19 trials\n",
      "Processing 13_1: 15 trials\n",
      "Processing 13_2: 18 trials\n",
      "Processing 13_3: 20 trials\n",
      "Processing 14_1: 14 trials\n",
      "Processing 14_2: 17 trials\n",
      "Processing 14_3: 17 trials\n",
      "Processing 15_1: 11 trials\n",
      "Processing 15_2: 24 trials\n",
      "Processing 15_3: 11 trials\n",
      "Processing 16_1: 12 trials\n",
      "Processing 16_2: 19 trials\n",
      "Processing 16_3: 23 trials\n",
      "Processing 17_1: 14 trials\n",
      "Processing 17_2: 21 trials\n",
      "Processing 17_3: 19 trials\n",
      "Processing 18_1: 14 trials\n",
      "Processing 18_2: 19 trials\n",
      "Processing 18_3: 20 trials\n",
      "Processing 19_1: 21 trials\n",
      "Processing 19_2: 18 trials\n",
      "Processing 19_3: 23 trials\n",
      "Processing 20_1: 17 trials\n",
      "Processing 20_2: 19 trials\n",
      "Processing 20_3: 22 trials\n",
      "Processing 21_1: 17 trials\n",
      "Processing 21_2: 21 trials\n",
      "Processing 21_3: 15 trials\n",
      "Processing 22_1: 28 trials\n",
      "Processing 22_2: 12 trials\n",
      "Processing 22_3: 14 trials\n",
      "Processing 23_1: 17 trials\n",
      "Processing 23_2: 16 trials\n",
      "Processing 23_3: 10 trials\n",
      "Processing 24_1: 15 trials\n",
      "Processing 24_2: 16 trials\n",
      "Processing 24_3: 14 trials\n",
      "\n",
      "Spike train dictionary created!\n",
      "Number of condition-target combinations: 72\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating spike train dictionary...\")\n",
    "spike_train_dict = create_spike_train_dict(spike_inf, trigger_time, t_start=0, t_stop=500)\n",
    "\n",
    "print(f\"\\nSpike train dictionary created!\")\n",
    "print(f\"Number of condition-target combinations: {len(spike_train_dict)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6188b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('trail_activity_500.pkl', 'wb') as f:\n",
    "    pickle.dump(spike_train_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1292c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('trail_activity_500.pkl', 'rb') as f:\n",
    "    spike_train_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e1ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_train_mean = {}\n",
    "for key in spike_train_dict.keys():\n",
    "    spike_train_mean[key] = []\n",
    "    for i in spike_train_dict[key]:\n",
    "        temp = []\n",
    "        for neuron in i:\n",
    "            temp.append(neuron.mean())\n",
    "        temp = np.array(temp)\n",
    "        spike_train_mean[key].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "910923a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('trail_activity_mean_500.pkl', 'wb') as f:\n",
    "    pickle.dump(spike_train_mean, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

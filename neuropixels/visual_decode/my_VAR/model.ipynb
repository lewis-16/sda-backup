{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "vae_ckpt, var_ckpt = 'vae_ch160v4096z32.pth', f'var_d{MODEL_DEPTH}.pth'\n",
    "\n",
    "# build vae, var\n",
    "FOR_512_px = MODEL_DEPTH == 16\n",
    "# if FOR_512_px:\n",
    "#     patch_nums = (1, 2, 3, 4, 6, 9, 13, 18, 24, 32)\n",
    "# else:\n",
    "#     patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vae, var = build_vae_var(\n",
    "    V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "    device=device, patch_nums=patch_nums,\n",
    "    num_classes=1000, depth=MODEL_DEPTH, shared_aln=FOR_512_px,\n",
    ")\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "#var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralVAR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cond_dim=1024,          # 条件向量维度\n",
    "        feature_dim=32,         # 特征图通道数\n",
    "        depth=12,               # Transformer层数\n",
    "        num_heads=8,            # 注意力头数\n",
    "        mlp_ratio=4.0,          # MLP扩展比例\n",
    "        dropout=0.1,            # Dropout率\n",
    "        patch_nums=(1, 2, 3, 4, 6, 9, 13, 18, 24, 32),  # 层级尺寸序列\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 超参数设置\n",
    "        self.cond_dim = cond_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_nums = patch_nums\n",
    "        self.num_levels = len(patch_nums)\n",
    "        \n",
    "        # 条件投影模块\n",
    "        self.cond_proj = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 4 * cond_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * cond_dim, cond_dim),\n",
    "            nn.LayerNorm(cond_dim)\n",
    "        )\n",
    "        \n",
    "        # 位置编码系统\n",
    "        self.init_level_embeddings()\n",
    "        \n",
    "        # Transformer核心\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                dim=feature_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                dropout=dropout,\n",
    "                cond_dim=cond_dim\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # 特征预测头\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.Linear(feature_dim, feature_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(feature_dim * 2, feature_dim)\n",
    "        )\n",
    "        \n",
    "        # 上采样模块 - 修复尺寸问题\n",
    "        self.upsample_layers = nn.ModuleList()\n",
    "        for i in range(1, self.num_levels):\n",
    "            input_size = patch_nums[i-1]\n",
    "            output_size = patch_nums[i]\n",
    "            scale_factor = output_size / input_size\n",
    "            \n",
    "            self.upsample_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False),\n",
    "                    nn.Conv2d(feature_dim, feature_dim, 3, padding=1),\n",
    "                    nn.GroupNorm(8, feature_dim),\n",
    "                    nn.GELU()\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # 初始特征标记\n",
    "        self.init_token = nn.Parameter(torch.zeros(1, 1, feature_dim))\n",
    "        nn.init.normal_(self.init_token, std=0.02)\n",
    "\n",
    "    def init_level_embeddings(self):\n",
    "        \"\"\"初始化层级位置编码\"\"\"\n",
    "        # 层级嵌入\n",
    "        self.level_embed = nn.Embedding(self.num_levels, self.feature_dim)\n",
    "        nn.init.normal_(self.level_embed.weight, std=0.02)\n",
    "        \n",
    "        # 空间位置编码\n",
    "        self.pos_embeddings = nn.ParameterList()\n",
    "        for size in self.patch_nums:\n",
    "            pos = torch.zeros(1, size*size, self.feature_dim)\n",
    "            nn.init.trunc_normal_(pos, std=0.02)\n",
    "            self.pos_embeddings.append(nn.Parameter(pos))\n",
    "\n",
    "    def forward(self, cond_vector):\n",
    "        \"\"\"\n",
    "        修复后的前向传播\n",
    "        :param cond_vector: 条件向量 [B, cond_dim]\n",
    "        :return: 最终特征图 [B, C, H, W]\n",
    "        \"\"\"\n",
    "        # 1. 预处理条件向量\n",
    "        cond = self.cond_proj(cond_vector)  # [B, cond_dim]\n",
    "        \n",
    "        # 2. 初始化特征图\n",
    "        B = cond.size(0)\n",
    "        device = cond.device\n",
    "        f_hat_current = None  # 动态特征图\n",
    "        \n",
    "        # 3. 逐层级生成\n",
    "        for level_idx in range(self.num_levels):\n",
    "            current_size = self.patch_nums[level_idx]\n",
    "            \n",
    "            # 获取当前层级的token\n",
    "            if level_idx == 0:\n",
    "                # 初始层级使用可学习token\n",
    "                tokens = self.init_token.expand(B, 1, -1)\n",
    "            else:\n",
    "                # 从当前特征图提取token\n",
    "                tokens = self.get_level_tokens(f_hat_current, level_idx)\n",
    "            \n",
    "            # 添加位置编码\n",
    "            pos_emb = self.pos_embeddings[level_idx]\n",
    "            level_emb = self.level_embed(\n",
    "                torch.tensor([level_idx], device=device)\n",
    "            ).view(1, 1, -1)\n",
    "            \n",
    "            tokens = tokens + pos_emb + level_emb\n",
    "            \n",
    "            # Transformer处理\n",
    "            tokens = self.apply_transformer(tokens, cond)\n",
    "            \n",
    "            # 预测特征更新\n",
    "            pred_tokens = self.output_head(tokens)\n",
    "            \n",
    "            # 转换为空间特征\n",
    "            new_features = pred_tokens.reshape(\n",
    "                B, current_size, current_size, -1\n",
    "            ).permute(0, 3, 1, 2)\n",
    "            \n",
    "            # 更新特征图\n",
    "            if level_idx == 0:\n",
    "                f_hat_current = new_features\n",
    "            else:\n",
    "                # 关键修复：正确上采样当前特征图\n",
    "                upsampled = self.upsample_layers[level_idx-1](f_hat_current)\n",
    "                \n",
    "                # 确保尺寸匹配\n",
    "                _, _, h, w = upsampled.shape\n",
    "                target_size = self.patch_nums[level_idx]\n",
    "                \n",
    "                if h != target_size or w != target_size:\n",
    "                    # 二次调整确保尺寸精确匹配\n",
    "                    upsampled = F.interpolate(\n",
    "                        upsampled, \n",
    "                        size=(target_size, target_size),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=False\n",
    "                    )\n",
    "                \n",
    "                # 融合特征\n",
    "                f_hat_current = upsampled + new_features\n",
    "        \n",
    "        return f_hat_current\n",
    "\n",
    "    def get_level_tokens(self, f_hat, level_idx):\n",
    "        \"\"\"\n",
    "        从当前特征图中提取层级token\n",
    "        :param f_hat: 当前特征图 [B, C, H, W]\n",
    "        :param level_idx: 当前层级索引\n",
    "        :return: token序列 [B, N, C]\n",
    "        \"\"\"\n",
    "        # 获取当前层级尺寸\n",
    "        current_size = self.patch_nums[level_idx]\n",
    "        \n",
    "        # 调整到当前分辨率\n",
    "        resized = F.interpolate(\n",
    "            f_hat, \n",
    "            size=(current_size, current_size), \n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        # 展开为token序列 [B, C, S, S] -> [B, S*S, C]\n",
    "        return resized.permute(0, 2, 3, 1).reshape(\n",
    "            resized.size(0), current_size*current_size, -1\n",
    "        )\n",
    "\n",
    "    def apply_transformer(self, tokens, cond):\n",
    "        \"\"\"应用Transformer块\"\"\"\n",
    "        for block in self.transformer_blocks:\n",
    "            tokens = block(tokens, cond)\n",
    "        return tokens\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"条件Transformer模块\"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1, cond_dim=1024):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads, dropout)\n",
    "        \n",
    "        # 条件自适应层归一化\n",
    "        self.cond_norm = AdaLN(dim, cond_dim)\n",
    "        self.mlp = MLP(dim, int(dim * mlp_ratio), dim, dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, cond):\n",
    "        # 1. 自注意力\n",
    "        attn_out = self.attn(self.norm1(x))\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # 2. 条件MLP\n",
    "        residual = x\n",
    "        x = self.cond_norm(x, cond)\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = residual + self.dropout(mlp_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力机制\"\"\"\n",
    "    def __init__(self, dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim必须能被num_heads整除\"\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(2)  # [B, N, num_heads, head_dim]\n",
    "        \n",
    "        # 缩放点积注意力\n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # 聚合值\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class AdaLN(nn.Module):\n",
    "    \"\"\"自适应层归一化\"\"\"\n",
    "    def __init__(self, dim, cond_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim, elementwise_affine=False)\n",
    "        self.ada_lin = nn.Sequential(\n",
    "            nn.Linear(cond_dim, dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 2, dim * 2)\n",
    "        )\n",
    "        \n",
    "        # 初始化适配器\n",
    "        nn.init.zeros_(self.ada_lin[-1].weight)\n",
    "        nn.init.zeros_(self.ada_lin[-1].bias)\n",
    "    \n",
    "    def forward(self, x, cond):\n",
    "        # 计算自适应参数\n",
    "        params = self.ada_lin(cond)\n",
    "        scale, shift = params.chunk(2, dim=-1)\n",
    "        \n",
    "        # 应用自适应归一化\n",
    "        x = self.norm(x)\n",
    "        return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"多层感知机\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8538a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuralvar = NeuralVAR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((10, 3, 256, 256)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcba6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_ms_idx_Bl = vae.img_to_idxBl(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_ms_idx_Bl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0664cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_BL = torch.cat(gt_ms_idx_Bl, dim=1)\n",
    "v_patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe323ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_BLCv_wo_first_l: Ten = var.quantize_local.idxBl_to_var_input(gt_idx_Bl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_scales = []\n",
    "B = gt_ms_idx_Bl[0].shape[0]\n",
    "C = 32\n",
    "H = W = 16\n",
    "SN = 10\n",
    "\n",
    "f_hat = gt_ms_idx_Bl[0].new_zeros(B, C, H, W, dtype=torch.float32)\n",
    "pn_next: int = v_patch_nums[0]\n",
    "for si in range(SN-1):\n",
    "    h_BChw = F.interpolate(self.embedding(gt_ms_idx_Bl[si]).transpose_(1, 2).view(B, C, pn_next, pn_next), size=(H, W), mode='bicubic')\n",
    "    f_hat.add_(self.quant_resi[si/(SN-1)](h_BChw))\n",
    "    pn_next = v_patch_nums[si+1]\n",
    "    next_scales.append(F.interpolate(f_hat, size=(pn_next, pn_next), mode='area').view(B, C, -1).transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxBl_to_var_input(self, gt_ms_idx_Bl: List[torch.Tensor]) -> torch.Tensor:\n",
    "    \n",
    "    return torch.cat(next_scales, dim=1) if len(next_scales) else None    # cat BlCs to BLC, this should be float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f1800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = cond_BD = var.class_emb(torch.cat((label_B, torch.full_like(label_B, fill_value=var.num_classes)), dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_B = torch.tensor((1, 2, 3, 4)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "var.class_emb(torch.cat((label_B, torch.full_like(label_B, fill_value=var.num_classes)), dim=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(np.array(range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = var.class_emb(torch.full_like(torch.tensor(np.array(range(4))).to('cuda'), fill_value=var.num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn((4, 1024)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.vstack((a, b)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_B = torch.where(torch.rand(B, device=label_B.device) < var.cond_drop_rate, var.num_classes, torch.tensor(range(B)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccefb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c24753",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_index = torch.where(label_B == 1000)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7013cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_B = torch.randn((10, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_B[[1, 2, 4, 5], :] = torch.randn((1, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57912882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a77fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(drift_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

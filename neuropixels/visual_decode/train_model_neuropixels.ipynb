{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import csv\n",
    "from torch import Tensor\n",
    "import itertools\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self,\n",
    "                 input_neuron=25,        \n",
    "                 time_bins=20,          \n",
    "                 d_model = 150,          \n",
    "                 nhead=10,                \n",
    "                 num_transformer_layers=1, \n",
    "                 conv_channels=64,      \n",
    "                 num_conv_blocks=3,      \n",
    "                 num_classes=117,        \n",
    "                 residual_dims=[256, 512, 1024], \n",
    "                 use_positional_encoding=True,  \n",
    "                 dim_feedforward_ratio=4,      \n",
    "                 activation='relu',\n",
    "                 use_neuron_masking=True,  \n",
    "                 mask_ratio=0,\n",
    "                 mask_replacement='zero'):\n",
    "        \n",
    "        # Transformer \n",
    "        self.transformer = {\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_layers': num_transformer_layers,\n",
    "            'dim_feedforward': d_model * dim_feedforward_ratio,\n",
    "            'activation': activation\n",
    "        }\n",
    "        \n",
    "        # cnn\n",
    "        self.convolution = {\n",
    "            'channels': conv_channels,\n",
    "            'num_blocks': num_conv_blocks,\n",
    "            'kernel_size': (3, 3),\n",
    "            'pool_size': (2, 2)\n",
    "        }\n",
    "        \n",
    "        # resnet\n",
    "        self.residual = {\n",
    "            'dims': residual_dims,\n",
    "            'skip_connection': True\n",
    "        }\n",
    "        \n",
    "        self.masking = {\n",
    "            'enabled': use_neuron_masking,\n",
    "            'ratio': mask_ratio,\n",
    "            'replacement': mask_replacement\n",
    "        }\n",
    "\n",
    "        self.input_dim = input_neuron\n",
    "        self.time_steps = time_bins\n",
    "        self.num_classes = num_classes\n",
    "        self.positional_encoding = use_positional_encoding\n",
    "        self.lr = 2e-4\n",
    "        self.epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuronMasker(nn.Module):\n",
    "    def __init__(self, mask_ratio=0.15, replacement='random'):\n",
    "        super().__init__()\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.replacement = replacement\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            if x is None:\n",
    "                raise ValueError(\"Input tensor x is None\")\n",
    "                \n",
    "            batch_size, seq_len, feat_dim = x.shape\n",
    "            mask = torch.rand(batch_size, 1, feat_dim, device=x.device) < self.mask_ratio\n",
    "            mask = mask.expand_as(x)\n",
    "            \n",
    "            if self.replacement == 'zero':\n",
    "                x_masked = x.masked_fill(mask, 0)\n",
    "            elif self.replacement == 'random':\n",
    "                random_values = torch.randn_like(x) * 0.02\n",
    "                x_masked = x.masked_scatter(mask, random_values)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid replacement: {self.replacement}\")\n",
    "            \n",
    "            return x_masked \n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class ResidualLinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.downsample = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x + residual\n",
    "\n",
    "class TimeTransformerConvModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.input_proj = nn.Linear(config.input_dim, config.transformer['d_model'])\n",
    "        self.pos_encoder = PositionalEncoding(config.transformer['d_model']) if config.positional_encoding else nn.Identity()\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.transformer['d_model'],\n",
    "            nhead=config.transformer['nhead'],\n",
    "            dim_feedforward=config.transformer['dim_feedforward'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, config.transformer['num_layers'])\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential()\n",
    "        in_channels = 1\n",
    "        for _ in range(config.convolution['num_blocks']):\n",
    "            self.conv_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, config.convolution['channels'], \n",
    "                            kernel_size=config.convolution['kernel_size'], padding='same'),\n",
    "                    nn.BatchNorm2d(config.convolution['channels']),\n",
    "                    nn.ELU(),\n",
    "                    nn.MaxPool2d(kernel_size=config.convolution['pool_size'])\n",
    "                )\n",
    "            )\n",
    "            in_channels = config.convolution['channels']\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(config.convolution['channels'], config.num_classes)\n",
    "        \n",
    "        self.residual_layers = nn.Sequential()\n",
    "        current_dim = config.convolution['channels']\n",
    "        for dim in config.residual['dims']:\n",
    "            self.residual_layers.append(ResidualLinearBlock(current_dim, dim))\n",
    "            current_dim = dim\n",
    "        if current_dim != 1024:\n",
    "            self.residual_layers.append(nn.Linear(current_dim, 1024))\n",
    "            self.residual_layers.append(nn.LayerNorm(1024))\n",
    "\n",
    "\n",
    "        self.masker = NeuronMasker(\n",
    "            mask_ratio=self.config.masking['ratio'],\n",
    "            replacement=self.config.masking['replacement']\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.masker(x)  # [B, T, D]\n",
    "        #print(x)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        features = self.residual_layers(x)\n",
    "        \n",
    "        return logits, features\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, alpha=0, temp=0.07):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha      # 分类损失权重\n",
    "        self.temp = temp\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.temp = temp\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def contrastive_loss(self, h_neuro, h_img):\n",
    "        h_neuro = F.normalize(h_neuro, dim=1) + 1e-10\n",
    "        h_img = F.normalize(h_img, dim=1) + 1e-10\n",
    "        \n",
    "        logits_ab = torch.matmul(h_neuro, h_img.T) / self.temp\n",
    "        logits_ba = torch.matmul(h_img, h_neuro.T) / self.temp\n",
    "        \n",
    "        labels = torch.arange(h_neuro.size(0), device=h_neuro.device)\n",
    "        loss_ab = F.cross_entropy(logits_ab, labels)\n",
    "        loss_ba = F.cross_entropy(logits_ba, labels)\n",
    "        \n",
    "        return (loss_ab + loss_ba) / 2\n",
    "    \n",
    "    def forward(self, logits, labels, img_feature, features):\n",
    "        loss_cls = self.ce_loss(logits, labels)\n",
    "        loss_cont = self.contrastive_loss(features, img_feature)\n",
    "        total_loss = self.alpha * loss_cls + (1 - self.alpha) * loss_cont\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, device, criterion, config):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (neuro, labels, img_feature) in enumerate(dataloader):\n",
    "        neuro = neuro.to(device)\n",
    "        labels = labels.to(device)\n",
    "        img_feature = img_feature.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, features = model(neuro)\n",
    "        loss = criterion(logits, labels, img_feature, features)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), \n",
    "            max_norm=3.0,                   \n",
    "            norm_type=2.0                   \n",
    "        )\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    train_loss = total_loss / len(dataloader)\n",
    "    return train_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device, criterion, config):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_features = []\n",
    "    all_img_features = []\n",
    "    \n",
    "    for neuro, labels, img_feature in dataloader:\n",
    "        neuro = neuro.to(device)\n",
    "        labels = labels.to(device)\n",
    "        img_feature = img_feature.to(device)\n",
    "        \n",
    "        logits, features = model(neuro)\n",
    "        loss = criterion(logits, labels, img_feature, features)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        all_features.append(F.normalize(features, dim=1))\n",
    "        all_img_features.append(F.normalize(img_feature, dim=1))\n",
    "    \n",
    "    features_mat = torch.cat(all_features, dim=0)\n",
    "    img_mat = torch.cat(all_img_features, dim=0)\n",
    "    \n",
    "    sims = torch.matmul(features_mat, img_mat.t())\n",
    "    N = sims.size(0)\n",
    "    indices = torch.arange(N, device=sims.device)\n",
    "    \n",
    "    # neuro -> image 排名\n",
    "    ranks_ab = torch.argsort(sims, dim=1, descending=True)\n",
    "    pos_ab = torch.argmax((ranks_ab == indices.view(-1, 1)).int(), dim=1) + 1\n",
    "    \n",
    "    # image -> neuro 排名\n",
    "    ranks_ba = torch.argsort(sims.t(), dim=1, descending=True)\n",
    "    pos_ba = torch.argmax((ranks_ba == indices.view(-1, 1)).int(), dim=1) + 1\n",
    "    \n",
    "    def recall_at(k, ranks):\n",
    "        return (ranks <= k).float().mean().item()\n",
    "    \n",
    "    r1 = 0.5 * (recall_at(1, pos_ab) + recall_at(1, pos_ba))\n",
    "    r5 = 0.5 * (recall_at(5, pos_ab) + recall_at(5, pos_ba))\n",
    "    r10 = 0.5 * (recall_at(10, pos_ab) + recall_at(10, pos_ba))\n",
    "    \n",
    "    mrr = 0.5 * ((1.0 / pos_ab.float()).mean().item() + (1.0 / pos_ba.float()).mean().item())\n",
    "    median_rank = 0.5 * (pos_ab.float().median().item() + pos_ba.float().median().item())\n",
    "    \n",
    "    val_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"recall@1\": r1,\n",
    "        \"recall@5\": r5,\n",
    "        \"recall@10\": r10,\n",
    "        \"mrr\": mrr,\n",
    "        \"median_rank\": median_rank\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_loop(config, model, train_loader, test_loader, device, image_cluster=None):\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "    criterion = MultitaskLoss(alpha=0.0, temp=0.07)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    recalls1 = []\n",
    "    recalls5 = []\n",
    "    recalls10 = []\n",
    "    mrrs = []\n",
    "    med_ranks = []\n",
    "    best_r1 = 0.0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = train_model(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        metrics = evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=test_loader,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(metrics[\"val_loss\"])\n",
    "        recalls1.append(metrics[\"recall@1\"])\n",
    "        recalls5.append(metrics[\"recall@5\"])\n",
    "        recalls10.append(metrics[\"recall@10\"])\n",
    "        mrrs.append(metrics[\"mrr\"])\n",
    "        med_ranks.append(metrics[\"median_rank\"])\n",
    "        \n",
    "        if metrics[\"recall@1\"] > best_r1:\n",
    "            best_r1 = metrics[\"recall@1\"]\n",
    "            torch.save(model, \"best_model_VISp.pth\")\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {metrics['val_loss']:.4f}\")\n",
    "        print(f\"Recall@1: {metrics['recall@1']:.4f} | Recall@5: {metrics['recall@5']:.4f} | Recall@10: {metrics['recall@10']:.4f}\")\n",
    "        print(f\"MRR: {metrics['mrr']:.4f} | Median Rank: {metrics['median_rank']:.2f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recalls1, label=\"Recall@1\")\n",
    "    plt.plot(recalls5, label=\"Recall@5\")\n",
    "    plt.plot(recalls10, label=\"Recall@10\")\n",
    "    plt.title(\"Retrieval Metrics\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        \"best_recall@1\": best_r1,\n",
    "        \"train_history\": {\n",
    "            \"loss\": train_losses\n",
    "        },\n",
    "        \"val_history\": {\n",
    "            \"loss\": val_losses,\n",
    "            \"recall@1\": recalls1,\n",
    "            \"recall@5\": recalls5,\n",
    "            \"recall@10\": recalls10,\n",
    "            \"mrr\": mrrs,\n",
    "            \"median_rank\": med_ranks\n",
    "        },\n",
    "        \"best_epoch\": best_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"activity_dict_region.pkl\", 'rb') as f:\n",
    "    activity_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPDataset(Dataset):\n",
    "    def __init__(self, EP_data, labels, features):  \n",
    "        self.EP_data = EP_data\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        #self.max_pool = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.EP_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        EP_tensor = torch.tensor(self.EP_data[idx], dtype=torch.float32) \n",
    "        label = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        \n",
    "        return EP_tensor.T, label, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start region: VISp\n",
      "Start train: VISp\n",
      "Epoch 1/200\n",
      "Train Loss: 6.8286 | Val Loss: 5.9872\n",
      "Recall@1: 0.0009 | Recall@5: 0.0043 | Recall@10: 0.0111\n",
      "MRR: 0.0072 | Median Rank: 539.00\n",
      "------------------------------------------------------------\n",
      "Epoch 2/200\n",
      "Train Loss: 6.8026 | Val Loss: 5.9735\n",
      "Recall@1: 0.0030 | Recall@5: 0.0081 | Recall@10: 0.0137\n",
      "MRR: 0.0104 | Median Rank: 473.00\n",
      "------------------------------------------------------------\n",
      "Epoch 3/200\n",
      "Train Loss: 6.7211 | Val Loss: 5.9480\n",
      "Recall@1: 0.0017 | Recall@5: 0.0077 | Recall@10: 0.0141\n",
      "MRR: 0.0104 | Median Rank: 437.00\n",
      "------------------------------------------------------------\n",
      "Epoch 4/200\n",
      "Train Loss: 6.5415 | Val Loss: 5.9545\n",
      "Recall@1: 0.0009 | Recall@5: 0.0094 | Recall@10: 0.0171\n",
      "MRR: 0.0104 | Median Rank: 405.00\n",
      "------------------------------------------------------------\n",
      "Epoch 5/200\n",
      "Train Loss: 6.3350 | Val Loss: 5.9590\n",
      "Recall@1: 0.0013 | Recall@5: 0.0064 | Recall@10: 0.0154\n",
      "MRR: 0.0103 | Median Rank: 405.50\n",
      "------------------------------------------------------------\n",
      "Epoch 6/200\n",
      "Train Loss: 6.1434 | Val Loss: 5.8582\n",
      "Recall@1: 0.0013 | Recall@5: 0.0111 | Recall@10: 0.0226\n",
      "MRR: 0.0132 | Median Rank: 353.50\n",
      "------------------------------------------------------------\n",
      "Epoch 7/200\n",
      "Train Loss: 6.0025 | Val Loss: 5.5616\n",
      "Recall@1: 0.0038 | Recall@5: 0.0209 | Recall@10: 0.0423\n",
      "MRR: 0.0219 | Median Rank: 233.50\n",
      "------------------------------------------------------------\n",
      "Epoch 8/200\n",
      "Train Loss: 5.8481 | Val Loss: 5.6673\n",
      "Recall@1: 0.0026 | Recall@5: 0.0145 | Recall@10: 0.0321\n",
      "MRR: 0.0167 | Median Rank: 276.00\n",
      "------------------------------------------------------------\n",
      "Epoch 9/200\n",
      "Train Loss: 5.6812 | Val Loss: 5.7180\n",
      "Recall@1: 0.0026 | Recall@5: 0.0132 | Recall@10: 0.0256\n",
      "MRR: 0.0153 | Median Rank: 280.50\n",
      "------------------------------------------------------------\n",
      "Epoch 10/200\n",
      "Train Loss: 5.5097 | Val Loss: 5.4449\n",
      "Recall@1: 0.0047 | Recall@5: 0.0226 | Recall@10: 0.0444\n",
      "MRR: 0.0239 | Median Rank: 194.50\n",
      "------------------------------------------------------------\n",
      "Epoch 11/200\n",
      "Train Loss: 5.3417 | Val Loss: 4.9954\n",
      "Recall@1: 0.0184 | Recall@5: 0.0803 | Recall@10: 0.1415\n",
      "MRR: 0.0611 | Median Rank: 81.50\n",
      "------------------------------------------------------------\n",
      "Epoch 12/200\n",
      "Train Loss: 5.1877 | Val Loss: 4.9714\n",
      "Recall@1: 0.0171 | Recall@5: 0.0748 | Recall@10: 0.1342\n",
      "MRR: 0.0585 | Median Rank: 86.50\n",
      "------------------------------------------------------------\n",
      "Epoch 13/200\n",
      "Train Loss: 5.0279 | Val Loss: 4.8049\n",
      "Recall@1: 0.0214 | Recall@5: 0.0953 | Recall@10: 0.1714\n",
      "MRR: 0.0723 | Median Rank: 63.50\n",
      "------------------------------------------------------------\n",
      "Epoch 14/200\n",
      "Train Loss: 4.8871 | Val Loss: 4.5622\n",
      "Recall@1: 0.0291 | Recall@5: 0.1222 | Recall@10: 0.2282\n",
      "MRR: 0.0926 | Median Rank: 40.00\n",
      "------------------------------------------------------------\n",
      "Epoch 15/200\n",
      "Train Loss: 4.7480 | Val Loss: 4.3639\n",
      "Recall@1: 0.0350 | Recall@5: 0.1684 | Recall@10: 0.2902\n",
      "MRR: 0.1141 | Median Rank: 28.50\n",
      "------------------------------------------------------------\n",
      "Epoch 16/200\n",
      "Train Loss: 4.5946 | Val Loss: 4.4057\n",
      "Recall@1: 0.0346 | Recall@5: 0.1603 | Recall@10: 0.2645\n",
      "MRR: 0.1090 | Median Rank: 33.00\n",
      "------------------------------------------------------------\n",
      "Epoch 17/200\n",
      "Train Loss: 4.4660 | Val Loss: 4.3386\n",
      "Recall@1: 0.0355 | Recall@5: 0.1637 | Recall@10: 0.2859\n",
      "MRR: 0.1132 | Median Rank: 31.50\n",
      "------------------------------------------------------------\n",
      "Epoch 18/200\n",
      "Train Loss: 4.3271 | Val Loss: 4.1131\n",
      "Recall@1: 0.0423 | Recall@5: 0.1966 | Recall@10: 0.3308\n",
      "MRR: 0.1326 | Median Rank: 21.50\n",
      "------------------------------------------------------------\n",
      "Epoch 19/200\n",
      "Train Loss: 4.1895 | Val Loss: 3.9654\n",
      "Recall@1: 0.0487 | Recall@5: 0.2239 | Recall@10: 0.3795\n",
      "MRR: 0.1484 | Median Rank: 17.50\n",
      "------------------------------------------------------------\n",
      "Epoch 20/200\n",
      "Train Loss: 4.0646 | Val Loss: 3.7981\n",
      "Recall@1: 0.0585 | Recall@5: 0.2641 | Recall@10: 0.4419\n",
      "MRR: 0.1705 | Median Rank: 12.50\n",
      "------------------------------------------------------------\n",
      "Epoch 21/200\n",
      "Train Loss: 3.9516 | Val Loss: 3.6179\n",
      "Recall@1: 0.0658 | Recall@5: 0.3034 | Recall@10: 0.5120\n",
      "MRR: 0.1905 | Median Rank: 10.00\n",
      "------------------------------------------------------------\n",
      "Epoch 22/200\n",
      "Train Loss: 3.8430 | Val Loss: 3.6107\n",
      "Recall@1: 0.0628 | Recall@5: 0.2876 | Recall@10: 0.4855\n",
      "MRR: 0.1845 | Median Rank: 11.00\n",
      "------------------------------------------------------------\n",
      "Epoch 23/200\n",
      "Train Loss: 3.7407 | Val Loss: 3.4889\n",
      "Recall@1: 0.0671 | Recall@5: 0.3128 | Recall@10: 0.5295\n",
      "MRR: 0.1970 | Median Rank: 9.50\n",
      "------------------------------------------------------------\n",
      "Epoch 24/200\n",
      "Train Loss: 3.6512 | Val Loss: 3.4786\n",
      "Recall@1: 0.0675 | Recall@5: 0.3051 | Recall@10: 0.5179\n",
      "MRR: 0.1948 | Median Rank: 9.50\n",
      "------------------------------------------------------------\n",
      "Epoch 25/200\n",
      "Train Loss: 3.5610 | Val Loss: 3.6504\n",
      "Recall@1: 0.0590 | Recall@5: 0.2675 | Recall@10: 0.4500\n",
      "MRR: 0.1731 | Median Rank: 12.50\n",
      "------------------------------------------------------------\n",
      "Epoch 26/200\n",
      "Train Loss: 3.4848 | Val Loss: 3.6259\n",
      "Recall@1: 0.0603 | Recall@5: 0.2739 | Recall@10: 0.4697\n",
      "MRR: 0.1765 | Median Rank: 12.00\n",
      "------------------------------------------------------------\n",
      "Epoch 27/200\n",
      "Train Loss: 3.3954 | Val Loss: 3.4313\n",
      "Recall@1: 0.0671 | Recall@5: 0.3004 | Recall@10: 0.5068\n",
      "MRR: 0.1915 | Median Rank: 10.50\n",
      "------------------------------------------------------------\n",
      "Epoch 28/200\n",
      "Train Loss: 3.3200 | Val Loss: 3.1837\n",
      "Recall@1: 0.0761 | Recall@5: 0.3509 | Recall@10: 0.5885\n",
      "MRR: 0.2168 | Median Rank: 8.50\n",
      "------------------------------------------------------------\n",
      "Epoch 29/200\n",
      "Train Loss: 3.2454 | Val Loss: 3.1616\n",
      "Recall@1: 0.0739 | Recall@5: 0.3427 | Recall@10: 0.5842\n",
      "MRR: 0.2137 | Median Rank: 8.50\n",
      "------------------------------------------------------------\n",
      "Epoch 30/200\n",
      "Train Loss: 3.1857 | Val Loss: 2.9119\n",
      "Recall@1: 0.0825 | Recall@5: 0.3885 | Recall@10: 0.6598\n",
      "MRR: 0.2355 | Median Rank: 7.50\n",
      "------------------------------------------------------------\n",
      "Epoch 31/200\n",
      "Train Loss: 3.1255 | Val Loss: 2.8803\n",
      "Recall@1: 0.0829 | Recall@5: 0.3902 | Recall@10: 0.6607\n",
      "MRR: 0.2366 | Median Rank: 7.50\n",
      "------------------------------------------------------------\n",
      "Epoch 32/200\n",
      "Train Loss: 3.0665 | Val Loss: 2.9284\n",
      "Recall@1: 0.0816 | Recall@5: 0.3829 | Recall@10: 0.6453\n",
      "MRR: 0.2331 | Median Rank: 7.50\n",
      "------------------------------------------------------------\n",
      "Epoch 33/200\n",
      "Train Loss: 3.0227 | Val Loss: 2.7694\n",
      "Recall@1: 0.0876 | Recall@5: 0.4218 | Recall@10: 0.7094\n",
      "MRR: 0.2518 | Median Rank: 6.50\n",
      "------------------------------------------------------------\n",
      "Epoch 34/200\n",
      "Train Loss: 2.9716 | Val Loss: 2.9075\n",
      "Recall@1: 0.0833 | Recall@5: 0.3906 | Recall@10: 0.6641\n",
      "MRR: 0.2373 | Median Rank: 7.00\n",
      "------------------------------------------------------------\n",
      "Epoch 35/200\n",
      "Train Loss: 2.9347 | Val Loss: 2.8399\n",
      "Recall@1: 0.0859 | Recall@5: 0.4073 | Recall@10: 0.6872\n",
      "MRR: 0.2446 | Median Rank: 7.00\n",
      "------------------------------------------------------------\n",
      "Epoch 36/200\n",
      "Train Loss: 2.8887 | Val Loss: 2.7077\n",
      "Recall@1: 0.0885 | Recall@5: 0.4235 | Recall@10: 0.7167\n",
      "MRR: 0.2518 | Median Rank: 6.50\n",
      "------------------------------------------------------------\n",
      "Epoch 37/200\n",
      "Train Loss: 2.8368 | Val Loss: 2.6683\n",
      "Recall@1: 0.0889 | Recall@5: 0.4256 | Recall@10: 0.7299\n",
      "MRR: 0.2541 | Median Rank: 6.50\n",
      "------------------------------------------------------------\n",
      "Epoch 38/200\n",
      "Train Loss: 2.8059 | Val Loss: 2.6375\n",
      "Recall@1: 0.0872 | Recall@5: 0.4265 | Recall@10: 0.7308\n",
      "MRR: 0.2529 | Median Rank: 6.50\n",
      "------------------------------------------------------------\n",
      "Epoch 39/200\n",
      "Train Loss: 2.7717 | Val Loss: 2.4596\n",
      "Recall@1: 0.0906 | Recall@5: 0.4547 | Recall@10: 0.7756\n",
      "MRR: 0.2655 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 40/200\n",
      "Train Loss: 2.7371 | Val Loss: 2.4357\n",
      "Recall@1: 0.0927 | Recall@5: 0.4491 | Recall@10: 0.7795\n",
      "MRR: 0.2662 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 41/200\n",
      "Train Loss: 2.7075 | Val Loss: 2.4959\n",
      "Recall@1: 0.0902 | Recall@5: 0.4470 | Recall@10: 0.7688\n",
      "MRR: 0.2627 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 42/200\n",
      "Train Loss: 2.6759 | Val Loss: 2.4336\n",
      "Recall@1: 0.0889 | Recall@5: 0.4513 | Recall@10: 0.7778\n",
      "MRR: 0.2633 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 43/200\n",
      "Train Loss: 2.6525 | Val Loss: 2.4292\n",
      "Recall@1: 0.0893 | Recall@5: 0.4521 | Recall@10: 0.7761\n",
      "MRR: 0.2639 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 44/200\n",
      "Train Loss: 2.6217 | Val Loss: 2.4604\n",
      "Recall@1: 0.0880 | Recall@5: 0.4389 | Recall@10: 0.7556\n",
      "MRR: 0.2583 | Median Rank: 6.50\n",
      "------------------------------------------------------------\n",
      "Epoch 45/200\n",
      "Train Loss: 2.5965 | Val Loss: 2.3372\n",
      "Recall@1: 0.0919 | Recall@5: 0.4620 | Recall@10: 0.7962\n",
      "MRR: 0.2694 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 46/200\n",
      "Train Loss: 2.5744 | Val Loss: 2.2950\n",
      "Recall@1: 0.0923 | Recall@5: 0.4637 | Recall@10: 0.8047\n",
      "MRR: 0.2699 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 47/200\n",
      "Train Loss: 2.5510 | Val Loss: 2.5118\n",
      "Recall@1: 0.0889 | Recall@5: 0.4342 | Recall@10: 0.7487\n",
      "MRR: 0.2578 | Median Rank: 6.50\n",
      "------------------------------------------------------------\n",
      "Epoch 48/200\n",
      "Train Loss: 2.5303 | Val Loss: 2.3038\n",
      "Recall@1: 0.0927 | Recall@5: 0.4632 | Recall@10: 0.8004\n",
      "MRR: 0.2701 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 49/200\n",
      "Train Loss: 2.5085 | Val Loss: 2.2022\n",
      "Recall@1: 0.0927 | Recall@5: 0.4701 | Recall@10: 0.8205\n",
      "MRR: 0.2732 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 50/200\n",
      "Train Loss: 2.4874 | Val Loss: 2.2896\n",
      "Recall@1: 0.0932 | Recall@5: 0.4620 | Recall@10: 0.8017\n",
      "MRR: 0.2704 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 51/200\n",
      "Train Loss: 2.4768 | Val Loss: 2.2535\n",
      "Recall@1: 0.0927 | Recall@5: 0.4671 | Recall@10: 0.8090\n",
      "MRR: 0.2712 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 52/200\n",
      "Train Loss: 2.4556 | Val Loss: 2.1468\n",
      "Recall@1: 0.0936 | Recall@5: 0.4722 | Recall@10: 0.8308\n",
      "MRR: 0.2751 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 53/200\n",
      "Train Loss: 2.4432 | Val Loss: 2.1260\n",
      "Recall@1: 0.0953 | Recall@5: 0.4722 | Recall@10: 0.8333\n",
      "MRR: 0.2762 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 54/200\n",
      "Train Loss: 2.4236 | Val Loss: 2.1686\n",
      "Recall@1: 0.0932 | Recall@5: 0.4684 | Recall@10: 0.8214\n",
      "MRR: 0.2730 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 55/200\n",
      "Train Loss: 2.4134 | Val Loss: 2.2895\n",
      "Recall@1: 0.0919 | Recall@5: 0.4543 | Recall@10: 0.7906\n",
      "MRR: 0.2668 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 56/200\n",
      "Train Loss: 2.4027 | Val Loss: 2.1472\n",
      "Recall@1: 0.0940 | Recall@5: 0.4709 | Recall@10: 0.8269\n",
      "MRR: 0.2740 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 57/200\n",
      "Train Loss: 2.3821 | Val Loss: 2.1089\n",
      "Recall@1: 0.0944 | Recall@5: 0.4675 | Recall@10: 0.8291\n",
      "MRR: 0.2745 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 58/200\n",
      "Train Loss: 2.3733 | Val Loss: 2.1217\n",
      "Recall@1: 0.0923 | Recall@5: 0.4671 | Recall@10: 0.8171\n",
      "MRR: 0.2719 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 59/200\n",
      "Train Loss: 2.3638 | Val Loss: 2.0543\n",
      "Recall@1: 0.0944 | Recall@5: 0.4731 | Recall@10: 0.8410\n",
      "MRR: 0.2768 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 60/200\n",
      "Train Loss: 2.3484 | Val Loss: 2.1383\n",
      "Recall@1: 0.0927 | Recall@5: 0.4675 | Recall@10: 0.8239\n",
      "MRR: 0.2728 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 61/200\n",
      "Train Loss: 2.3496 | Val Loss: 2.0948\n",
      "Recall@1: 0.0944 | Recall@5: 0.4692 | Recall@10: 0.8316\n",
      "MRR: 0.2751 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 62/200\n",
      "Train Loss: 2.3399 | Val Loss: 1.9773\n",
      "Recall@1: 0.0962 | Recall@5: 0.4808 | Recall@10: 0.8534\n",
      "MRR: 0.2805 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 63/200\n",
      "Train Loss: 2.3366 | Val Loss: 1.9646\n",
      "Recall@1: 0.0979 | Recall@5: 0.4825 | Recall@10: 0.8577\n",
      "MRR: 0.2823 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 64/200\n",
      "Train Loss: 2.3299 | Val Loss: 2.0572\n",
      "Recall@1: 0.0962 | Recall@5: 0.4778 | Recall@10: 0.8457\n",
      "MRR: 0.2789 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 65/200\n",
      "Train Loss: 2.3196 | Val Loss: 2.1807\n",
      "Recall@1: 0.0927 | Recall@5: 0.4632 | Recall@10: 0.8132\n",
      "MRR: 0.2714 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 66/200\n",
      "Train Loss: 2.3133 | Val Loss: 1.9397\n",
      "Recall@1: 0.0962 | Recall@5: 0.4821 | Recall@10: 0.8564\n",
      "MRR: 0.2804 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 67/200\n",
      "Train Loss: 2.3093 | Val Loss: 1.9608\n",
      "Recall@1: 0.0957 | Recall@5: 0.4791 | Recall@10: 0.8504\n",
      "MRR: 0.2795 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 68/200\n",
      "Train Loss: 2.2924 | Val Loss: 1.9866\n",
      "Recall@1: 0.0966 | Recall@5: 0.4774 | Recall@10: 0.8483\n",
      "MRR: 0.2797 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 69/200\n",
      "Train Loss: 2.2917 | Val Loss: 2.0108\n",
      "Recall@1: 0.0962 | Recall@5: 0.4774 | Recall@10: 0.8419\n",
      "MRR: 0.2784 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 70/200\n",
      "Train Loss: 2.2886 | Val Loss: 1.9715\n",
      "Recall@1: 0.0957 | Recall@5: 0.4782 | Recall@10: 0.8504\n",
      "MRR: 0.2794 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 71/200\n",
      "Train Loss: 2.2763 | Val Loss: 1.9418\n",
      "Recall@1: 0.0962 | Recall@5: 0.4808 | Recall@10: 0.8577\n",
      "MRR: 0.2805 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 72/200\n",
      "Train Loss: 2.2673 | Val Loss: 1.9000\n",
      "Recall@1: 0.0970 | Recall@5: 0.4859 | Recall@10: 0.8624\n",
      "MRR: 0.2826 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 73/200\n",
      "Train Loss: 2.2745 | Val Loss: 1.9479\n",
      "Recall@1: 0.0957 | Recall@5: 0.4782 | Recall@10: 0.8521\n",
      "MRR: 0.2796 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 74/200\n",
      "Train Loss: 2.2643 | Val Loss: 1.8870\n",
      "Recall@1: 0.0966 | Recall@5: 0.4846 | Recall@10: 0.8628\n",
      "MRR: 0.2823 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 75/200\n",
      "Train Loss: 2.2677 | Val Loss: 1.8826\n",
      "Recall@1: 0.0962 | Recall@5: 0.4833 | Recall@10: 0.8611\n",
      "MRR: 0.2816 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 76/200\n",
      "Train Loss: 2.2626 | Val Loss: 1.8720\n",
      "Recall@1: 0.0970 | Recall@5: 0.4838 | Recall@10: 0.8615\n",
      "MRR: 0.2825 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 77/200\n",
      "Train Loss: 2.2663 | Val Loss: 1.8733\n",
      "Recall@1: 0.0970 | Recall@5: 0.4855 | Recall@10: 0.8637\n",
      "MRR: 0.2827 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 78/200\n",
      "Train Loss: 2.2497 | Val Loss: 1.9461\n",
      "Recall@1: 0.0957 | Recall@5: 0.4778 | Recall@10: 0.8534\n",
      "MRR: 0.2792 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 79/200\n",
      "Train Loss: 2.2533 | Val Loss: 1.9310\n",
      "Recall@1: 0.0962 | Recall@5: 0.4778 | Recall@10: 0.8513\n",
      "MRR: 0.2794 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 80/200\n",
      "Train Loss: 2.2442 | Val Loss: 1.8935\n",
      "Recall@1: 0.0966 | Recall@5: 0.4812 | Recall@10: 0.8585\n",
      "MRR: 0.2813 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 81/200\n",
      "Train Loss: 2.2469 | Val Loss: 1.8596\n",
      "Recall@1: 0.0974 | Recall@5: 0.4872 | Recall@10: 0.8692\n",
      "MRR: 0.2838 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 82/200\n",
      "Train Loss: 2.2452 | Val Loss: 1.9187\n",
      "Recall@1: 0.0953 | Recall@5: 0.4778 | Recall@10: 0.8568\n",
      "MRR: 0.2792 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 83/200\n",
      "Train Loss: 2.2392 | Val Loss: 1.8558\n",
      "Recall@1: 0.0966 | Recall@5: 0.4838 | Recall@10: 0.8637\n",
      "MRR: 0.2821 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 84/200\n",
      "Train Loss: 2.2417 | Val Loss: 1.8489\n",
      "Recall@1: 0.0970 | Recall@5: 0.4842 | Recall@10: 0.8645\n",
      "MRR: 0.2826 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 85/200\n",
      "Train Loss: 2.2315 | Val Loss: 1.8773\n",
      "Recall@1: 0.0966 | Recall@5: 0.4821 | Recall@10: 0.8598\n",
      "MRR: 0.2815 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 86/200\n",
      "Train Loss: 2.2343 | Val Loss: 1.8672\n",
      "Recall@1: 0.0966 | Recall@5: 0.4846 | Recall@10: 0.8632\n",
      "MRR: 0.2819 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 87/200\n",
      "Train Loss: 2.2308 | Val Loss: 1.8546\n",
      "Recall@1: 0.0966 | Recall@5: 0.4838 | Recall@10: 0.8667\n",
      "MRR: 0.2822 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 88/200\n",
      "Train Loss: 2.2298 | Val Loss: 1.8347\n",
      "Recall@1: 0.0970 | Recall@5: 0.4868 | Recall@10: 0.8658\n",
      "MRR: 0.2834 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 89/200\n",
      "Train Loss: 2.2275 | Val Loss: 1.8682\n",
      "Recall@1: 0.0962 | Recall@5: 0.4829 | Recall@10: 0.8624\n",
      "MRR: 0.2813 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 90/200\n",
      "Train Loss: 2.2199 | Val Loss: 1.8530\n",
      "Recall@1: 0.0966 | Recall@5: 0.4850 | Recall@10: 0.8650\n",
      "MRR: 0.2822 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 91/200\n",
      "Train Loss: 2.2199 | Val Loss: 1.8411\n",
      "Recall@1: 0.0970 | Recall@5: 0.4855 | Recall@10: 0.8675\n",
      "MRR: 0.2828 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 92/200\n",
      "Train Loss: 2.2210 | Val Loss: 1.8372\n",
      "Recall@1: 0.0966 | Recall@5: 0.4850 | Recall@10: 0.8697\n",
      "MRR: 0.2827 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 93/200\n",
      "Train Loss: 2.2231 | Val Loss: 1.8496\n",
      "Recall@1: 0.0966 | Recall@5: 0.4812 | Recall@10: 0.8615\n",
      "MRR: 0.2814 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 94/200\n",
      "Train Loss: 2.2244 | Val Loss: 1.8417\n",
      "Recall@1: 0.0974 | Recall@5: 0.4846 | Recall@10: 0.8650\n",
      "MRR: 0.2827 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 95/200\n",
      "Train Loss: 2.2157 | Val Loss: 1.8176\n",
      "Recall@1: 0.0966 | Recall@5: 0.4876 | Recall@10: 0.8692\n",
      "MRR: 0.2835 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 96/200\n",
      "Train Loss: 2.2130 | Val Loss: 1.8449\n",
      "Recall@1: 0.0962 | Recall@5: 0.4842 | Recall@10: 0.8654\n",
      "MRR: 0.2819 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 97/200\n",
      "Train Loss: 2.2120 | Val Loss: 1.8159\n",
      "Recall@1: 0.0974 | Recall@5: 0.4876 | Recall@10: 0.8701\n",
      "MRR: 0.2839 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 98/200\n",
      "Train Loss: 2.2131 | Val Loss: 1.8419\n",
      "Recall@1: 0.0974 | Recall@5: 0.4868 | Recall@10: 0.8654\n",
      "MRR: 0.2831 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 99/200\n",
      "Train Loss: 2.2088 | Val Loss: 1.8395\n",
      "Recall@1: 0.0962 | Recall@5: 0.4850 | Recall@10: 0.8675\n",
      "MRR: 0.2822 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 100/200\n",
      "Train Loss: 2.2113 | Val Loss: 1.8324\n",
      "Recall@1: 0.0970 | Recall@5: 0.4855 | Recall@10: 0.8667\n",
      "MRR: 0.2828 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 101/200\n",
      "Train Loss: 2.2109 | Val Loss: 1.7920\n",
      "Recall@1: 0.0979 | Recall@5: 0.4880 | Recall@10: 0.8722\n",
      "MRR: 0.2847 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 102/200\n",
      "Train Loss: 2.2081 | Val Loss: 1.8149\n",
      "Recall@1: 0.0966 | Recall@5: 0.4872 | Recall@10: 0.8735\n",
      "MRR: 0.2834 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 103/200\n",
      "Train Loss: 2.2087 | Val Loss: 1.7862\n",
      "Recall@1: 0.0979 | Recall@5: 0.4880 | Recall@10: 0.8739\n",
      "MRR: 0.2845 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 104/200\n",
      "Train Loss: 2.2013 | Val Loss: 1.8042\n",
      "Recall@1: 0.0966 | Recall@5: 0.4863 | Recall@10: 0.8705\n",
      "MRR: 0.2830 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 105/200\n",
      "Train Loss: 2.1997 | Val Loss: 1.8422\n",
      "Recall@1: 0.0966 | Recall@5: 0.4821 | Recall@10: 0.8607\n",
      "MRR: 0.2816 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 106/200\n",
      "Train Loss: 2.2005 | Val Loss: 1.7846\n",
      "Recall@1: 0.0974 | Recall@5: 0.4876 | Recall@10: 0.8722\n",
      "MRR: 0.2842 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 107/200\n",
      "Train Loss: 2.2073 | Val Loss: 1.7872\n",
      "Recall@1: 0.0966 | Recall@5: 0.4889 | Recall@10: 0.8731\n",
      "MRR: 0.2839 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 108/200\n",
      "Train Loss: 2.1951 | Val Loss: 1.7999\n",
      "Recall@1: 0.0970 | Recall@5: 0.4859 | Recall@10: 0.8714\n",
      "MRR: 0.2834 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 109/200\n",
      "Train Loss: 2.1924 | Val Loss: 1.8025\n",
      "Recall@1: 0.0974 | Recall@5: 0.4889 | Recall@10: 0.8705\n",
      "MRR: 0.2839 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 110/200\n",
      "Train Loss: 2.1931 | Val Loss: 1.7763\n",
      "Recall@1: 0.0974 | Recall@5: 0.4893 | Recall@10: 0.8735\n",
      "MRR: 0.2845 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 111/200\n",
      "Train Loss: 2.1922 | Val Loss: 1.8288\n",
      "Recall@1: 0.0962 | Recall@5: 0.4850 | Recall@10: 0.8684\n",
      "MRR: 0.2823 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 112/200\n",
      "Train Loss: 2.2052 | Val Loss: 1.7800\n",
      "Recall@1: 0.0979 | Recall@5: 0.4893 | Recall@10: 0.8726\n",
      "MRR: 0.2848 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 113/200\n",
      "Train Loss: 2.1912 | Val Loss: 1.7841\n",
      "Recall@1: 0.0979 | Recall@5: 0.4868 | Recall@10: 0.8735\n",
      "MRR: 0.2843 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 114/200\n",
      "Train Loss: 2.1980 | Val Loss: 1.8571\n",
      "Recall@1: 0.0966 | Recall@5: 0.4829 | Recall@10: 0.8607\n",
      "MRR: 0.2814 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 115/200\n",
      "Train Loss: 2.2012 | Val Loss: 1.7609\n",
      "Recall@1: 0.0983 | Recall@5: 0.4906 | Recall@10: 0.8761\n",
      "MRR: 0.2855 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 116/200\n",
      "Train Loss: 2.1895 | Val Loss: 1.7640\n",
      "Recall@1: 0.0983 | Recall@5: 0.4910 | Recall@10: 0.8752\n",
      "MRR: 0.2856 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 117/200\n",
      "Train Loss: 2.1870 | Val Loss: 1.7771\n",
      "Recall@1: 0.0974 | Recall@5: 0.4863 | Recall@10: 0.8726\n",
      "MRR: 0.2837 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 118/200\n",
      "Train Loss: 2.1953 | Val Loss: 1.7874\n",
      "Recall@1: 0.0970 | Recall@5: 0.4876 | Recall@10: 0.8726\n",
      "MRR: 0.2836 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 119/200\n",
      "Train Loss: 2.1894 | Val Loss: 1.7719\n",
      "Recall@1: 0.0979 | Recall@5: 0.4872 | Recall@10: 0.8744\n",
      "MRR: 0.2843 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 120/200\n",
      "Train Loss: 2.1902 | Val Loss: 1.7861\n",
      "Recall@1: 0.0970 | Recall@5: 0.4868 | Recall@10: 0.8718\n",
      "MRR: 0.2836 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 121/200\n",
      "Train Loss: 2.1936 | Val Loss: 1.7704\n",
      "Recall@1: 0.0970 | Recall@5: 0.4863 | Recall@10: 0.8714\n",
      "MRR: 0.2834 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 122/200\n",
      "Train Loss: 2.1824 | Val Loss: 1.7541\n",
      "Recall@1: 0.0979 | Recall@5: 0.4889 | Recall@10: 0.8756\n",
      "MRR: 0.2850 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 123/200\n",
      "Train Loss: 2.1823 | Val Loss: 1.7730\n",
      "Recall@1: 0.0974 | Recall@5: 0.4872 | Recall@10: 0.8735\n",
      "MRR: 0.2841 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 124/200\n",
      "Train Loss: 2.1873 | Val Loss: 1.7619\n",
      "Recall@1: 0.0979 | Recall@5: 0.4902 | Recall@10: 0.8752\n",
      "MRR: 0.2852 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 125/200\n",
      "Train Loss: 2.1861 | Val Loss: 1.7947\n",
      "Recall@1: 0.0970 | Recall@5: 0.4868 | Recall@10: 0.8697\n",
      "MRR: 0.2836 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 126/200\n",
      "Train Loss: 2.1842 | Val Loss: 1.7704\n",
      "Recall@1: 0.0979 | Recall@5: 0.4893 | Recall@10: 0.8748\n",
      "MRR: 0.2846 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 127/200\n",
      "Train Loss: 2.1904 | Val Loss: 1.7470\n",
      "Recall@1: 0.0979 | Recall@5: 0.4902 | Recall@10: 0.8765\n",
      "MRR: 0.2851 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 128/200\n",
      "Train Loss: 2.1761 | Val Loss: 1.7504\n",
      "Recall@1: 0.0987 | Recall@5: 0.4910 | Recall@10: 0.8769\n",
      "MRR: 0.2861 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 129/200\n",
      "Train Loss: 2.1844 | Val Loss: 1.8021\n",
      "Recall@1: 0.0966 | Recall@5: 0.4855 | Recall@10: 0.8714\n",
      "MRR: 0.2830 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 130/200\n",
      "Train Loss: 2.1811 | Val Loss: 1.7445\n",
      "Recall@1: 0.0983 | Recall@5: 0.4906 | Recall@10: 0.8778\n",
      "MRR: 0.2857 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 131/200\n",
      "Train Loss: 2.1841 | Val Loss: 1.7652\n",
      "Recall@1: 0.0970 | Recall@5: 0.4889 | Recall@10: 0.8744\n",
      "MRR: 0.2842 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 132/200\n",
      "Train Loss: 2.1838 | Val Loss: 1.7724\n",
      "Recall@1: 0.0979 | Recall@5: 0.4889 | Recall@10: 0.8748\n",
      "MRR: 0.2848 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 133/200\n",
      "Train Loss: 2.1825 | Val Loss: 1.7614\n",
      "Recall@1: 0.0974 | Recall@5: 0.4876 | Recall@10: 0.8748\n",
      "MRR: 0.2842 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 134/200\n",
      "Train Loss: 2.1767 | Val Loss: 1.7777\n",
      "Recall@1: 0.0974 | Recall@5: 0.4876 | Recall@10: 0.8744\n",
      "MRR: 0.2840 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 135/200\n",
      "Train Loss: 2.1860 | Val Loss: 1.7331\n",
      "Recall@1: 0.0979 | Recall@5: 0.4910 | Recall@10: 0.8795\n",
      "MRR: 0.2855 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 136/200\n",
      "Train Loss: 2.1810 | Val Loss: 1.7333\n",
      "Recall@1: 0.0979 | Recall@5: 0.4906 | Recall@10: 0.8765\n",
      "MRR: 0.2852 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 137/200\n",
      "Train Loss: 2.1781 | Val Loss: 1.7647\n",
      "Recall@1: 0.0974 | Recall@5: 0.4885 | Recall@10: 0.8748\n",
      "MRR: 0.2844 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 138/200\n",
      "Train Loss: 2.1774 | Val Loss: 1.7352\n",
      "Recall@1: 0.0979 | Recall@5: 0.4906 | Recall@10: 0.8774\n",
      "MRR: 0.2854 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 139/200\n",
      "Train Loss: 2.1794 | Val Loss: 1.7443\n",
      "Recall@1: 0.0979 | Recall@5: 0.4893 | Recall@10: 0.8774\n",
      "MRR: 0.2849 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 140/200\n",
      "Train Loss: 2.1812 | Val Loss: 1.7455\n",
      "Recall@1: 0.0974 | Recall@5: 0.4880 | Recall@10: 0.8756\n",
      "MRR: 0.2843 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 141/200\n",
      "Train Loss: 2.1753 | Val Loss: 1.7518\n",
      "Recall@1: 0.0979 | Recall@5: 0.4897 | Recall@10: 0.8756\n",
      "MRR: 0.2849 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 142/200\n",
      "Train Loss: 2.1704 | Val Loss: 1.7290\n",
      "Recall@1: 0.0983 | Recall@5: 0.4910 | Recall@10: 0.8778\n",
      "MRR: 0.2858 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 143/200\n",
      "Train Loss: 2.1759 | Val Loss: 1.7406\n",
      "Recall@1: 0.0974 | Recall@5: 0.4880 | Recall@10: 0.8748\n",
      "MRR: 0.2844 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 144/200\n",
      "Train Loss: 2.1746 | Val Loss: 1.7427\n",
      "Recall@1: 0.0983 | Recall@5: 0.4902 | Recall@10: 0.8774\n",
      "MRR: 0.2857 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 145/200\n",
      "Train Loss: 2.1784 | Val Loss: 1.7598\n",
      "Recall@1: 0.0974 | Recall@5: 0.4876 | Recall@10: 0.8739\n",
      "MRR: 0.2843 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 146/200\n",
      "Train Loss: 2.1715 | Val Loss: 1.7553\n",
      "Recall@1: 0.0966 | Recall@5: 0.4868 | Recall@10: 0.8722\n",
      "MRR: 0.2832 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 147/200\n",
      "Train Loss: 2.1779 | Val Loss: 1.7452\n",
      "Recall@1: 0.0974 | Recall@5: 0.4876 | Recall@10: 0.8744\n",
      "MRR: 0.2843 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 148/200\n",
      "Train Loss: 2.1763 | Val Loss: 1.7450\n",
      "Recall@1: 0.0974 | Recall@5: 0.4885 | Recall@10: 0.8778\n",
      "MRR: 0.2847 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 149/200\n",
      "Train Loss: 2.1699 | Val Loss: 1.7376\n",
      "Recall@1: 0.0974 | Recall@5: 0.4897 | Recall@10: 0.8756\n",
      "MRR: 0.2847 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 150/200\n",
      "Train Loss: 2.1721 | Val Loss: 1.7551\n",
      "Recall@1: 0.0979 | Recall@5: 0.4889 | Recall@10: 0.8744\n",
      "MRR: 0.2848 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 151/200\n",
      "Train Loss: 2.1681 | Val Loss: 1.7407\n",
      "Recall@1: 0.0979 | Recall@5: 0.4876 | Recall@10: 0.8756\n",
      "MRR: 0.2846 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 152/200\n",
      "Train Loss: 2.1738 | Val Loss: 1.7589\n",
      "Recall@1: 0.0974 | Recall@5: 0.4868 | Recall@10: 0.8731\n",
      "MRR: 0.2838 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 153/200\n",
      "Train Loss: 2.1702 | Val Loss: 1.7531\n",
      "Recall@1: 0.0974 | Recall@5: 0.4876 | Recall@10: 0.8739\n",
      "MRR: 0.2840 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 154/200\n",
      "Train Loss: 2.1720 | Val Loss: 1.7275\n",
      "Recall@1: 0.0974 | Recall@5: 0.4910 | Recall@10: 0.8778\n",
      "MRR: 0.2852 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 155/200\n",
      "Train Loss: 2.1695 | Val Loss: 1.7355\n",
      "Recall@1: 0.0979 | Recall@5: 0.4889 | Recall@10: 0.8769\n",
      "MRR: 0.2849 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 156/200\n",
      "Train Loss: 2.1713 | Val Loss: 1.7221\n",
      "Recall@1: 0.0983 | Recall@5: 0.4910 | Recall@10: 0.8769\n",
      "MRR: 0.2856 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 157/200\n",
      "Train Loss: 2.1689 | Val Loss: 1.7318\n",
      "Recall@1: 0.0979 | Recall@5: 0.4906 | Recall@10: 0.8765\n",
      "MRR: 0.2853 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 158/200\n",
      "Train Loss: 2.1707 | Val Loss: 1.7373\n",
      "Recall@1: 0.0974 | Recall@5: 0.4885 | Recall@10: 0.8761\n",
      "MRR: 0.2843 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 159/200\n",
      "Train Loss: 2.1657 | Val Loss: 1.7101\n",
      "Recall@1: 0.0987 | Recall@5: 0.4923 | Recall@10: 0.8791\n",
      "MRR: 0.2863 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 160/200\n",
      "Train Loss: 2.1768 | Val Loss: 1.7251\n",
      "Recall@1: 0.0979 | Recall@5: 0.4893 | Recall@10: 0.8765\n",
      "MRR: 0.2850 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 161/200\n",
      "Train Loss: 2.1694 | Val Loss: 1.7141\n",
      "Recall@1: 0.0979 | Recall@5: 0.4915 | Recall@10: 0.8812\n",
      "MRR: 0.2857 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 162/200\n",
      "Train Loss: 2.1725 | Val Loss: 1.7214\n",
      "Recall@1: 0.0974 | Recall@5: 0.4889 | Recall@10: 0.8778\n",
      "MRR: 0.2848 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 163/200\n",
      "Train Loss: 2.1718 | Val Loss: 1.7391\n",
      "Recall@1: 0.0979 | Recall@5: 0.4880 | Recall@10: 0.8752\n",
      "MRR: 0.2849 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 164/200\n",
      "Train Loss: 2.1651 | Val Loss: 1.7279\n",
      "Recall@1: 0.0983 | Recall@5: 0.4889 | Recall@10: 0.8765\n",
      "MRR: 0.2854 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 165/200\n",
      "Train Loss: 2.1702 | Val Loss: 1.7282\n",
      "Recall@1: 0.0979 | Recall@5: 0.4893 | Recall@10: 0.8774\n",
      "MRR: 0.2851 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 166/200\n",
      "Train Loss: 2.1707 | Val Loss: 1.7149\n",
      "Recall@1: 0.0979 | Recall@5: 0.4915 | Recall@10: 0.8791\n",
      "MRR: 0.2856 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 167/200\n",
      "Train Loss: 2.1606 | Val Loss: 1.7217\n",
      "Recall@1: 0.0979 | Recall@5: 0.4902 | Recall@10: 0.8782\n",
      "MRR: 0.2852 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 168/200\n",
      "Train Loss: 2.1710 | Val Loss: 1.7272\n",
      "Recall@1: 0.0983 | Recall@5: 0.4880 | Recall@10: 0.8774\n",
      "MRR: 0.2852 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 169/200\n",
      "Train Loss: 2.1654 | Val Loss: 1.7220\n",
      "Recall@1: 0.0979 | Recall@5: 0.4902 | Recall@10: 0.8786\n",
      "MRR: 0.2854 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 170/200\n",
      "Train Loss: 2.1613 | Val Loss: 1.7222\n",
      "Recall@1: 0.0974 | Recall@5: 0.4876 | Recall@10: 0.8769\n",
      "MRR: 0.2844 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 171/200\n",
      "Train Loss: 2.1647 | Val Loss: 1.7227\n",
      "Recall@1: 0.0983 | Recall@5: 0.4902 | Recall@10: 0.8769\n",
      "MRR: 0.2857 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 172/200\n",
      "Train Loss: 2.1661 | Val Loss: 1.7223\n",
      "Recall@1: 0.0979 | Recall@5: 0.4885 | Recall@10: 0.8774\n",
      "MRR: 0.2851 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 173/200\n",
      "Train Loss: 2.1670 | Val Loss: 1.7004\n",
      "Recall@1: 0.0983 | Recall@5: 0.4910 | Recall@10: 0.8786\n",
      "MRR: 0.2860 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 174/200\n",
      "Train Loss: 2.1686 | Val Loss: 1.7289\n",
      "Recall@1: 0.0970 | Recall@5: 0.4885 | Recall@10: 0.8769\n",
      "MRR: 0.2842 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 175/200\n",
      "Train Loss: 2.1641 | Val Loss: 1.7093\n",
      "Recall@1: 0.0987 | Recall@5: 0.4923 | Recall@10: 0.8803\n",
      "MRR: 0.2865 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 176/200\n",
      "Train Loss: 2.1671 | Val Loss: 1.7278\n",
      "Recall@1: 0.0970 | Recall@5: 0.4876 | Recall@10: 0.8769\n",
      "MRR: 0.2843 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 177/200\n",
      "Train Loss: 2.1680 | Val Loss: 1.7234\n",
      "Recall@1: 0.0979 | Recall@5: 0.4893 | Recall@10: 0.8774\n",
      "MRR: 0.2853 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 178/200\n",
      "Train Loss: 2.1625 | Val Loss: 1.7259\n",
      "Recall@1: 0.0983 | Recall@5: 0.4893 | Recall@10: 0.8778\n",
      "MRR: 0.2854 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 179/200\n",
      "Train Loss: 2.1638 | Val Loss: 1.7120\n",
      "Recall@1: 0.0987 | Recall@5: 0.4902 | Recall@10: 0.8799\n",
      "MRR: 0.2860 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 180/200\n",
      "Train Loss: 2.1678 | Val Loss: 1.7055\n",
      "Recall@1: 0.0983 | Recall@5: 0.4906 | Recall@10: 0.8791\n",
      "MRR: 0.2858 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 181/200\n",
      "Train Loss: 2.1658 | Val Loss: 1.7175\n",
      "Recall@1: 0.0987 | Recall@5: 0.4910 | Recall@10: 0.8778\n",
      "MRR: 0.2860 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 182/200\n",
      "Train Loss: 2.1596 | Val Loss: 1.7108\n",
      "Recall@1: 0.0991 | Recall@5: 0.4919 | Recall@10: 0.8791\n",
      "MRR: 0.2868 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 183/200\n",
      "Train Loss: 2.1681 | Val Loss: 1.7122\n",
      "Recall@1: 0.0991 | Recall@5: 0.4919 | Recall@10: 0.8782\n",
      "MRR: 0.2867 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 184/200\n",
      "Train Loss: 2.1591 | Val Loss: 1.7015\n",
      "Recall@1: 0.0987 | Recall@5: 0.4910 | Recall@10: 0.8799\n",
      "MRR: 0.2863 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 185/200\n",
      "Train Loss: 2.1581 | Val Loss: 1.7083\n",
      "Recall@1: 0.0983 | Recall@5: 0.4910 | Recall@10: 0.8791\n",
      "MRR: 0.2860 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 186/200\n",
      "Train Loss: 2.1699 | Val Loss: 1.7112\n",
      "Recall@1: 0.0974 | Recall@5: 0.4889 | Recall@10: 0.8782\n",
      "MRR: 0.2849 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 187/200\n",
      "Train Loss: 2.1602 | Val Loss: 1.7116\n",
      "Recall@1: 0.0979 | Recall@5: 0.4902 | Recall@10: 0.8782\n",
      "MRR: 0.2855 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 188/200\n",
      "Train Loss: 2.1622 | Val Loss: 1.7152\n",
      "Recall@1: 0.0983 | Recall@5: 0.4893 | Recall@10: 0.8778\n",
      "MRR: 0.2857 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 189/200\n",
      "Train Loss: 2.1609 | Val Loss: 1.6943\n",
      "Recall@1: 0.0991 | Recall@5: 0.4915 | Recall@10: 0.8808\n",
      "MRR: 0.2867 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 190/200\n",
      "Train Loss: 2.1581 | Val Loss: 1.7131\n",
      "Recall@1: 0.0983 | Recall@5: 0.4910 | Recall@10: 0.8786\n",
      "MRR: 0.2859 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 191/200\n",
      "Train Loss: 2.1634 | Val Loss: 1.7058\n",
      "Recall@1: 0.0987 | Recall@5: 0.4902 | Recall@10: 0.8799\n",
      "MRR: 0.2860 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 192/200\n",
      "Train Loss: 2.1669 | Val Loss: 1.7118\n",
      "Recall@1: 0.0979 | Recall@5: 0.4897 | Recall@10: 0.8782\n",
      "MRR: 0.2853 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 193/200\n",
      "Train Loss: 2.1625 | Val Loss: 1.7038\n",
      "Recall@1: 0.0983 | Recall@5: 0.4923 | Recall@10: 0.8799\n",
      "MRR: 0.2862 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 194/200\n",
      "Train Loss: 2.1584 | Val Loss: 1.7018\n",
      "Recall@1: 0.0979 | Recall@5: 0.4897 | Recall@10: 0.8803\n",
      "MRR: 0.2854 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 195/200\n",
      "Train Loss: 2.1587 | Val Loss: 1.7028\n",
      "Recall@1: 0.0983 | Recall@5: 0.4919 | Recall@10: 0.8803\n",
      "MRR: 0.2862 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 196/200\n",
      "Train Loss: 2.1601 | Val Loss: 1.6974\n",
      "Recall@1: 0.0983 | Recall@5: 0.4915 | Recall@10: 0.8799\n",
      "MRR: 0.2860 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 197/200\n",
      "Train Loss: 2.1565 | Val Loss: 1.6909\n",
      "Recall@1: 0.0991 | Recall@5: 0.4915 | Recall@10: 0.8812\n",
      "MRR: 0.2867 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 198/200\n",
      "Train Loss: 2.1614 | Val Loss: 1.6904\n",
      "Recall@1: 0.0987 | Recall@5: 0.4915 | Recall@10: 0.8808\n",
      "MRR: 0.2865 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 199/200\n",
      "Train Loss: 2.1547 | Val Loss: 1.7138\n",
      "Recall@1: 0.0979 | Recall@5: 0.4897 | Recall@10: 0.8786\n",
      "MRR: 0.2855 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n",
      "Epoch 200/200\n",
      "Train Loss: 2.1583 | Val Loss: 1.6840\n",
      "Recall@1: 0.0991 | Recall@5: 0.4919 | Recall@10: 0.8816\n",
      "MRR: 0.2869 | Median Rank: 6.00\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for region in [#'VISam', \n",
    "               'VISp'#, 'VISl', 'VISrl', 'VISpm', 'VISal'\n",
    "               ]:\n",
    "    print(f'Start region: {region}')\n",
    "\n",
    "    session_list = list(activity_dict[region].keys())\n",
    "    for session in session_list:\n",
    "        if len(activity_dict[region][session].keys()) != 5850:\n",
    "            session_list.remove(session)\n",
    "        \n",
    "    activity_dict_all = {}\n",
    "    for image in activity_dict[region][session].keys():\n",
    "        temp = pd.DataFrame()\n",
    "        for session in session_list:\n",
    "            temp = pd.concat((temp, pd.DataFrame(activity_dict[region][session][image])), axis=0)\n",
    "        \n",
    "        global_min = temp.min()\n",
    "        global_max = temp.max()\n",
    "        activity_dict_all[image] = (temp - global_min) / (global_max - global_min + 1e-8)\n",
    "    image_feature = pd.read_csv(\"/media/ubuntu/sda/neuropixels/visual_decode/image_feature.csv\", index_col=0)\n",
    "    \n",
    "    EP_data = []\n",
    "    labels = []\n",
    "    features = []\n",
    "\n",
    "    for image in activity_dict_all.keys():\n",
    "        if \"117\" not in image:\n",
    "            EP_data.append(np.array(activity_dict_all[image]))\n",
    "            image = int(image.split(\"_\")[0])\n",
    "            labels.append(image)\n",
    "            features.append(np.array(image_feature.iloc[image, :]))\n",
    "\n",
    "    from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "    dataset = EPDataset(EP_data=EP_data, labels=labels, features=features)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    batch_size = 1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    config = ModelConfig(input_neuron=EP_data[0].shape[0])\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TimeTransformerConvModel(config).to(device)\n",
    "\n",
    "    image_cluster = pd.read_csv(\"image_cluster.csv\")\n",
    "    print(f'Start train: {region}')\n",
    "    results[region] = main_train_loop(\n",
    "        config=config,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=device,\n",
    "        image_cluster = image_cluster\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(4, 3), dpi=300)\n",
    "\n",
    "# for region in results.keys():\n",
    "#     sns.lineplot(\n",
    "#         data=results[region]['test_history']['accuracy'],\n",
    "#         label= region\n",
    "#     )\n",
    "\n",
    "# plt.ylabel('Accuracy') \n",
    "# plt.xlabel('Epoch')  \n",
    "# plt.tight_layout()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "train_features = []\n",
    "train_labels = []\n",
    "for neuro, labels, img_feature in train_loader:\n",
    "    neuro = neuro.to(device)\n",
    "    labels = labels.to(device)\n",
    "    img_feature = img_feature.to(device)\n",
    "    \n",
    "    logits, features = model(neuro)\n",
    "\n",
    "    train_labels.extend(labels.cpu().detach().numpy())\n",
    "    train_features.append(features.cpu().detach().numpy())\n",
    "\n",
    "train_features = np.vstack(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "eval_features = []\n",
    "eval_labels = []\n",
    "for neuro, labels, img_feature in test_loader:\n",
    "    neuro = neuro.to(device)\n",
    "    labels = labels.to(device)\n",
    "    img_feature = img_feature.to(device)\n",
    "    \n",
    "    logits, features = model(neuro)\n",
    "\n",
    "    eval_labels.extend(labels.cpu().detach().numpy())\n",
    "    eval_features.append(features.cpu().detach().numpy())\n",
    "\n",
    "eval_features = np.vstack(eval_features)\n",
    "eval_labels = np.array(eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'concat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_features \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m((train_features, train_labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m eval_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcat((eval_features, eval_labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/numpy/__init__.py:333\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoved in NumPy 1.25.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTester was removed in NumPy 1.25.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'concat'"
     ]
    }
   ],
   "source": [
    "train_features = np.concat((train_features, train_labels.reshape(-1, 1)), axis=1)\n",
    "eval_features = np.concat((eval_features, eval_labels.reshape(-1, 1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('VISp_train_features.npy', train_features)\n",
    "np.save('VISp_eval_features.npy', eval_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

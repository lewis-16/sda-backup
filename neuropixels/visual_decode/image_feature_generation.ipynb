{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8594d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import csv\n",
    "from torch import Tensor\n",
    "import itertools\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca30f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/visual_decoding/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "device = 'cuda'\n",
    "model_type = 'ViT-H-14'\n",
    "\n",
    "vlmodel, preprocess_train, feature_extractor = open_clip.create_model_and_transforms(\n",
    "    'ViT-H-14', pretrained = '/media/ubuntu/sda/neuropixels/visual_decode/open_clip_pytorch_model.bin', device = device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed1fd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_encoder(images, vlmodel, preprocess, batch_size):\n",
    "        features = []\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch_images = images[i:i+batch_size]\n",
    "            batch_input = torch.stack([preprocess(img) for img in batch_images]).to('cuda')\n",
    "            with torch.no_grad():\n",
    "                batch_features = vlmodel.encode_image(batch_input)\n",
    "                batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "            features.append(batch_features.cpu())\n",
    "        return torch.cat(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folderpath = '/media/ubuntu/sda/neuropixels/natural_movie/natural_movie_1_frame'\n",
    "image_list = []\n",
    "for i in range(900):\n",
    "    image = Image.open(os.path.join(image_folderpath, f\"frame_{i}.png\")).convert(\"L\")\n",
    "    image_list.append(image)\n",
    "\n",
    "image_features = image_encoder(image_list, vlmodel, preprocess_train, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8778cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = pd.DataFrame(image_features.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d86f3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = image_features.iloc[2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15e4a33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(898, 1024)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b78235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features.index = list(range(898))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51db9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features.to_csv(\"/media/ubuntu/sda/neuropixels/natural_movie/natural_movie_1_frame_features.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

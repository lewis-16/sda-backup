{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767dae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import csv\n",
    "from torch import Tensor\n",
    "import itertools\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tqdm\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# session_folder = os.listdir(\"/media/ubuntu/sda/neuropixels/output_dir\")\n",
    "# activity_dict = {}  \n",
    "# num_bins = 30  # 定义时间窗口的分段数量\n",
    "\n",
    "# for session in tqdm.tqdm(session_folder):\n",
    "#     if 'session_' not in session:\n",
    "#         continue\n",
    "        \n",
    "#     trigger_time = pd.read_csv(f\"/media/ubuntu/sda/neuropixels/output_dir/{session}/stimulus_table_natural_movie.csv\")\n",
    "#     trigger_time = trigger_time[trigger_time['stimulus_name'] == 'natural_movie_one']\n",
    "\n",
    "#     spike_inf = pd.read_csv(f\"/media/ubuntu/sda/neuropixels/output_dir/{session}/spike_inf_natural_movie.csv\", index_col=0)\n",
    "    \n",
    "#     filtered_spikes = spike_inf['id'].value_counts()\n",
    "#     filtered_spikes = filtered_spikes[filtered_spikes > 30000].index\n",
    "#     spike_inf = spike_inf[spike_inf['id'].isin(filtered_spikes)]\n",
    "    \n",
    "#     regions = spike_inf['region'].unique()\n",
    "    \n",
    "#     neuron_mappings = {}\n",
    "#     for region in regions:\n",
    "#         region_neurons = spike_inf[spike_inf['region'] == region]['id'].unique()\n",
    "#         region_neurons = np.intersect1d(region_neurons, filtered_spikes)\n",
    "        \n",
    "#         if len(region_neurons) == 0:\n",
    "#             continue\n",
    "            \n",
    "#         neuron_id_to_idx = {id: idx for idx, id in enumerate(region_neurons)}\n",
    "#         neuron_mappings[region] = {\n",
    "#             'neurons': region_neurons,\n",
    "#             'id_to_idx': neuron_id_to_idx,\n",
    "#             'n_neurons': len(region_neurons)\n",
    "#         }\n",
    "    \n",
    "#     if not neuron_mappings:\n",
    "#         continue\n",
    "        \n",
    "#     spike_times = spike_inf['time'].values\n",
    "#     spike_ids = spike_inf['id'].values\n",
    "#     spike_regions = spike_inf['region'].values\n",
    "    \n",
    "#     for region in neuron_mappings:\n",
    "#         if region not in activity_dict:\n",
    "#             activity_dict[region] = {}\n",
    "#         activity_dict[region][session] = {}\n",
    "    \n",
    "#     for image in range(900):\n",
    "#         trigger_time_temp = trigger_time[trigger_time['frame'] == image]\n",
    "        \n",
    "#         for trial_idx, (_, row) in enumerate(trigger_time_temp.iterrows()):\n",
    "#             start_time = row['start_time']\n",
    "#             end_time = row['stop_time']\n",
    "#             duration = end_time - start_time\n",
    "            \n",
    "#             mask = (spike_times >= start_time) & (spike_times < end_time)\n",
    "#             trial_spike_times = spike_times[mask] - start_time\n",
    "#             trial_spike_ids = spike_ids[mask]\n",
    "#             trial_spike_regions = spike_regions[mask]\n",
    "            \n",
    "#             for region, mapping in neuron_mappings.items():\n",
    "#                 region_mask = np.isin(trial_spike_regions, [region])\n",
    "#                 region_spike_times = trial_spike_times[region_mask]\n",
    "#                 region_spike_ids = trial_spike_ids[region_mask]\n",
    "                \n",
    "#                 n_neurons = mapping['n_neurons']\n",
    "#                 neuron_id_to_idx = mapping['id_to_idx']\n",
    "                \n",
    "#                 neuron_indices = np.array([neuron_id_to_idx[id] for id in region_spike_ids])\n",
    "                \n",
    "#                 # 直接计算每个神经元在时间窗口内的放电次数\n",
    "#                 counts, _, _ = np.histogram2d(\n",
    "#                     neuron_indices, \n",
    "#                     region_spike_times,\n",
    "#                     bins=[n_neurons, num_bins],\n",
    "#                     range=[[0, n_neurons], [0, duration]]\n",
    "#                 )\n",
    "                \n",
    "#                 spike_counts = np.sum(counts, axis=1)\n",
    "                \n",
    "#                 # 存储原始放电次数 (n_neurons, ) 形状的数组\n",
    "#                 activity_dict[region][session][f'{image}_{trial_idx}'] = spike_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85974dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tqdm\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# session_folder = os.listdir(\"/media/ubuntu/sda/neuropixels/output_dir\")\n",
    "# activity_dict = {}  \n",
    "# num_bins = 30\n",
    "# for session in tqdm.tqdm(session_folder):\n",
    "#     if 'session_' not in session:\n",
    "#         continue\n",
    "        \n",
    "#     trigger_time = pd.read_csv(f\"/media/ubuntu/sda/neuropixels/output_dir/{session}/stimulus_table_natural_movie.csv\")\n",
    "#     trigger_time = trigger_time[trigger_time['stimulus_name'] == 'natural_movie_one']\n",
    "\n",
    "#     spike_inf = pd.read_csv(f\"/media/ubuntu/sda/neuropixels/output_dir/{session}/spike_inf_natural_movie.csv\", index_col=0)\n",
    "    \n",
    "#     filtered_spikes = spike_inf['id'].value_counts()\n",
    "#     filtered_spikes = filtered_spikes[filtered_spikes > 30000].index\n",
    "#     spike_inf = spike_inf[spike_inf['id'].isin(filtered_spikes)]\n",
    "    \n",
    "#     regions = spike_inf['region'].unique()\n",
    "    \n",
    "#     neuron_mappings = {}\n",
    "#     for region in regions:\n",
    "#         region_neurons = spike_inf[spike_inf['region'] == region]['id'].unique()\n",
    "#         region_neurons = np.intersect1d(region_neurons, filtered_spikes)\n",
    "        \n",
    "#         if len(region_neurons) == 0:\n",
    "#             continue\n",
    "            \n",
    "#         neuron_id_to_idx = {id: idx for idx, id in enumerate(region_neurons)}\n",
    "#         neuron_mappings[region] = {\n",
    "#             'neurons': region_neurons,\n",
    "#             'id_to_idx': neuron_id_to_idx,\n",
    "#             'n_neurons': len(region_neurons)\n",
    "#         }\n",
    "    \n",
    "#     if not neuron_mappings:\n",
    "#         continue\n",
    "        \n",
    "#     spike_times = spike_inf['time'].values\n",
    "#     spike_ids = spike_inf['id'].values\n",
    "#     spike_regions = spike_inf['region'].values\n",
    "    \n",
    "#     for region in neuron_mappings:\n",
    "#         if region not in activity_dict:\n",
    "#             activity_dict[region] = {}\n",
    "#         activity_dict[region][session] = {}\n",
    "    \n",
    "#     for trail_idx, time in enumerate(start_time):\n",
    "#         end_time = time + 30\n",
    "#         duration = 30\n",
    "        \n",
    "#         mask = (spike_times >= time) & (spike_times < end_time)\n",
    "#         trial_spike_times = spike_times[mask] - time\n",
    "#         trial_spike_ids = spike_ids[mask]\n",
    "#         trial_spike_regions = spike_regions[mask]\n",
    "        \n",
    "#         for region, mapping in neuron_mappings.items():\n",
    "#             region_mask = np.isin(trial_spike_regions, [region])\n",
    "#             region_spike_times = trial_spike_times[region_mask]\n",
    "#             region_spike_ids = trial_spike_ids[region_mask]\n",
    "            \n",
    "#             n_neurons = mapping['n_neurons']\n",
    "#             neuron_id_to_idx = mapping['id_to_idx']\n",
    "            \n",
    "#             if len(region_spike_times) == 0:\n",
    "#                 activity_matrix = np.zeros((n_neurons, num_bins))\n",
    "#                 activity_dict[region][session][f'{trail_idx}'] = activity_matrix\n",
    "#                 continue\n",
    "            \n",
    "#             neuron_indices = np.array([neuron_id_to_idx[id] for id in region_spike_ids])\n",
    "            \n",
    "#             counts, _, _ = np.histogram2d(\n",
    "#                 neuron_indices, \n",
    "#                 region_spike_times,\n",
    "#                 bins=[n_neurons, num_bins],\n",
    "#                 range=[[0, n_neurons], [0, duration]]\n",
    "#             )\n",
    "            \n",
    "#             bin_width = duration / num_bins\n",
    "#             firing_rates = counts / bin_width\n",
    "            \n",
    "#             smoothed_rates = gaussian_filter1d(\n",
    "#                 firing_rates, \n",
    "#                 sigma=2, \n",
    "#                 axis=1,  \n",
    "#                 mode='nearest'\n",
    "#             )\n",
    "            \n",
    "#             activity_dict[region][session][f'{trail_idx}'] = smoothed_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_inf_all = pd.DataFrame()\n",
    "# activity_dict = {}\n",
    "\n",
    "# for session in session_list:\n",
    "#     output_dir = f'/media/ubuntu/sda/neuropixels/output_dir/{session}'\n",
    "#     cluster_inf = pd.read_csv(f'{output_dir}/cluster_inf.csv')\n",
    "#     spike_inf = pd.read_csv(f'{output_dir}/spike_inf_static_gratings.csv', index_col=0)\n",
    "#     stimulus_table = pd.read_csv(f'{output_dir}/stimulus_table_static_gratings.csv')\n",
    "#     stimulus_table = stimulus_table[~stimulus_table['orientation'].isna()]\n",
    "    \n",
    "#     stimulus_table['orientation_phase_frequency'] = (\n",
    "#         stimulus_table['orientation'].astype(str) + '_' + \n",
    "#         stimulus_table['phase'].astype(str) + '_' + \n",
    "#         stimulus_table['spatial_frequency'].astype(str)\n",
    "#     )\n",
    "\n",
    "#     filtered_spikes = spike_inf['id'].value_counts()\n",
    "#     filtered_spikes = filtered_spikes[filtered_spikes > 30000].index\n",
    "#     cluster_inf = cluster_inf[cluster_inf['unit_id'].isin(filtered_spikes)]\n",
    "#     spike_inf = spike_inf[spike_inf['id'].isin(filtered_spikes)]\n",
    "\n",
    "#     cluster_inf_all = pd.concat((cluster_inf_all, cluster_inf), axis = 0)\n",
    "\n",
    "#     for neuron in cluster_inf['unit_id']:\n",
    "#         spike_inf_temp = spike_inf[spike_inf['id'] == neuron]\n",
    "#         neuron = f'{neuron}_{session}'\n",
    "#         activity_dict[neuron] = {}\n",
    "#         for image in stimulus_table['orientation_phase_frequency'].unique():\n",
    "#             activity_dict[neuron][image] = []\n",
    "\n",
    "#             trigger_time_temp = stimulus_table[(stimulus_table['orientation_phase_frequency'] == image)]\n",
    "\n",
    "#             for index, row in trigger_time_temp.iterrows():\n",
    "#                 start = row['start_time']\n",
    "#                 end = row['stop_time']\n",
    "\n",
    "#                 filtered_spikes = spike_inf_temp[(spike_inf_temp['time'] >= start) &\n",
    "#                                                 (spike_inf_temp['time'] <= end)]\n",
    "#                 if not filtered_spikes.empty:\n",
    "#                     relative_spikes = (filtered_spikes['time'] - start) * 1000\n",
    "#                     activity_dict[neuron][image].append(np.array(relative_spikes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c1a412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:38<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "session_folder = os.listdir(\"/media/ubuntu/sda/neuropixels/output_dir\")\n",
    "activity_dict = {}  \n",
    "for session in tqdm.tqdm(session_folder):\n",
    "    if 'session_' not in session:\n",
    "        continue\n",
    "        \n",
    "    trigger_time = pd.read_csv(f\"/media/ubuntu/sda/neuropixels/output_dir/{session}/stimulus_table_natural_movie.csv\")\n",
    "    trigger_time = trigger_time[trigger_time['stimulus_name'] == 'natural_movie_one']\n",
    "\n",
    "    spike_inf = pd.read_csv(f\"/media/ubuntu/sda/neuropixels/output_dir/{session}/spike_inf_natural_movie.csv\", index_col=0)\n",
    "    \n",
    "    filtered_spikes = spike_inf['id'].value_counts()\n",
    "    filtered_spikes = filtered_spikes[filtered_spikes > 30000].index\n",
    "    spike_inf = spike_inf[spike_inf['id'].isin(filtered_spikes)]\n",
    "    \n",
    "    regions = spike_inf['region'].unique()\n",
    "    \n",
    "    neuron_mappings = {}\n",
    "    for region in regions:\n",
    "        region_neurons = spike_inf[spike_inf['region'] == region]['id'].unique()\n",
    "        region_neurons = np.intersect1d(region_neurons, filtered_spikes)\n",
    "        \n",
    "        if len(region_neurons) == 0:\n",
    "            continue\n",
    "            \n",
    "        neuron_id_to_idx = {id: idx for idx, id in enumerate(region_neurons)}\n",
    "        neuron_mappings[region] = {\n",
    "            'neurons': region_neurons,\n",
    "            'id_to_idx': neuron_id_to_idx,\n",
    "            'n_neurons': len(region_neurons)\n",
    "        }\n",
    "    \n",
    "    if not neuron_mappings:\n",
    "        continue\n",
    "        \n",
    "    spike_times = spike_inf['time'].values\n",
    "    spike_ids = spike_inf['id'].values\n",
    "    spike_regions = spike_inf['region'].values\n",
    "    \n",
    "    for region in neuron_mappings:\n",
    "        if region not in activity_dict:\n",
    "            activity_dict[region] = {}\n",
    "        activity_dict[region][session] = {}\n",
    "    \n",
    "    trigger_time.index = range(len(trigger_time))\n",
    "    start_time = trigger_time.loc[np.where(trigger_time['frame'] == 0.0)[0], 'start_time'].values\n",
    "    for trail_idx, time in enumerate(start_time):\n",
    "        end_time = time + 30\n",
    "        \n",
    "        mask = (spike_times >= time) & (spike_times < end_time)\n",
    "        trial_spike_times = spike_times[mask] - time  # 相对于trial开始的时间\n",
    "        trial_spike_ids = spike_ids[mask]\n",
    "        trial_spike_regions = spike_regions[mask]\n",
    "        \n",
    "        for region, mapping in neuron_mappings.items():\n",
    "            region_mask = np.isin(trial_spike_regions, [region])\n",
    "            region_spike_times = trial_spike_times[region_mask] \n",
    "            region_spike_times = region_spike_times.astype(int)\n",
    "            region_spike_ids = trial_spike_ids[region_mask]\n",
    "            \n",
    "            n_neurons = mapping['n_neurons']\n",
    "            neuron_id_to_idx = mapping['id_to_idx']\n",
    "            \n",
    "            spike_trains = [[] for _ in range(n_neurons)]\n",
    "            \n",
    "            for spike_time, neuron_id in zip(region_spike_times, region_spike_ids):\n",
    "                neuron_idx = neuron_id_to_idx[neuron_id]\n",
    "                spike_trains[neuron_idx].append(spike_time)\n",
    "                        \n",
    "            activity_dict[region][session][f'{trail_idx}'] = spike_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f15f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'VISp'\n",
    "session_list = list(activity_dict[region].keys())\n",
    "for session in session_list:\n",
    "    if len(activity_dict[region][session].keys()) != 20:\n",
    "        session_list.remove(session)\n",
    "    \n",
    "activity_dict_all = {}\n",
    "for image in activity_dict[region][session].keys():\n",
    "    activity_dict_all[image] = []\n",
    "    for session in session_list:\n",
    "        activity_dict_all[image].extend(activity_dict[region][session][image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04e1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_reps = len(activity_dict_all.keys())\n",
    "# n_neurons = len(next(iter(activity_dict_all.values())))  \n",
    "# n_frames = 900\n",
    "\n",
    "# result_matrix = np.zeros((n_reps * n_frames, n_neurons, 33), dtype=np.int8)\n",
    "\n",
    "# rep_idx = 0\n",
    "# for rep in activity_dict_all.keys():\n",
    "#     neuron_idx = 0\n",
    "#     for neuron_spikes in activity_dict_all[rep]:\n",
    "#         for frame in range(n_frames):\n",
    "#             start_time = frame * 100 / 3 \n",
    "#             end_time = (frame + 1) * 100 / 3  \n",
    "            \n",
    "#             spike_train = [i for i in neuron_spikes if start_time <= i < end_time]\n",
    "            \n",
    "#             bin_duration = (end_time - start_time) / 33 \n",
    "            \n",
    "#             for spike_time in spike_train:\n",
    "#                 time_offset = spike_time - start_time\n",
    "#                 bin_index = int(time_offset / bin_duration)\n",
    "#                 if 0 <= bin_index < 33:\n",
    "#                     result_matrix[rep_idx * n_frames + frame, neuron_idx, bin_index] = 1\n",
    "            \n",
    "#         neuron_idx += 1\n",
    "#     rep_idx += 1\n",
    "\n",
    "# print(\"结果矩阵形状:\", result_matrix.shape)\n",
    "# print(\"非零元素数量:\", np.count_nonzero(result_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a2890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"/media/ubuntu/sda/neuropixels/natural_movie/response_matrix_binary.npy\", result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5f470d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_matrix = np.load(\"/media/ubuntu/sda/neuropixels/natural_movie/response_matrix_binary.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b086f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron_count = len(next(iter(activity_dict_all.values())))\n",
    "\n",
    "# with PdfPages(\"/media/ubuntu/sda/neuropixels/natural_movie/figure/peth_lineplot_all.pdf\") as pdf:\n",
    "#     for neuron_idx in range(neuron_count):\n",
    "#         print(f\"Processing neuron {neuron_idx+1}/{neuron_count}\")\n",
    "        \n",
    "#         fig, axes = plt.subplots(5, 6, figsize=(15, 6))\n",
    "#         fig.suptitle(f'Neuron {neuron_idx}', fontsize=16)\n",
    "        \n",
    "#         for time_window in range(30):\n",
    "#             start_time = time_window * 1000\n",
    "#             end_time = (time_window + 1) * 1000\n",
    "            \n",
    "#             all_spikes = []\n",
    "            \n",
    "#             for rep_key, rep_data in activity_dict_all.items():\n",
    "#                 neuron_spikes = rep_data[neuron_idx]\n",
    "                \n",
    "#                 window_spikes = [spike for spike in neuron_spikes \n",
    "#                                 if start_time <= spike < end_time]\n",
    "                \n",
    "#                 relative_spikes = [spike - start_time for spike in window_spikes]\n",
    "#                 all_spikes.extend(relative_spikes)\n",
    "            \n",
    "#             if not all_spikes:\n",
    "#                 spike_counts = np.zeros(30)\n",
    "#             else:\n",
    "#                 time_bins = np.linspace(0, 1000, 31)  # 11个点，形成10个区间\n",
    "#                 spike_counts, _ = np.histogram(all_spikes, bins=time_bins)\n",
    "            \n",
    "#             bin_duration = 0.1 \n",
    "#             spike_rates = spike_counts / bin_duration\n",
    "            \n",
    "#             bin_centers = (time_bins[:-1] + time_bins[1:]) / 2\n",
    "            \n",
    "#             row = time_window // 6\n",
    "#             col = time_window % 6\n",
    "#             ax = axes[row, col]\n",
    "            \n",
    "#             ax.plot(bin_centers, spike_rates, color='k', linewidth=1.5)\n",
    "#             ax.set_xticks([])\n",
    "#             ax.set_yticks([])\n",
    "            \n",
    "        \n",
    "#         plt.tight_layout(rect=[0, 0, 1, 0.96]) \n",
    "#         pdf.savefig(fig)\n",
    "#         plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2571b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephant.gpfa import GPFA\n",
    "activity = []\n",
    "for image in activity_dict_all.keys():\n",
    "    activity.append(activity_dict_all[image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5944101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo\n",
    "from quantities import s\n",
    "\n",
    "spike_trains = []\n",
    "for i in range(len(activity)):\n",
    "    temp = []\n",
    "    for j in range(len(activity[i])):\n",
    "        temp_spiketrain = neo.SpikeTrain(activity[i][j] * s, t_stop=30)\n",
    "        temp.append(temp_spiketrain)\n",
    "    spike_trains.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b6230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantities import ms\n",
    "\n",
    "from elephant.statistics import instantaneous_rate\n",
    "import neo\n",
    "from elephant.kernels import GaussianKernel\n",
    "\n",
    "gk = GaussianKernel(1000 * ms)\n",
    "trial_data = []\n",
    "for t, trial in enumerate(spike_trains):\n",
    "\n",
    "    inst_rates = []\n",
    "    for ch in trial:\n",
    "\n",
    "        inst_rate = instantaneous_rate(ch, kernel=gk, sampling_period=500*ms).magnitude\n",
    "\n",
    "        # append into list\n",
    "        inst_rates.append(inst_rate.flatten())\n",
    "\n",
    "    # stack rates back together and transpose = (channels by time)\n",
    "    inst_rates = np.stack(inst_rates, axis=1)\n",
    "\n",
    "    trial_data.append(inst_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a85cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "pca_result = pca.fit_transform(trial_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66d6c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d11bec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "name",
         "name": "螺旋线",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "LdkJGmHJQMC9vEEClqEBwIj8b1VLHSBA0xB3kXidGkAhv8FhCgkGQOqnn7SihPM/kGjXg7fHEkCWpFY3SRAuQL7nZGUiEUBAsATrmkFwSkC7Elfe+f9RQBAVYHEXKVVALeVEsrVDVkA7LDuUNvlVQJNpX9PlwVVAqEdjC5YQV0B+eRsJynBaQB6F9xewM19ABgyeb1b5YUANG1TCAcJjQOC7ZSlIvWRAGYYoEw/hZECNjeltOwNkQLwZ62FK4mFAc7kFVrX/XEBzRKcXm9JUQAUYdI6TR0pACg4Ae0RYPkADPV98D9cyQGat/KbjZDBAAZsyw+P8M0DGnR9s8Zw6QM8k85dMrUBAXotZsFDAQkCEeDnNlX5CQBBXXi+Ftz5AvS/7bRnWMkByEzJT94QGQPeuH2tyzS7AnyO3F1dpQcDubOrRULtLwCq+Vo3eHVPAzqoMXkjJV8Crg9/CbD5bwHIQ2S+gjF3AcQerplxxX8C1kmYjut1gwOkj/6noVmLAM1MRtkbjY8B23IW03/pkwPLyyVD1DWXAhcXwyurHY8CPFxo1TTRhwDQX/DTdeVvAuhRv2fsyVMD9QlzEh/xMwNc9Rp091EjAzJw0TjfNTsAearTqJlhXwFDL+Lg9G2HA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "bgO/4DqiYMD2jGgRhUlgwIkb8j/RnWDAEe6M1noyYcCUVfb333ZhwBPVH6YmFGHAtdmokVUbYMA8w69/rc9dwNKgwR2Sp1vA9Old43P3WcCTbgniF4RYwJUIhBXzC1fAgkZFRXOKVcA6xB7hKP9TwKxFm/o3IlLAt5geGf75TsAgK4kxwHZHwHTMCOReUDvAIcRNeGyBEMCapNyoO1g0QF0XYAceMUVAkis4WWK+TEBkgQ2E5RxPQA6RGT9lRUxADW+9J/ZRRkAra8zsC6JAQClwehphYDxA7KgUAslEQEDFzSBoFmdGQEoo/L7+TE9ADMNmIu7BVED9e4Xkg55ZQOXIYKgeKl1AquCxBPy3XkD2wLJyDZheQFT6UnzH8l1AB9j237HuXUCsNJ/01c5eQMPD/ywdv19Ap0QTzwJ+X0AmPLApP1xdQDPgSCDzxFlAR5Ihu53cVUBLQK5rVYRSQCYAluRgX09Agkn5PMpLSUAwWPrWSoZBQJ0NdsZ+1y9AthBNQ+PXEcDWIb7dhCo2wBEBy2G8fUDAdAGiuAIAQcA9Nno2Ul85wFwiGHNJ3ibA9ACeo55CyT+QfwgekO38PwWBXdlDtyXA7/Q4+a1zQsBFzEU0Kb1RwNuw+hk4X1rA",
          "dtype": "f8"
         },
         "z": {
          "bdata": "LHuPREXlWcA5dy2LrPlZwAannlTjTlnA6EEv1FxLV8CZVHnF3FBUwEDoNsLT4VDAaMFIFkJUScAzCZR6naI7wN7D2MYUp/o/tjDEAVZwQECuB2aJy9VNQEL2jT8eXFNA9NLQp4NUVUDgXUJV6GpVQDSGiXhTflRAyyxljOU1U0AQuWdv7tdRQO8m1BpPelBAfffFq150TkB5KSHmZWBMQIDPOwhRY0pA4vo+w/tdR0B+9IYXLt9BQKSDrG/OVzJAJ1F/ysgkEMAAHMJRKpM7wD40wRxCDEjADZlyBAWWT8D2brYELUxSwIM0c3wKMVTAGnS1hgUkVsAaU+MXyzNYwMV9DXhnslnAYD7do4m8WcBw4MotLdBXwD6B5OsE5VPAADx8f1N7TMBC0qONTog9wCQk9QfK/+m/dpmUH47qOEBULwjNvkFGQNc+KGMr1kxA+z0B8e2bUEBsK5e0tyJSQF/p2h2e31JATXWCUqVfUkCoPIiSDW9QQEZNTZcguEpABcTiCpOlQ0AHF/jzAWI6QEBRTW1oPDFA4UB0+a8tKkBTs3AaihMrQPhaA3pQzzBAGutAN8N3NEBPAX4PvOw1QMK7tug+UjJAlBJTI/osGkB96uJWSRIuwGK9U9FQ4UXA",
          "dtype": "f8"
         }
        },
        {
         "hoverinfo": "name",
         "name": "螺旋线",
         "showlegend": true,
         "type": "scatter3d",
         "x": {
          "bdata": "hVFxCprlRMCacR9w8fcywL0DOO2UpiPAiGOyDXDQH8DeEXhP0A4gwOiisP9e4R7AjYWo4BREFcBcnd2+UcDwP8LKmx4+cSdA/yNZqH91OECeXMRFWMZBQOEcFdDHJEVAlEgnTUhvRkBaSBuM6TFHQDWlf/kBhUlAzl1tiObeTkDtMsdYVrBTQKLXBaTz9FhAUpRpmqdIXkBay7XYwUdhQHImLmDZaGJAaLpm/E5MYkA/viaQCg5hQPJd2lArFF5A1X1pLDxsWUAKzneTARVVQDN5bd4s2FFAm8tQ2KYPUEDG2B6FcP1OQFNPFhfcDU9ALVSTSAEKT0AhGzxx8wJOQD8Cuu3cWEtADNh4hHafRkC2BCk+BVQ/QOQYLLc12ipAmXjpU6CpG8DpdTidnFQ7wPhf1C/6sUbA1q574RjFTcBl8WfYDmVRwG7UClqAG1PAmOM5E8VlVMCLSd57VqxVwJ1ogFE0QVfAXtHsc342WcB/KwwR2EhbwGpu3woP/VzAV82OmRzhXcBNA7lSEcFdwDnRjIylqlzAq4Qkcc3CWsDKZB6JeBxYwMpQjGDKwlTAWjyK4Lf9UMBVx8DSO0FLwPCLgdnXGUjA2J0XEJRSS8DCdRn2svFSwE9URFnKyFrA",
          "dtype": "f8"
         },
         "y": {
          "bdata": "JPUIR/wYUsCQz9n3nvtTwCTMNtoLRlbAHEwCXz56WMAfOlggitlZwAuFGsw/D1rAfiYCvWmqWcA0Il6DzrtZwEby5GGf5VrAdOvHJkq9XMAqBD/+Cw9ewBRHRdSQyF3AFhN1q16rW8AUgk42rDJYwFH3ryjOBVTAujVdwlsvT8D57ucSei5GwIa8/bSPFzrAwPli3HAlHsAvirWa1BElQMDWXp8aTDpAx2qwhs/WQkAQk7+LRtNFQMomYeonfUZAXmq659L7RUA6EkTNVb9FQDMqYJzu8kZAJsri2BEdSkCXDG7LzQBPQKnB8byXX1JAo47RtwUZVUDfxHGcPjBXQD3iC1h8ZlhAGI6o1Iu6WEAAK2RJ6lBYQEOVQEd5XVdAkxaQL6AWVkCuDW4zb69UQML4xHJeWVNAudMPSJZGUkD/6BotGJpRQMzaO8irRVFAVpN/7sD6UEA47Al+hU1QQMMOr8oo7U1AiHTdUk8ASkCZjm+7r3VFQCqLUZGSBkFArmRxkrM2OkDNu/P7LYQzQLPRaKFboitAgihTWrTyIUAS0MLOzeMRQE5YHoSaZfK/r9qbHLcBJMBU4Lm650Q3wMSnvEKtxkPArQUtmWV0S8DbF5Kf6kNQwPzNvj2+E1HA",
          "dtype": "f8"
         },
         "z": {
          "bdata": "BSm2jjP5VsCdcLxWLo1WwJmBsW5H3lXAQl09SvuNVMCr2Nv1BOFSwAD63L1zBVHARFvDK4DMTMA5iakEpHlEwNspr441hDLAz/mHlZuwFEA1aba0ib04QPNyATeW0EJA7GHQkueARkAFPJske9tIQCGBbjYm2UpAFpPs49+bTEAJiRPpC+9NQN/iENb24E5A1A6xjYrIT0CV1awQ6lxQQIgeW8i/ilBA/Cl3wAe8T0CbbbGGpa5LQDAp4XrumURAG/f7FSAcNkDebG9hWTXnP85tHglpzzPAxaQbDkekQsCLZuWZ0KRJwJRSJl7+Zk/AOufJD+I2UsCv7Ee4BlNUwIUC3/vRpVXAMI5zsImuVcCKKlfkHSBUwEvpD4TmDlHAphBHnpe8ScABAi0QlQ5AwPfp/GvaiyfAX3aeOP1LIED8h4FGRmM6QCq7cx/HEUVAymVJojmOS0ACplyvYjVQQHbPxFCMg1FAEJpF+ctNUUD83+t4IddOQOruKSbWVUhAdJqBiPWdQEAyxPOdGUczQP6TU5KYOSRAb9fW517NHEDeUgxzdocjQBI5fHvV6C5AUyDegBh5NUAuSk4luSk5QIagTfszpDdAqh9EF48hK0AeTqS6gzwcwAynfZZO3EHA",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "pca_result = pca.fit_transform(trial_data[1])\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=pca_result[:, 0],\n",
    "    y=pca_result[:, 1],\n",
    "    z=pca_result[:, 2],\n",
    "    name='螺旋线',\n",
    "    hoverinfo='name',\n",
    "    showlegend=True\n",
    "))\n",
    "pca_result = pca.fit_transform(trial_data[3])\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=pca_result[:, 0],\n",
    "    y=pca_result[:, 1],\n",
    "    z=pca_result[:, 2],\n",
    "    name='螺旋线',\n",
    "    hoverinfo='name',\n",
    "    showlegend=True\n",
    "))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title='三维折线图示例 - 螺旋线、正弦曲线与随机漫步',\n",
    "#     scene=dict(\n",
    "#         xaxis_title='X轴',\n",
    "#         yaxis_title='Y轴',\n",
    "#         zaxis_title='Z轴',\n",
    "#         xaxis=dict(backgroundcolor='rgba(0,0,0,0)', gridcolor='lightgray'),\n",
    "#         yaxis=dict(backgroundcolor='rgba(0,0,0,0)', gridcolor='lightgray'),\n",
    "#         zaxis=dict(backgroundcolor='rgba(0,0,0,0)', gridcolor='lightgray'),\n",
    "#         bgcolor='rgba(245, 245, 245, 1)',\n",
    "#         camera=dict(\n",
    "#             eye=dict(x=1.5, y=1.5, z=0.8)  \n",
    "#         )\n",
    "#     ),\n",
    "#     margin=dict(l=0, r=0, b=0, t=40),\n",
    "#     legend=dict(orientation='h', x=0.3, y=0.05),\n",
    "#     height=800,\n",
    "#     width=1000,\n",
    "#     template='plotly_white',\n",
    "#     font=dict(family=\"Arial\", size=12, color=\"black\")\n",
    "# )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd5afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"activity_dict_region_bin3.pkl\", 'wb') as f:\n",
    "#     pickle.dump(activity_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2c7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"activity_dict_region.pkl\", 'rb') as f:\n",
    "#     activity_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c255d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_folder = os.listdir(\"/media/ubuntu/sda/neuropixels/output_dir\")\n",
    "# activity_dict = {}  \n",
    "# num_bins = 3\n",
    "# for session in tqdm.tqdm(session_folder):\n",
    "#     if 'session_' not in session:\n",
    "#         continue\n",
    "        \n",
    "#     trigger_time = pd.read_csv(f\"/media/ubuntu/sda/neuropixels/output_dir/{session}/stimulus_table_natural_movie.csv\")\n",
    "#     trigger_time = trigger_time[trigger_time['stimulus_name'] == 'natural_movie_one']\n",
    "\n",
    "#     spike_inf = pd.read_csv(f\"/media/ubuntu/sda/neuropixels/output_dir/{session}/spike_inf_natural_movie.csv\", index_col=0)\n",
    "    \n",
    "#     filtered_spikes = spike_inf['id'].value_counts()\n",
    "#     filtered_spikes = filtered_spikes[filtered_spikes > 30000].index\n",
    "#     spike_inf = spike_inf[spike_inf['id'].isin(filtered_spikes)]\n",
    "    \n",
    "#     regions = spike_inf['region'].unique()\n",
    "    \n",
    "#     neuron_mappings = {}\n",
    "#     for region in regions:\n",
    "#         region_neurons = spike_inf[spike_inf['region'] == region]['id'].unique()\n",
    "#         region_neurons = np.intersect1d(region_neurons, filtered_spikes)\n",
    "        \n",
    "#         if len(region_neurons) == 0:\n",
    "#             continue\n",
    "            \n",
    "#         neuron_id_to_idx = {id: idx for idx, id in enumerate(region_neurons)}\n",
    "#         neuron_mappings[region] = {\n",
    "#             'neurons': region_neurons,\n",
    "#             'id_to_idx': neuron_id_to_idx,\n",
    "#             'n_neurons': len(region_neurons)\n",
    "#         }\n",
    "    \n",
    "#     if not neuron_mappings:\n",
    "#         continue\n",
    "        \n",
    "#     spike_times = spike_inf['time'].values\n",
    "#     spike_ids = spike_inf['id'].values\n",
    "#     spike_regions = spike_inf['region'].values\n",
    "    \n",
    "#     for region in neuron_mappings:\n",
    "#         if region not in activity_dict:\n",
    "#             activity_dict[region] = {}\n",
    "#         activity_dict[region][session] = {}\n",
    "    \n",
    "#     for image in range(2, 900):\n",
    "#         trigger_time_temp = trigger_time[trigger_time['frame'] == image]\n",
    "        \n",
    "#         for trial_idx, (_, row) in enumerate(trigger_time_temp.iterrows()):\n",
    "#             start_time = row['start_time'] - 33 / 1000\n",
    "#             end_time = row['stop_time'] + 33 / 1000\n",
    "#             duration = end_time - start_time\n",
    "            \n",
    "#             mask = (spike_times >= start_time) & (spike_times < end_time)\n",
    "#             trial_spike_times = spike_times[mask] - start_time\n",
    "#             trial_spike_ids = spike_ids[mask]\n",
    "#             trial_spike_regions = spike_regions[mask]\n",
    "            \n",
    "#             for region, mapping in neuron_mappings.items():\n",
    "#                 region_mask = np.isin(trial_spike_regions, [region])\n",
    "#                 region_spike_times = trial_spike_times[region_mask]\n",
    "#                 region_spike_ids = trial_spike_ids[region_mask]\n",
    "                \n",
    "#                 n_neurons = mapping['n_neurons']\n",
    "#                 neuron_id_to_idx = mapping['id_to_idx']\n",
    "                \n",
    "#                 if len(region_spike_times) == 0:\n",
    "#                     activity_matrix = np.zeros((n_neurons, num_bins))\n",
    "#                     activity_dict[region][session][f'{image}_{trial_idx}'] = activity_matrix\n",
    "#                     continue\n",
    "                \n",
    "#                 neuron_indices = np.array([neuron_id_to_idx[id] for id in region_spike_ids])\n",
    "                \n",
    "#                 counts, _, _ = np.histogram2d(\n",
    "#                     neuron_indices, \n",
    "#                     region_spike_times,\n",
    "#                     bins=[n_neurons, num_bins],\n",
    "#                     range=[[0, n_neurons], [0, duration]]\n",
    "#                 )\n",
    "                \n",
    "#                 bin_width = duration / num_bins\n",
    "#                 firing_rates = counts / bin_width\n",
    "                \n",
    "#                 smoothed_rates = gaussian_filter1d(\n",
    "#                     firing_rates, \n",
    "#                     sigma=2, \n",
    "#                     axis=1,  \n",
    "#                     mode='nearest'\n",
    "#                 )\n",
    "                \n",
    "#                 activity_dict[region][session][f'{image}_{trial_idx}'] = smoothed_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e03aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/ubuntu/sda/neuropixels/activity_dict_region_bin3_past_future.pkl\", 'rb') as f:\n",
    "    activity_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4918510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self,\n",
    "                 input_neuron=25,        \n",
    "                 time_bins=20,          \n",
    "                 d_model = 150,          \n",
    "                 nhead=10,                \n",
    "                 num_transformer_layers=1, \n",
    "                 conv_channels=64,      \n",
    "                 num_conv_blocks=1,      \n",
    "                 num_classes=898,        \n",
    "                 residual_dims=[256, 512, 1024], \n",
    "                 use_positional_encoding=True,  \n",
    "                 dim_feedforward_ratio=4,      \n",
    "                 activation='relu',\n",
    "                 use_neuron_masking=True,  \n",
    "                 mask_ratio=0,\n",
    "                 mask_replacement='zero'):\n",
    "        \n",
    "        # Transformer \n",
    "        self.transformer = {\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_layers': num_transformer_layers,\n",
    "            'dim_feedforward': d_model * dim_feedforward_ratio,\n",
    "            'activation': activation\n",
    "        }\n",
    "        \n",
    "        # cnn\n",
    "        self.convolution = {\n",
    "            'channels': conv_channels,\n",
    "            'num_blocks': num_conv_blocks,\n",
    "            'kernel_size': (3, 3),\n",
    "            'pool_size': (2, 2)\n",
    "        }\n",
    "        \n",
    "        # resnet\n",
    "        self.residual = {\n",
    "            'dims': residual_dims,\n",
    "            'skip_connection': True\n",
    "        }\n",
    "        \n",
    "        self.masking = {\n",
    "            'enabled': use_neuron_masking,\n",
    "            'ratio': mask_ratio,\n",
    "            'replacement': mask_replacement\n",
    "        }\n",
    "\n",
    "        self.input_dim = input_neuron\n",
    "        self.time_steps = time_bins\n",
    "        self.num_classes = num_classes\n",
    "        self.positional_encoding = use_positional_encoding\n",
    "        self.lr = 2e-4\n",
    "        self.epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5385cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuronMasker(nn.Module):\n",
    "    def __init__(self, mask_ratio=0.15, replacement='random'):\n",
    "        super().__init__()\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.replacement = replacement\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            if x is None:\n",
    "                raise ValueError(\"Input tensor x is None\")\n",
    "                \n",
    "            batch_size, seq_len, feat_dim = x.shape\n",
    "            mask = torch.rand(batch_size, 1, feat_dim, device=x.device) < self.mask_ratio\n",
    "            mask = mask.expand_as(x)\n",
    "            \n",
    "            if self.replacement == 'zero':\n",
    "                x_masked = x.masked_fill(mask, 0)\n",
    "            elif self.replacement == 'random':\n",
    "                random_values = torch.randn_like(x) * 0.02\n",
    "                x_masked = x.masked_scatter(mask, random_values)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid replacement: {self.replacement}\")\n",
    "            \n",
    "            return x_masked \n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "class ResidualLinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.downsample = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.downsample(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x + residual\n",
    "\n",
    "class TimeTransformerConvModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.input_proj = nn.Linear(config.input_dim, config.transformer['d_model'])\n",
    "        self.pos_encoder = PositionalEncoding(config.transformer['d_model']) if config.positional_encoding else nn.Identity()\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.transformer['d_model'],\n",
    "            nhead=config.transformer['nhead'],\n",
    "            dim_feedforward=config.transformer['dim_feedforward'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, config.transformer['num_layers'])\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential()\n",
    "        in_channels = 1\n",
    "        for _ in range(config.convolution['num_blocks']):\n",
    "            self.conv_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, config.convolution['channels'], \n",
    "                            kernel_size=config.convolution['kernel_size'], padding='same'),\n",
    "                    nn.BatchNorm2d(config.convolution['channels']),\n",
    "                    nn.ELU(),\n",
    "                    nn.MaxPool2d(kernel_size=config.convolution['pool_size'])\n",
    "                )\n",
    "            )\n",
    "            in_channels = config.convolution['channels']\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(config.convolution['channels'], config.num_classes)\n",
    "        \n",
    "        self.residual_layers = nn.Sequential()\n",
    "        current_dim = config.convolution['channels']\n",
    "        for dim in config.residual['dims']:\n",
    "            self.residual_layers.append(ResidualLinearBlock(current_dim, dim))\n",
    "            current_dim = dim\n",
    "        if current_dim != 1024:\n",
    "            self.residual_layers.append(nn.Linear(current_dim, 1024))\n",
    "            self.residual_layers.append(nn.LayerNorm(1024))\n",
    "\n",
    "\n",
    "        self.masker = NeuronMasker(\n",
    "            mask_ratio=self.config.masking['ratio'],\n",
    "            replacement=self.config.masking['replacement']\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.masker(x)  # [B, T, D]\n",
    "        #print(x)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_blocks(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        features = self.residual_layers(x)\n",
    "        \n",
    "        return logits, features\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return x\n",
    "\n",
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, alpha=0, temp=0.07):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha     \n",
    "        self.temp = temp\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.temp = temp\n",
    "        \n",
    "    \n",
    "    def contrastive_loss(self, h_neuro, h_img):\n",
    "        h_neuro = F.normalize(h_neuro, dim=1) + 1e-10\n",
    "        h_img = F.normalize(h_img, dim=1) + 1e-10\n",
    "        \n",
    "        logits_ab = torch.matmul(h_neuro, h_img.T) / self.temp\n",
    "        logits_ba = torch.matmul(h_img, h_neuro.T) / self.temp\n",
    "        \n",
    "        labels = torch.arange(h_neuro.size(0), device=h_neuro.device)\n",
    "        loss_ab = F.cross_entropy(logits_ab, labels)\n",
    "        loss_ba = F.cross_entropy(logits_ba, labels)\n",
    "        \n",
    "        return (loss_ab + loss_ba) / 2\n",
    "    \n",
    "    def forward(self, logits, labels, img_feature, features):\n",
    "        loss_cls = self.ce_loss(logits, labels)\n",
    "        loss_cont = self.contrastive_loss(features, img_feature)\n",
    "        total_loss = self.alpha * loss_cls + (1 - self.alpha) * loss_cont\n",
    "        return total_loss\n",
    "    \n",
    "\n",
    "def train_model(model, dataloader, optimizer, device, criterion, config):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (neuro, labels, img_feature) in enumerate(dataloader):\n",
    "        neuro = neuro.to(device)\n",
    "        labels = labels.to(device)\n",
    "        img_feature = img_feature.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, features = model(neuro)\n",
    "        loss = criterion(logits, labels, img_feature, features)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), \n",
    "            max_norm=3.0,                   \n",
    "            norm_type=2.0                   \n",
    "        )\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_loss = total_loss / len(dataloader)\n",
    "    train_accuracy = correct / total\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device, criterion, config):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    for neuro, labels, img_feature in dataloader:\n",
    "        neuro = neuro.to(device)\n",
    "        labels = labels.to(device)\n",
    "        img_feature = img_feature.to(device)\n",
    "        \n",
    "        logits, features = model(neuro)\n",
    "        \n",
    "        loss = criterion(logits, labels, img_feature, features)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # （Top-1 和 Top-5）\n",
    "        _, predicted_top1 = torch.max(logits, 1)\n",
    "        correct_top1 += (predicted_top1 == labels).sum().item()\n",
    "        _, predicted_top5 = logits.topk(5, dim=1)\n",
    "        correct_top5 += torch.sum(predicted_top5.eq(labels.view(-1, 1))).item()\n",
    "    \n",
    "        \n",
    "        total += labels.size(0)\n",
    "    \n",
    "    test_loss = total_loss / len(dataloader)\n",
    "    test_accuracy = correct_top1 / total\n",
    "    top5_accuracy = correct_top5 / total\n",
    "    \n",
    "    return test_loss, test_accuracy, top5_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90a98491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_loop(config, model, train_loader, test_loader, device):\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "    criterion = MultitaskLoss(alpha=0.5, temp=0.07)\n",
    "    \n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs, test_top5 = [], [], []\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss, train_acc = train_model(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        test_loss, test_acc, top5_acc = evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=test_loader,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        test_top5.append(top5_acc)\n",
    "        \n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model, \"best_model_VISp.pth\")\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        # print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "        # print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%}\")\n",
    "        # print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2%} | Top-5 Acc: {top5_acc:.2%}\")\n",
    "        # print(f\"2-way Acc: {accuracy_2way:.2%} | 4-way Acc: {accuracy_4way:.2%} | 10-way Acc: {accuracy_10way:.2%}\")\n",
    "        # print(\"-\" * 60)\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"best_test_acc\": best_acc,\n",
    "        \"final_top5_acc\": test_top5[-1],\n",
    "        \"train_history\": {\n",
    "            \"loss\": train_losses,\n",
    "            \"accuracy\": train_accs\n",
    "        },\n",
    "        \"test_history\": {\n",
    "            \"loss\": test_losses,\n",
    "            \"accuracy\": test_accs,\n",
    "            \"top5_accuracy\": test_top5\n",
    "        },\n",
    "        \"best_epoch\": best_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2a7458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPDataset(Dataset):\n",
    "    def __init__(self, EP_data, labels, features):  \n",
    "        self.EP_data = EP_data\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        #self.max_pool = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.EP_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        EP_tensor = torch.tensor(self.EP_data[idx], dtype=torch.float32) \n",
    "        label = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        \n",
    "        return EP_tensor.T, label, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6214528",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feature = pd.read_csv(\"/media/ubuntu/sda/neuropixels/natural_movie/natural_movie_1_frame_features.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "region = 'VISp'\n",
    "print(f'Start region: {region}')\n",
    "\n",
    "session_list = list(activity_dict[region].keys())\n",
    "for session in session_list:\n",
    "    if len(activity_dict[region][session].keys()) != 17960:\n",
    "        session_list.remove(session)\n",
    "    \n",
    "activity_dict_all = {}\n",
    "for image in activity_dict[region][session].keys():\n",
    "    temp = pd.DataFrame()\n",
    "    for session in session_list:\n",
    "        temp = pd.concat((temp, pd.DataFrame(activity_dict[region][session][image])), axis=0)\n",
    "    \n",
    "    global_min = temp.min()\n",
    "    global_max = temp.max()\n",
    "    activity_dict_all[image] = (temp - global_min) / (global_max - global_min + 1e-8)\n",
    "image_feature = pd.read_csv(\"/media/ubuntu/sda/neuropixels/natural_movie/natural_movie_1_frame_features.csv\", index_col=0)\n",
    "\n",
    "EP_data = []\n",
    "labels = []\n",
    "features = []\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "for image in activity_dict_all.keys():\n",
    "        EP_data.append(np.array(activity_dict_all[image]))\n",
    "        image = int(image.split(\"_\")[0]) - 2\n",
    "        labels.append(image)\n",
    "        features.append(np.array(image_feature.iloc[image, :]))\n",
    "\n",
    "dataset = EPDataset(EP_data=EP_data, labels=labels, features=features)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 100 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "config = ModelConfig(input_neuron=EP_data[0].shape[0])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TimeTransformerConvModel(config).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba56167c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train: VISp\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (64x1x75). Calculated output size: (64x0x37). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m results[region] \u001b[38;5;241m=\u001b[39m \u001b[43mmain_train_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#image_cluster = image_cluster\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m, in \u001b[0;36mmain_train_loop\u001b[0;34m(config, model, train_loader, test_loader, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 10\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     test_loss, test_acc, top5_acc \u001b[38;5;241m=\u001b[39m evaluate_model(\n\u001b[1;32m     20\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[26], line 165\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, device, criterion, config)\u001b[0m\n\u001b[1;32m    161\u001b[0m img_feature \u001b[38;5;241m=\u001b[39m img_feature\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    163\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 165\u001b[0m logits, features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneuro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels, img_feature, features)\n\u001b[1;32m    168\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[26], line 99\u001b[0m, in \u001b[0;36mTimeTransformerConvModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[1;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madaptive_pool(x)\n\u001b[1;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/modules/pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/visual_decoding/lib/python3.13/site-packages/torch/nn/functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (64x1x75). Calculated output size: (64x0x37). Output size is too small"
     ]
    }
   ],
   "source": [
    "print(f'Start train: {region}')\n",
    "results[region] = main_train_loop(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device\n",
    "    #image_cluster = image_cluster\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_sorting_jct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
